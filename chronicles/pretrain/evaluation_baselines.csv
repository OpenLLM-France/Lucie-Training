model_name,training_tokens,benchmark,dataset,score,stderr,score_name,model_num_parameters,num_fewshot,doc_to_target,"acc,none","acc_stderr,none","acc_norm,none","acc_norm_stderr,none","rouge1,none","rouge1_stderr,none","f1,none","f1_stderr,none","exact,none","exact_stderr,none","is_included,none","is_included_stderr,none","prompt_level_strict_acc,none","prompt_level_strict_acc_stderr,none","inst_level_strict_acc,none","inst_level_strict_acc_stderr,none","prompt_level_loose_acc,none","prompt_level_loose_acc_stderr,none","inst_level_loose_acc,none","inst_level_loose_acc_stderr,none","exact_match,none","exact_match_stderr,none","exact_match,strict-match","exact_match_stderr,strict-match","exact_match,flexible-extract","exact_match_stderr,flexible-extract","bleu_max,none","bleu_max_stderr,none","bleu_acc,none","bleu_acc_stderr,none","bleu_diff,none","bleu_diff_stderr,none","rouge1_max,none","rouge1_max_stderr,none","rouge1_acc,none","rouge1_acc_stderr,none","rouge1_diff,none","rouge1_diff_stderr,none","rouge2_max,none","rouge2_max_stderr,none","rouge2_acc,none","rouge2_acc_stderr,none","rouge2_diff,none","rouge2_diff_stderr,none","rougeL_max,none","rougeL_max_stderr,none","rougeL_acc,none","rougeL_acc_stderr,none","rougeL_diff,none","rougeL_diff_stderr,none"
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_compréhension_de_la_date,0.144,0.022249407735450213,"acc_norm,none",6857302016,3.0,{{target}},,,0.144,0.022249407735450213,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_compréhension_des_sports,0.552,0.03151438761115351,"acc_norm,none",6857302016,3.0,{{target}},,,0.552,0.03151438761115351,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_comptage_d_objets,0.224,0.02642136168734791,"acc_norm,none",6857302016,3.0,{{target}},,,0.224,0.02642136168734791,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_déduction_logique_cinq_objets,0.22,0.02625179282460584,"acc_norm,none",6857302016,3.0,{{target}},,,0.22,0.02625179282460584,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_déduction_logique_sept_objets,0.148,0.02250354724380614,"acc_norm,none",6857302016,3.0,{{target}},,,0.148,0.02250354724380614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_déduction_logique_trois_objets,0.348,0.030186568464511686,"acc_norm,none",6857302016,3.0,{{target}},,,0.348,0.030186568464511686,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_désambiguïsation_qa,0.312,0.029361067575219817,"acc_norm,none",6857302016,3.0,{{target}},,,0.312,0.029361067575219817,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_expressions_booléennes,0.46,0.031584653891499004,"acc_norm,none",6857302016,3.0,{{target}},,,0.46,0.031584653891499004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_formes_géométriques,0.112,0.019985536939171437,"acc_norm,none",6857302016,3.0,{{target}},,,0.112,0.019985536939171437,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_hyperbate,0.484,0.03166998503010742,"acc_norm,none",6857302016,3.0,{{target}},,,0.484,0.03166998503010742,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_jugement_causal,0.4919786096256685,0.03665706061581778,"acc_norm,none",6857302016,3.0,{{target}},,,0.4919786096256685,0.03665706061581778,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_naviguer,0.576,0.03131803437491614,"acc_norm,none",6857302016,3.0,{{target}},,,0.576,0.03131803437491614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_pingouins_sur_une_table,0.2191780821917808,0.03435504786264928,"acc_norm,none",6857302016,3.0,{{target}},,,0.2191780821917808,0.03435504786264928,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_raisonnement_sur_les_objets_colorés,0.156,0.022995023034068748,"acc_norm,none",6857302016,3.0,{{target}},,,0.156,0.022995023034068748,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_recommandation_de_film,0.296,0.02892893938837963,"acc_norm,none",6857302016,3.0,{{target}},,,0.296,0.02892893938837963,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_sarcasmes,0.5393258426966292,0.03746587736387869,"acc_norm,none",6857302016,3.0,{{target}},,,0.5393258426966292,0.03746587736387869,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_sophismes_formels,0.532,0.031621252575725504,"acc_norm,none",6857302016,3.0,{{target}},,,0.532,0.031621252575725504,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_cinq_objets,0.2,0.025348970020979078,"acc_norm,none",6857302016,3.0,{{target}},,,0.2,0.025348970020979078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_sept_objets,0.108,0.019669559381568752,"acc_norm,none",6857302016,3.0,{{target}},,,0.108,0.019669559381568752,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_trois_objets,0.332,0.02984403904746589,"acc_norm,none",6857302016,3.0,{{target}},,,0.332,0.02984403904746589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_séquences_temporelles,0.184,0.02455581299422256,"acc_norm,none",6857302016,3.0,{{target}},,,0.184,0.02455581299422256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_toile_de_mensonges,0.512,0.03167708558254708,"acc_norm,none",6857302016,3.0,{{target}},,,0.512,0.03167708558254708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_gpqa_diamond_fr,0.2639593908629442,0.03148410927642122,"acc_norm,none",6857302016,0.0,answer,,,0.2639593908629442,0.03148410927642122,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_gpqa_extended_fr,0.24861878453038674,0.018565108552111804,"acc_norm,none",6857302016,0.0,answer,,,0.24861878453038674,0.018565108552111804,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_gpqa_main_fr,0.2261072261072261,0.020219747704114503,"acc_norm,none",6857302016,0.0,answer,,,0.2261072261072261,0.020219747704114503,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_ifeval_fr,0.02330097087378641,0.006654046431364153,"prompt_level_loose_acc,none",6857302016,0.0,0,,,,,,,,,,,,,0.02524271844660194,0.006918863237981381,0.2086824067022087,N/A,0.02330097087378641,0.006654046431364153,0.21858339680121858,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_algebra_hard_fr,0.014285714285714285,0.006352048301969054,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.014285714285714285,0.006352048301969054,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_counting_and_prob_hard_fr,0.00510204081632653,0.005102040816326531,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.00510204081632653,0.005102040816326531,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_geometry_hard_fr,0.009708737864077669,0.006848349729346166,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.009708737864077669,0.006848349729346166,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_num_theory_hard_fr,0.004608294930875576,0.004608294930875575,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.004608294930875576,0.004608294930875575,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_prealgebra_hard_fr,0.0,0.0,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_precalculus_hard_fr,0.0,0.0,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_mmlu_fr,0.2519584104828372,0.0036637706770020913,"acc,none",6857302016,5.0,Answer,0.2519584104828372,0.0036637706770020913,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_musr_murder_mysteries_fr,0.504,0.031685198551199154,"acc_norm,none",6857302016,0.0,{{answer_choice}},,,0.504,0.031685198551199154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_musr_object_placements_fr,0.265625,0.027658162598649488,"acc_norm,none",6857302016,0.0,{{answer_choice}},,,0.265625,0.027658162598649488,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_musr_team_allocation_fr,0.248,0.027367497504863555,"acc_norm,none",6857302016,0.0,{{answer_choice}},,,0.248,0.027367497504863555,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,french_bench,french_bench_arc_challenge,0.290846877673225,0.013288647804165358,"acc_norm,none",6857302016,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.2446535500427716,0.012578458921815737,0.290846877673225,0.013288647804165358,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,french_bench,french_bench_grammar,0.7815126050420168,0.03803997152889484,"acc,none",6857302016,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7815126050420168,0.03803997152889484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,french_bench,french_bench_hellaswag,0.4197901049475262,0.005107460126998205,"acc_norm,none",6857302016,5.0,{{label}},0.3366887984579139,0.004890680817363448,0.4197901049475262,0.005107460126998205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,french_bench,french_bench_vocab,0.6974789915966386,0.04228655753449826,"acc,none",6857302016,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.6974789915966386,0.04228655753449826,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,arc_de,0.2728828058169376,0.013033734552785703,"acc_norm,none",6857302016,25.0,gold,0.21899059024807527,0.012100949230812877,0.2728828058169376,0.013033734552785703,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,arc_es,0.2923076923076923,0.013302556252265542,"acc_norm,none",6857302016,25.0,gold,0.2452991452991453,0.01258427449627725,0.2923076923076923,0.013302556252265542,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,arc_fr,0.29597946963216426,0.013356788048643695,"acc_norm,none",6857302016,25.0,gold,0.2506415739948674,0.012680895706050929,0.29597946963216426,0.013356788048643695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,arc_it,0.2694610778443114,0.012982199528535547,"acc_norm,none",6857302016,25.0,gold,0.24037639007698888,0.01250327289928353,0.2694610778443114,0.012982199528535547,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,m_mmlu_de,0.2582591642781717,0.003801292923533988,"acc,none",6857302016,25.0,answer,0.2582591642781717,0.003801292923533988,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,m_mmlu_es,0.2583620818959052,0.0037909356391134376,"acc,none",6857302016,25.0,answer,0.2583620818959052,0.0037909356391134376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,m_mmlu_fr,0.2542204568023833,0.003805753516423808,"acc,none",6857302016,25.0,answer,0.2542204568023833,0.003805753516423808,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,m_mmlu_it,0.26493918561607616,0.0038358035342660733,"acc,none",6857302016,25.0,answer,0.26493918561607616,0.0038358035342660733,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,arc_de,0.2728828058169376,0.013033734552785703,"acc_norm,none",6857302016,25.0,gold,0.21899059024807527,0.012100949230812877,0.2728828058169376,0.013033734552785703,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,arc_es,0.2923076923076923,0.013302556252265542,"acc_norm,none",6857302016,25.0,gold,0.2452991452991453,0.01258427449627725,0.2923076923076923,0.013302556252265542,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,arc_fr,0.29597946963216426,0.013356788048643695,"acc_norm,none",6857302016,25.0,gold,0.2506415739948674,0.012680895706050929,0.29597946963216426,0.013356788048643695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,arc_it,0.2694610778443114,0.012982199528535547,"acc_norm,none",6857302016,25.0,gold,0.24037639007698888,0.01250327289928353,0.2694610778443114,0.012982199528535547,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,m_mmlu_de,0.2582591642781717,0.003801292923533988,"acc,none",6857302016,25.0,answer,0.2582591642781717,0.003801292923533988,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,m_mmlu_es,0.2583620818959052,0.0037909356391134376,"acc,none",6857302016,25.0,answer,0.2583620818959052,0.0037909356391134376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,m_mmlu_fr,0.2542204568023833,0.003805753516423808,"acc,none",6857302016,25.0,answer,0.2542204568023833,0.003805753516423808,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,m_mmlu_it,0.26493918561607616,0.0038358035342660733,"acc,none",6857302016,25.0,answer,0.26493918561607616,0.0038358035342660733,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,arc_challenge,0.3856655290102389,0.01422425097325718,"acc_norm,none",6857302016,25.0,{{choices.label.index(answerKey)}},0.34897610921501704,0.013928933461382501,0.3856655290102389,0.01422425097325718,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,gsm8k,0.026535253980288095,0.004427045987265163,"exact_match,flexible-extract",6857302016,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.02350265352539803,0.004172883669643973,0.026535253980288095,0.004427045987265163,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,hellaswag,0.6539533957379008,0.004747360500742485,"acc_norm,none",6857302016,10.0,{{label}},0.4809798844851623,0.004986169849946311,0.6539533957379008,0.004747360500742485,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,humanities,0.3013815090329437,0.006632629194659845,"acc,none",6857302016,,,0.3013815090329437,0.006632629194659845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu,0.2641361629397522,0.0037153261259414965,"acc,none",6857302016,,,0.2641361629397522,0.0037153261259414965,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_abstract_algebra,0.25,0.04351941398892446,"acc,none",6857302016,5.0,answer,0.25,0.04351941398892446,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_anatomy,0.34074074074074073,0.04094376269996794,"acc,none",6857302016,5.0,answer,0.34074074074074073,0.04094376269996794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_astronomy,0.27631578947368424,0.03639057569952925,"acc,none",6857302016,5.0,answer,0.27631578947368424,0.03639057569952925,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_business_ethics,0.25,0.04351941398892446,"acc,none",6857302016,5.0,answer,0.25,0.04351941398892446,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_clinical_knowledge,0.2490566037735849,0.02661648298050171,"acc,none",6857302016,5.0,answer,0.2490566037735849,0.02661648298050171,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_biology,0.2777777777777778,0.03745554791462457,"acc,none",6857302016,5.0,answer,0.2777777777777778,0.03745554791462457,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_chemistry,0.21,0.040936018074033256,"acc,none",6857302016,5.0,answer,0.21,0.040936018074033256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_computer_science,0.3,0.046056618647183814,"acc,none",6857302016,5.0,answer,0.3,0.046056618647183814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_mathematics,0.21,0.040936018074033256,"acc,none",6857302016,5.0,answer,0.21,0.040936018074033256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_medicine,0.1907514450867052,0.029957851329869337,"acc,none",6857302016,5.0,answer,0.1907514450867052,0.029957851329869337,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_physics,0.2549019607843137,0.043364327079931785,"acc,none",6857302016,5.0,answer,0.2549019607843137,0.043364327079931785,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_computer_security,0.29,0.04560480215720684,"acc,none",6857302016,5.0,answer,0.29,0.04560480215720684,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_conceptual_physics,0.32340425531914896,0.030579442773610348,"acc,none",6857302016,5.0,answer,0.32340425531914896,0.030579442773610348,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation,0.32965389545648766,0.003908571442781024,"acc,none",6857302016,,,0.32965389545648766,0.003908571442781024,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_abstract_algebra,0.25,0.04351941398892446,"acc_norm,none",6857302016,0.0,{{answer}},0.22,0.04163331998932268,0.25,0.04351941398892446,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_anatomy,0.37777777777777777,0.04188307537595853,"acc_norm,none",6857302016,0.0,{{answer}},0.4,0.04232073695151589,0.37777777777777777,0.04188307537595853,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_astronomy,0.39473684210526316,0.039777499346220734,"acc_norm,none",6857302016,0.0,{{answer}},0.3026315789473684,0.03738520676119668,0.39473684210526316,0.039777499346220734,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_business_ethics,0.45,0.05000000000000001,"acc_norm,none",6857302016,0.0,{{answer}},0.47,0.050161355804659205,0.45,0.05000000000000001,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_clinical_knowledge,0.4188679245283019,0.030365050829115205,"acc_norm,none",6857302016,0.0,{{answer}},0.3433962264150943,0.02922452646912479,0.4188679245283019,0.030365050829115205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_biology,0.3611111111111111,0.040166600304512336,"acc_norm,none",6857302016,0.0,{{answer}},0.3680555555555556,0.040329990539607175,0.3611111111111111,0.040166600304512336,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_chemistry,0.24,0.042923469599092816,"acc_norm,none",6857302016,0.0,{{answer}},0.26,0.04408440022768078,0.24,0.042923469599092816,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_computer_science,0.23,0.04229525846816505,"acc_norm,none",6857302016,0.0,{{answer}},0.26,0.0440844002276808,0.23,0.04229525846816505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_mathematics,0.21,0.040936018074033256,"acc_norm,none",6857302016,0.0,{{answer}},0.22,0.041633319989322695,0.21,0.040936018074033256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_medicine,0.3468208092485549,0.03629146670159663,"acc_norm,none",6857302016,0.0,{{answer}},0.3352601156069364,0.03599586301247078,0.3468208092485549,0.03629146670159663,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_physics,0.23529411764705882,0.04220773659171453,"acc_norm,none",6857302016,0.0,{{answer}},0.19607843137254902,0.03950581861179964,0.23529411764705882,0.04220773659171453,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_computer_security,0.4,0.04923659639173309,"acc_norm,none",6857302016,0.0,{{answer}},0.4,0.049236596391733084,0.4,0.04923659639173309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_conceptual_physics,0.33191489361702126,0.030783736757745657,"acc_norm,none",6857302016,0.0,{{answer}},0.34893617021276596,0.03115852213135778,0.33191489361702126,0.030783736757745657,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_econometrics,0.22807017543859648,0.03947152782669415,"acc_norm,none",6857302016,0.0,{{answer}},0.21052631578947367,0.038351539543994194,0.22807017543859648,0.03947152782669415,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_electrical_engineering,0.32413793103448274,0.03900432069185553,"acc_norm,none",6857302016,0.0,{{answer}},0.2896551724137931,0.037800192304380156,0.32413793103448274,0.03900432069185553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_elementary_mathematics,0.30952380952380953,0.023809523809523864,"acc_norm,none",6857302016,0.0,{{answer}},0.29894179894179895,0.02357760479165581,0.30952380952380953,0.023809523809523864,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_formal_logic,0.38095238095238093,0.043435254289490986,"acc_norm,none",6857302016,0.0,{{answer}},0.3412698412698413,0.04240799327574923,0.38095238095238093,0.043435254289490986,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_global_facts,0.27,0.044619604333847394,"acc_norm,none",6857302016,0.0,{{answer}},0.29,0.045604802157206845,0.27,0.044619604333847394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_biology,0.432258064516129,0.02818173972001941,"acc_norm,none",6857302016,0.0,{{answer}},0.36451612903225805,0.027379871229943245,0.432258064516129,0.02818173972001941,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_chemistry,0.2512315270935961,0.030516530732694436,"acc_norm,none",6857302016,0.0,{{answer}},0.1921182266009852,0.027719315709614768,0.2512315270935961,0.030516530732694436,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_computer_science,0.37,0.04852365870939099,"acc_norm,none",6857302016,0.0,{{answer}},0.32,0.046882617226215034,0.37,0.04852365870939099,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_european_history,0.44242424242424244,0.03878372113711274,"acc_norm,none",6857302016,0.0,{{answer}},0.28484848484848485,0.03524390844511784,0.44242424242424244,0.03878372113711274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_geography,0.4292929292929293,0.03526552724601198,"acc_norm,none",6857302016,0.0,{{answer}},0.36363636363636365,0.03427308652999936,0.4292929292929293,0.03526552724601198,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_government_and_politics,0.47150259067357514,0.03602573571288442,"acc_norm,none",6857302016,0.0,{{answer}},0.41968911917098445,0.03561587327685884,0.47150259067357514,0.03602573571288442,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_macroeconomics,0.31794871794871793,0.02361088430892786,"acc_norm,none",6857302016,0.0,{{answer}},0.26666666666666666,0.02242127361292371,0.31794871794871793,0.02361088430892786,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_mathematics,0.2074074074074074,0.024720713193952155,"acc_norm,none",6857302016,0.0,{{answer}},0.15925925925925927,0.02231039463004063,0.2074074074074074,0.024720713193952155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_microeconomics,0.37815126050420167,0.031499305777849054,"acc_norm,none",6857302016,0.0,{{answer}},0.3067226890756303,0.029953823891887044,0.37815126050420167,0.031499305777849054,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_physics,0.26490066225165565,0.03603038545360384,"acc_norm,none",6857302016,0.0,{{answer}},0.31125827814569534,0.03780445850526732,0.26490066225165565,0.03603038545360384,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_psychology,0.45137614678899085,0.02133571471126879,"acc_norm,none",6857302016,0.0,{{answer}},0.4935779816513762,0.021435554820013077,0.45137614678899085,0.02133571471126879,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_statistics,0.28703703703703703,0.030851992993257013,"acc_norm,none",6857302016,0.0,{{answer}},0.2962962962962963,0.031141447823536048,0.28703703703703703,0.030851992993257013,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_us_history,0.39215686274509803,0.03426712349247271,"acc_norm,none",6857302016,0.0,{{answer}},0.3431372549019608,0.033321399446680854,0.39215686274509803,0.03426712349247271,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_world_history,0.3670886075949367,0.031376240725616185,"acc_norm,none",6857302016,0.0,{{answer}},0.33755274261603374,0.03078154910202621,0.3670886075949367,0.031376240725616185,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_human_aging,0.36771300448430494,0.03236198350928275,"acc_norm,none",6857302016,0.0,{{answer}},0.3901345291479821,0.03273766725459156,0.36771300448430494,0.03236198350928275,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_human_sexuality,0.37404580152671757,0.042438692422305246,"acc_norm,none",6857302016,0.0,{{answer}},0.4351145038167939,0.04348208051644858,0.37404580152671757,0.042438692422305246,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_international_law,0.33884297520661155,0.043207678075366705,"acc_norm,none",6857302016,0.0,{{answer}},0.2396694214876033,0.03896878985070416,0.33884297520661155,0.043207678075366705,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_jurisprudence,0.39814814814814814,0.04732332615978813,"acc_norm,none",6857302016,0.0,{{answer}},0.25925925925925924,0.04236511258094631,0.39814814814814814,0.04732332615978813,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_logical_fallacies,0.38650306748466257,0.038258255488486076,"acc_norm,none",6857302016,0.0,{{answer}},0.3558282208588957,0.03761521380046734,0.38650306748466257,0.038258255488486076,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_machine_learning,0.25,0.04109974682633932,"acc_norm,none",6857302016,0.0,{{answer}},0.30357142857142855,0.04364226155841044,0.25,0.04109974682633932,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_management,0.4563106796116505,0.04931801994220414,"acc_norm,none",6857302016,0.0,{{answer}},0.3106796116504854,0.0458212416016155,0.4563106796116505,0.04931801994220414,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_marketing,0.5042735042735043,0.032754892643821316,"acc_norm,none",6857302016,0.0,{{answer}},0.5,0.03275608910402091,0.5042735042735043,0.032754892643821316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_medical_genetics,0.48,0.050211673156867795,"acc_norm,none",6857302016,0.0,{{answer}},0.38,0.048783173121456316,0.48,0.050211673156867795,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_miscellaneous,0.46871008939974457,0.017844918090468544,"acc_norm,none",6857302016,0.0,{{answer}},0.5057471264367817,0.01787878232612923,0.46871008939974457,0.017844918090468544,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_moral_disputes,0.28034682080924855,0.02418242749657762,"acc_norm,none",6857302016,0.0,{{answer}},0.3063583815028902,0.024818350129436593,0.28034682080924855,0.02418242749657762,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_moral_scenarios,0.27262569832402234,0.014893391735249603,"acc_norm,none",6857302016,0.0,{{answer}},0.23798882681564246,0.014242630070574885,0.27262569832402234,0.014893391735249603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_nutrition,0.3954248366013072,0.02799672318063146,"acc_norm,none",6857302016,0.0,{{answer}},0.3006535947712418,0.026256053835718964,0.3954248366013072,0.02799672318063146,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_philosophy,0.3858520900321543,0.02764814959975147,"acc_norm,none",6857302016,0.0,{{answer}},0.3408360128617363,0.026920841260776162,0.3858520900321543,0.02764814959975147,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_prehistory,0.37962962962962965,0.027002521034516468,"acc_norm,none",6857302016,0.0,{{answer}},0.42901234567901236,0.027538925613470863,0.37962962962962965,0.027002521034516468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_professional_accounting,0.22695035460992907,0.024987106365642976,"acc_norm,none",6857302016,0.0,{{answer}},0.2624113475177305,0.026244920349843007,0.22695035460992907,0.024987106365642976,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_professional_law,0.2966101694915254,0.011665946586082849,"acc_norm,none",6857302016,0.0,{{answer}},0.26988265971316816,0.01133738108425041,0.2966101694915254,0.011665946586082849,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_professional_medicine,0.33088235294117646,0.02858270975389842,"acc_norm,none",6857302016,0.0,{{answer}},0.3088235294117647,0.028064998167040094,0.33088235294117646,0.02858270975389842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_professional_psychology,0.31862745098039214,0.018850084696468712,"acc_norm,none",6857302016,0.0,{{answer}},0.32189542483660133,0.018901015322093092,0.31862745098039214,0.018850084696468712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_public_relations,0.3090909090909091,0.044262946482000985,"acc_norm,none",6857302016,0.0,{{answer}},0.45454545454545453,0.04769300568972744,0.3090909090909091,0.044262946482000985,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_security_studies,0.2979591836734694,0.029279567411065667,"acc_norm,none",6857302016,0.0,{{answer}},0.3224489795918367,0.029923100563683913,0.2979591836734694,0.029279567411065667,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_sociology,0.3582089552238806,0.03390393042268815,"acc_norm,none",6857302016,0.0,{{answer}},0.3034825870646766,0.03251006816458619,0.3582089552238806,0.03390393042268815,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_us_foreign_policy,0.34,0.04760952285695236,"acc_norm,none",6857302016,0.0,{{answer}},0.37,0.04852365870939099,0.34,0.04760952285695236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_virology,0.3674698795180723,0.03753267402120574,"acc_norm,none",6857302016,0.0,{{answer}},0.26506024096385544,0.03436024037944967,0.3674698795180723,0.03753267402120574,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_world_religions,0.5730994152046783,0.03793620616529916,"acc_norm,none",6857302016,0.0,{{answer}},0.49707602339181284,0.03834759370936839,0.5730994152046783,0.03793620616529916,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_econometrics,0.2719298245614035,0.04185774424022057,"acc,none",6857302016,5.0,answer,0.2719298245614035,0.04185774424022057,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_electrical_engineering,0.2620689655172414,0.036646663372252565,"acc,none",6857302016,5.0,answer,0.2620689655172414,0.036646663372252565,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_elementary_mathematics,0.291005291005291,0.023393826500484865,"acc,none",6857302016,5.0,answer,0.291005291005291,0.023393826500484865,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_formal_logic,0.15873015873015872,0.03268454013011744,"acc,none",6857302016,5.0,answer,0.15873015873015872,0.03268454013011744,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_global_facts,0.32,0.04688261722621505,"acc,none",6857302016,5.0,answer,0.32,0.04688261722621505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_biology,0.25483870967741934,0.0247901184593322,"acc,none",6857302016,5.0,answer,0.25483870967741934,0.0247901184593322,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_chemistry,0.2512315270935961,0.030516530732694436,"acc,none",6857302016,5.0,answer,0.2512315270935961,0.030516530732694436,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_computer_science,0.32,0.046882617226215034,"acc,none",6857302016,5.0,answer,0.32,0.046882617226215034,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_european_history,0.24242424242424243,0.03346409881055953,"acc,none",6857302016,5.0,answer,0.24242424242424243,0.03346409881055953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_geography,0.24242424242424243,0.030532892233932036,"acc,none",6857302016,5.0,answer,0.24242424242424243,0.030532892233932036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_government_and_politics,0.23316062176165803,0.03051611137147601,"acc,none",6857302016,5.0,answer,0.23316062176165803,0.03051611137147601,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_macroeconomics,0.32051282051282054,0.023661296393964283,"acc,none",6857302016,5.0,answer,0.32051282051282054,0.023661296393964283,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_mathematics,0.24074074074074073,0.026067159222275798,"acc,none",6857302016,5.0,answer,0.24074074074074073,0.026067159222275798,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_microeconomics,0.23109243697478993,0.027381406927868952,"acc,none",6857302016,5.0,answer,0.23109243697478993,0.027381406927868952,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_physics,0.2582781456953642,0.035737053147634576,"acc,none",6857302016,5.0,answer,0.2582781456953642,0.035737053147634576,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_psychology,0.23853211009174313,0.01827257581023186,"acc,none",6857302016,5.0,answer,0.23853211009174313,0.01827257581023186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_statistics,0.35185185185185186,0.03256850570293647,"acc,none",6857302016,5.0,answer,0.35185185185185186,0.03256850570293647,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_us_history,0.25980392156862747,0.030778554678693254,"acc,none",6857302016,5.0,answer,0.25980392156862747,0.030778554678693254,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_world_history,0.2616033755274262,0.028609516716994927,"acc,none",6857302016,5.0,answer,0.2616033755274262,0.028609516716994927,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_human_aging,0.3004484304932735,0.03076935200822914,"acc,none",6857302016,5.0,answer,0.3004484304932735,0.03076935200822914,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_human_sexuality,0.2366412213740458,0.03727673575596919,"acc,none",6857302016,5.0,answer,0.2366412213740458,0.03727673575596919,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_humanities,0.25313496280552605,0.006332817760876259,"acc,none",6857302016,,,0.25313496280552605,0.006332817760876259,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_international_law,0.38016528925619836,0.04431324501968432,"acc,none",6857302016,5.0,answer,0.38016528925619836,0.04431324501968432,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_jurisprudence,0.28703703703703703,0.043733130409147614,"acc,none",6857302016,5.0,answer,0.28703703703703703,0.043733130409147614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_logical_fallacies,0.26993865030674846,0.03487825168497892,"acc,none",6857302016,5.0,answer,0.26993865030674846,0.03487825168497892,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_machine_learning,0.2767857142857143,0.04246624336697625,"acc,none",6857302016,5.0,answer,0.2767857142857143,0.04246624336697625,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_management,0.2912621359223301,0.04498676320572922,"acc,none",6857302016,5.0,answer,0.2912621359223301,0.04498676320572922,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_marketing,0.23076923076923078,0.02760192138141759,"acc,none",6857302016,5.0,answer,0.23076923076923078,0.02760192138141759,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_medical_genetics,0.22,0.04163331998932268,"acc,none",6857302016,5.0,answer,0.22,0.04163331998932268,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_miscellaneous,0.28607918263090676,0.016160871405127532,"acc,none",6857302016,5.0,answer,0.28607918263090676,0.016160871405127532,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_moral_disputes,0.26011560693641617,0.023618678310069374,"acc,none",6857302016,5.0,answer,0.26011560693641617,0.023618678310069374,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_moral_scenarios,0.24692737430167597,0.014422292204808838,"acc,none",6857302016,5.0,answer,0.24692737430167597,0.014422292204808838,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_nutrition,0.2581699346405229,0.025058503316958154,"acc,none",6857302016,5.0,answer,0.2581699346405229,0.025058503316958154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_other,0.27325394271000963,0.007988634722887952,"acc,none",6857302016,,,0.27325394271000963,0.007988634722887952,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_philosophy,0.2765273311897106,0.02540383297817961,"acc,none",6857302016,5.0,answer,0.2765273311897106,0.02540383297817961,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_prehistory,0.25925925925925924,0.02438366553103545,"acc,none",6857302016,5.0,answer,0.25925925925925924,0.02438366553103545,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_professional_accounting,0.30851063829787234,0.02755336616510137,"acc,none",6857302016,5.0,answer,0.30851063829787234,0.02755336616510137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_professional_law,0.23728813559322035,0.010865436690780257,"acc,none",6857302016,5.0,answer,0.23728813559322035,0.010865436690780257,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_professional_medicine,0.27941176470588236,0.02725720260611495,"acc,none",6857302016,5.0,answer,0.27941176470588236,0.02725720260611495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_professional_psychology,0.272875816993464,0.01802047414839358,"acc,none",6857302016,5.0,answer,0.272875816993464,0.01802047414839358,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_public_relations,0.35454545454545455,0.045820048415054174,"acc,none",6857302016,5.0,answer,0.35454545454545455,0.045820048415054174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_security_studies,0.17551020408163265,0.024352800722970015,"acc,none",6857302016,5.0,answer,0.17551020408163265,0.024352800722970015,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_social_sciences,0.257393565160871,0.007863516247594533,"acc,none",6857302016,,,0.257393565160871,0.007863516247594533,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_sociology,0.25870646766169153,0.03096590312357303,"acc,none",6857302016,5.0,answer,0.25870646766169153,0.03096590312357303,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_stem,0.2781477957500793,0.00797663241870028,"acc,none",6857302016,,,0.2781477957500793,0.00797663241870028,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_us_foreign_policy,0.26,0.04408440022768077,"acc,none",6857302016,5.0,answer,0.26,0.04408440022768077,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_virology,0.3253012048192771,0.03647168523683227,"acc,none",6857302016,5.0,answer,0.3253012048192771,0.03647168523683227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_world_religions,0.29239766081871343,0.034886477134579215,"acc,none",6857302016,5.0,answer,0.29239766081871343,0.034886477134579215,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,other,0.3826842613453492,0.008569963961337869,"acc,none",6857302016,,,0.3826842613453492,0.008569963961337869,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,social sciences,0.3587910302242444,0.00853650897578099,"acc,none",6857302016,,,0.3587910302242444,0.00853650897578099,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,stem,0.291151284490961,0.008020157440186268,"acc,none",6857302016,,,0.291151284490961,0.008020157440186268,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,truthfulqa_gen,0.2692778457772338,0.015528566637087267,"rouge1_acc,none",6857302016,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,24.63010173546932,0.7421573982715609,0.2802937576499388,0.015723139524608774,-8.847440755540575,0.7895947800599356,50.582545922869016,0.8275122047019977,0.2692778457772338,0.015528566637087267,-10.822346464064593,0.8350764254792622,33.52358105770809,0.958380497583704,0.22276621787025705,0.014566506961396742,-13.284869933945387,1.0121558045387418,47.64422658278255,0.8388993412931399,0.25458996328029376,0.015250117079156508,-11.12778948183462,0.8519785160659245
EleutherAI/pythia-6.9b,299892736000,openllm,truthfulqa_mc1,0.21909424724602203,0.01448003857875746,"acc,none",6857302016,0.0,0,0.21909424724602203,0.01448003857875746,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,truthfulqa_mc2,0.35137357991296697,0.013616200377976177,"acc,none",6857302016,0.0,0,0.35137357991296697,0.013616200377976177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,winogrande,0.648776637726914,0.013415981370545128,"acc,none",6857302016,5.0,,0.648776637726914,0.013415981370545128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_compréhension_de_la_date,0.208,0.025721398901416396,"acc_norm,none",7069016064,3.0,{{target}},,,0.208,0.025721398901416396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_compréhension_des_sports,0.54,0.031584653891499,"acc_norm,none",7069016064,3.0,{{target}},,,0.54,0.031584653891499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_comptage_d_objets,0.096,0.018668961419477187,"acc_norm,none",7069016064,3.0,{{target}},,,0.096,0.018668961419477187,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_cinq_objets,0.184,0.02455581299422256,"acc_norm,none",7069016064,3.0,{{target}},,,0.184,0.02455581299422256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_sept_objets,0.152,0.022752024491765468,"acc_norm,none",7069016064,3.0,{{target}},,,0.152,0.022752024491765468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_trois_objets,0.336,0.029933259094191516,"acc_norm,none",7069016064,3.0,{{target}},,,0.336,0.029933259094191516,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_désambiguïsation_qa,0.3,0.029040893477575862,"acc_norm,none",7069016064,3.0,{{target}},,,0.3,0.029040893477575862,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_expressions_booléennes,0.46,0.031584653891499004,"acc_norm,none",7069016064,3.0,{{target}},,,0.46,0.031584653891499004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_formes_géométriques,0.112,0.019985536939171444,"acc_norm,none",7069016064,3.0,{{target}},,,0.112,0.019985536939171444,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_hyperbate,0.516,0.03166998503010742,"acc_norm,none",7069016064,3.0,{{target}},,,0.516,0.03166998503010742,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_jugement_causal,0.5187165775401069,0.03663608375537842,"acc_norm,none",7069016064,3.0,{{target}},,,0.5187165775401069,0.03663608375537842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_naviguer,0.472,0.03163648953154439,"acc_norm,none",7069016064,3.0,{{target}},,,0.472,0.03163648953154439,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_pingouins_sur_une_table,0.2328767123287671,0.03510036341139227,"acc_norm,none",7069016064,3.0,{{target}},,,0.2328767123287671,0.03510036341139227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_raisonnement_sur_les_objets_colorés,0.148,0.022503547243806144,"acc_norm,none",7069016064,3.0,{{target}},,,0.148,0.022503547243806144,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_recommandation_de_film,0.248,0.027367497504863548,"acc_norm,none",7069016064,3.0,{{target}},,,0.248,0.027367497504863548,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_sarcasmes,0.550561797752809,0.03738964966056965,"acc_norm,none",7069016064,3.0,{{target}},,,0.550561797752809,0.03738964966056965,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_sophismes_formels,0.468,0.031621252575725504,"acc_norm,none",7069016064,3.0,{{target}},,,0.468,0.031621252575725504,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_cinq_objets,0.2,0.025348970020979078,"acc_norm,none",7069016064,3.0,{{target}},,,0.2,0.025348970020979078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_sept_objets,0.152,0.022752024491765468,"acc_norm,none",7069016064,3.0,{{target}},,,0.152,0.022752024491765468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_trois_objets,0.308,0.029256928606501864,"acc_norm,none",7069016064,3.0,{{target}},,,0.308,0.029256928606501864,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_séquences_temporelles,0.256,0.02765710871820491,"acc_norm,none",7069016064,3.0,{{target}},,,0.256,0.02765710871820491,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_toile_de_mensonges,0.456,0.03156328506121339,"acc_norm,none",7069016064,3.0,{{target}},,,0.456,0.03156328506121339,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_gpqa_diamond_fr,0.28426395939086296,0.032218796071824624,"acc_norm,none",7069016064,0.0,answer,,,0.28426395939086296,0.032218796071824624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_gpqa_extended_fr,0.2578268876611418,0.018789579807329218,"acc_norm,none",7069016064,0.0,answer,,,0.2578268876611418,0.018789579807329218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_gpqa_main_fr,0.2331002331002331,0.020437076579903176,"acc_norm,none",7069016064,0.0,answer,,,0.2331002331002331,0.020437076579903176,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_ifeval_fr,0.009708737864077669,0.004324952097790176,"prompt_level_loose_acc,none",7069016064,0.0,0,,,,,,,,,,,,,0.013592233009708738,0.005107308452923588,0.08453922315308454,N/A,0.009708737864077669,0.004324952097790176,0.08606245239908607,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_algebra_hard_fr,0.0,0.0,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_counting_and_prob_hard_fr,0.0,0.0,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_geometry_hard_fr,0.0,0.0,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_num_theory_hard_fr,0.004608294930875576,0.004608294930875572,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.004608294930875576,0.004608294930875572,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_prealgebra_hard_fr,0.0,0.0,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_precalculus_hard_fr,0.0,0.0,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_mmlu_fr,0.26299672411337416,0.0037154451893209877,"acc,none",7069016064,5.0,Answer,0.26299672411337416,0.0037154451893209877,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_musr_murder_mysteries_fr,0.5,0.031686212526223896,"acc_norm,none",7069016064,0.0,{{answer_choice}},,,0.5,0.031686212526223896,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_musr_object_placements_fr,0.25,0.02711630722733202,"acc_norm,none",7069016064,0.0,{{answer_choice}},,,0.25,0.02711630722733202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_musr_team_allocation_fr,0.252,0.02751385193303136,"acc_norm,none",7069016064,0.0,{{answer_choice}},,,0.252,0.02751385193303136,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench,french_bench_arc_challenge,0.3712574850299401,0.014136848432206825,"acc_norm,none",7069016064,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.31479897348160824,0.01358952368253817,0.3712574850299401,0.014136848432206825,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench,french_bench_grammar,0.8235294117647058,0.03509414936549958,"acc,none",7069016064,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.8235294117647058,0.03509414936549958,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench,french_bench_hellaswag,0.5739987149282502,0.005117492740296177,"acc_norm,none",7069016064,5.0,{{label}},0.4281430713214821,0.00512076062053857,0.5739987149282502,0.005117492740296177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench,french_bench_vocab,0.7899159663865546,0.03750126918012132,"acc,none",7069016064,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7899159663865546,0.03750126918012132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench_gen,french_bench_fquadv2_genq,0.225548205370323,0.0,"rouge1,none",7069016064,5.0,{{question}},,,,,0.225548205370323,N/A,0.19100956522380838,0.008865356603873258,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench_gen,french_bench_fquadv2_hasAns,0.6320749460209172,0.0,"rouge1,none",7069016064,5.0,{% if answers.text| length > 0 %}{{answers.text[0]}}{% else %}{{['Impossible']}}{% endif %},,,,,0.6320749460209172,N/A,0.615987946884996,0.019214447966658863,0.3675,0.024136399679191754,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench_gen,french_bench_multifquad,0.5288150672066294,0.0,"rouge1,none",7069016064,5.0,"{{', '.join(answers.text)}}",,,,,0.5288150672066294,N/A,0.5144646807578526,0.01596414206549759,0.0775,0.01338590044887033,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench_gen,french_bench_orangesum_abstract,0.2253368277538288,0.0,"rouge1,none",7069016064,5.0,{{summary}},,,,,0.2253368277538288,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench_gen,french_bench_trivia,0.5488751292567084,0.0,"rouge1,none",7069016064,5.0,{{Answer}},,,,,0.5488751292567084,N/A,0.5376839826839829,0.023571421106675133,0.4394736842105263,0.025494402360263287,0.41842105263157897,0.025339118200992206,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_compréhension_de_la_date,0.188,0.02476037772775051,"acc_norm,none",1345423360,3.0,{{target}},,,0.188,0.02476037772775051,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_compréhension_des_sports,0.46,0.031584653891499,"acc_norm,none",1345423360,3.0,{{target}},,,0.46,0.031584653891499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_comptage_d_objets,0.068,0.01595374841074702,"acc_norm,none",1345423360,3.0,{{target}},,,0.068,0.01595374841074702,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_cinq_objets,0.22,0.026251792824605845,"acc_norm,none",1345423360,3.0,{{target}},,,0.22,0.026251792824605845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_sept_objets,0.136,0.021723342617052065,"acc_norm,none",1345423360,3.0,{{target}},,,0.136,0.021723342617052065,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_trois_objets,0.36,0.030418764025174978,"acc_norm,none",1345423360,3.0,{{target}},,,0.36,0.030418764025174978,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_désambiguïsation_qa,0.372,0.030630325944558313,"acc_norm,none",1345423360,3.0,{{target}},,,0.372,0.030630325944558313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_expressions_booléennes,0.46,0.031584653891499004,"acc_norm,none",1345423360,3.0,{{target}},,,0.46,0.031584653891499004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_formes_géométriques,0.084,0.017578738526776324,"acc_norm,none",1345423360,3.0,{{target}},,,0.084,0.017578738526776324,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_hyperbate,0.436,0.03142556706028128,"acc_norm,none",1345423360,3.0,{{target}},,,0.436,0.03142556706028128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_jugement_causal,0.5187165775401069,0.03663608375537842,"acc_norm,none",1345423360,3.0,{{target}},,,0.5187165775401069,0.03663608375537842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_naviguer,0.42,0.03127799950463662,"acc_norm,none",1345423360,3.0,{{target}},,,0.42,0.03127799950463662,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_pingouins_sur_une_table,0.2534246575342466,0.03612245461624573,"acc_norm,none",1345423360,3.0,{{target}},,,0.2534246575342466,0.03612245461624573,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_raisonnement_sur_les_objets_colorés,0.168,0.023692813205492585,"acc_norm,none",1345423360,3.0,{{target}},,,0.168,0.023692813205492585,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_recommandation_de_film,0.228,0.026587432487268494,"acc_norm,none",1345423360,3.0,{{target}},,,0.228,0.026587432487268494,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_sarcasmes,0.46629213483146065,0.03749680060368987,"acc_norm,none",1345423360,3.0,{{target}},,,0.46629213483146065,0.03749680060368987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_sophismes_formels,0.46,0.031584653891499004,"acc_norm,none",1345423360,3.0,{{target}},,,0.46,0.031584653891499004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_cinq_objets,0.192,0.024960691989172005,"acc_norm,none",1345423360,3.0,{{target}},,,0.192,0.024960691989172005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_sept_objets,0.148,0.022503547243806148,"acc_norm,none",1345423360,3.0,{{target}},,,0.148,0.022503547243806148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_trois_objets,0.328,0.029752391824475373,"acc_norm,none",1345423360,3.0,{{target}},,,0.328,0.029752391824475373,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_séquences_temporelles,0.204,0.025537121574548148,"acc_norm,none",1345423360,3.0,{{target}},,,0.204,0.025537121574548148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_toile_de_mensonges,0.512,0.03167708558254708,"acc_norm,none",1345423360,3.0,{{target}},,,0.512,0.03167708558254708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_gpqa_diamond_fr,0.27411167512690354,0.03186182418247317,"acc_norm,none",1345423360,0.0,answer,,,0.27411167512690354,0.03186182418247317,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_gpqa_extended_fr,0.2504604051565378,0.01861089187330517,"acc_norm,none",1345423360,0.0,answer,,,0.2504604051565378,0.01861089187330517,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_gpqa_main_fr,0.2540792540792541,0.021043068137727102,"acc_norm,none",1345423360,0.0,answer,,,0.2540792540792541,0.021043068137727102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_ifeval_fr,0.015533980582524271,0.0054545657935066464,"prompt_level_loose_acc,none",1345423360,0.0,0,,,,,,,,,,,,,0.019417475728155338,0.006086349682701512,0.12871287128712872,N/A,0.015533980582524271,0.0054545657935066464,0.1210967250571211,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_algebra_hard_fr,0.011428571428571429,0.005689672739661832,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.011428571428571429,0.005689672739661832,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_counting_and_prob_hard_fr,0.01020408163265306,0.007196850575679085,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.01020408163265306,0.007196850575679085,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_geometry_hard_fr,0.0,0.0,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_num_theory_hard_fr,0.0,0.0,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_prealgebra_hard_fr,0.0,0.0,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_precalculus_hard_fr,0.0,0.0,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_mmlu_fr,0.25210084033613445,0.0036644571663738473,"acc,none",1345423360,5.0,Answer,0.25210084033613445,0.0036644571663738473,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_musr_murder_mysteries_fr,0.508,0.031682156431413803,"acc_norm,none",1345423360,0.0,{{answer_choice}},,,0.508,0.031682156431413803,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_musr_object_placements_fr,0.23828125,0.026679160987075002,"acc_norm,none",1345423360,0.0,{{answer_choice}},,,0.23828125,0.026679160987075002,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_musr_team_allocation_fr,0.236,0.02690933759495384,"acc_norm,none",1345423360,0.0,{{answer_choice}},,,0.236,0.02690933759495384,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,french_bench,french_bench_arc_challenge,0.3105218135158255,0.013538947396873264,"acc_norm,none",1345423360,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.2557741659538067,0.012766130743681107,0.3105218135158255,0.013538947396873264,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,french_bench,french_bench_grammar,0.7983193277310925,0.03693851725228281,"acc,none",1345423360,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7983193277310925,0.03693851725228281,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,french_bench,french_bench_hellaswag,0.5071749839366031,0.005173942584570164,"acc_norm,none",1345423360,5.0,{{label}},0.39408866995073893,0.005057056803697162,0.5071749839366031,0.005173942584570164,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,french_bench,french_bench_vocab,0.7815126050420168,0.03803997152889484,"acc,none",1345423360,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7815126050420168,0.03803997152889484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,arc_de,0.2557741659538067,0.012766130743681098,"acc_norm,none",1345423360,25.0,gold,0.1924721984602224,0.01153563015540481,0.2557741659538067,0.012766130743681098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,arc_es,0.2564102564102564,0.012771065618749026,"acc_norm,none",1345423360,25.0,gold,0.2111111111111111,0.011935928534109866,0.2564102564102564,0.012771065618749026,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,arc_fr,0.31137724550898205,0.013549170237200151,"acc_norm,none",1345423360,25.0,gold,0.2660393498716852,0.012929683850700155,0.31137724550898205,0.013549170237200151,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,arc_it,0.262617621899059,0.012876175520452837,"acc_norm,none",1345423360,25.0,gold,0.20102651839178784,0.011726581781869408,0.262617621899059,0.012876175520452837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,m_mmlu_de,0.2481520591341077,0.0037514686438599775,"acc,none",1345423360,25.0,answer,0.2481520591341077,0.0037514686438599775,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,m_mmlu_es,0.23953802309884506,0.003696256004644125,"acc,none",1345423360,25.0,answer,0.23953802309884506,0.003696256004644125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,m_mmlu_fr,0.25796348636467803,0.0038240356620502457,"acc,none",1345423360,25.0,answer,0.25796348636467803,0.0038240356620502457,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,m_mmlu_it,0.2433330815139382,0.0037297057684196814,"acc,none",1345423360,25.0,answer,0.2433330815139382,0.0037297057684196814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,arc_de,0.2557741659538067,0.012766130743681098,"acc_norm,none",1345423360,25.0,gold,0.1924721984602224,0.01153563015540481,0.2557741659538067,0.012766130743681098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,arc_es,0.2564102564102564,0.012771065618749026,"acc_norm,none",1345423360,25.0,gold,0.2111111111111111,0.011935928534109866,0.2564102564102564,0.012771065618749026,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,arc_fr,0.31137724550898205,0.013549170237200151,"acc_norm,none",1345423360,25.0,gold,0.2660393498716852,0.012929683850700155,0.31137724550898205,0.013549170237200151,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,arc_it,0.262617621899059,0.012876175520452837,"acc_norm,none",1345423360,25.0,gold,0.20102651839178784,0.011726581781869408,0.262617621899059,0.012876175520452837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,m_mmlu_de,0.2481520591341077,0.0037514686438599775,"acc,none",1345423360,25.0,answer,0.2481520591341077,0.0037514686438599775,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,m_mmlu_es,0.23953802309884506,0.003696256004644125,"acc,none",1345423360,25.0,answer,0.23953802309884506,0.003696256004644125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,m_mmlu_fr,0.25796348636467803,0.0038240356620502457,"acc,none",1345423360,25.0,answer,0.25796348636467803,0.0038240356620502457,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,m_mmlu_it,0.2433330815139382,0.0037297057684196814,"acc,none",1345423360,25.0,answer,0.2433330815139382,0.0037297057684196814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,arc_challenge,0.3174061433447099,0.01360223908803817,"acc_norm,none",1345423360,25.0,{{choices.label.index(answerKey)}},0.2713310580204778,0.0129938077275458,0.3174061433447099,0.01360223908803817,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,gsm8k,0.021986353297952996,0.0040391627581100615,"exact_match,flexible-extract",1345423360,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.006065200909780136,0.0021386703014604656,0.021986353297952996,0.0040391627581100615,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,hellaswag,0.5404301931886079,0.004973442060741631,"acc_norm,none",1345423360,10.0,{{label}},0.41047600079665403,0.004909148239488267,0.5404301931886079,0.004973442060741631,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,humanities,0.2680127523910733,0.006434959535282622,"acc,none",1345423360,,,0.2680127523910733,0.006434959535282622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu,0.25160233584959407,0.0036576787872109997,"acc,none",1345423360,,,0.25160233584959407,0.0036576787872109997,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_abstract_algebra,0.34,0.04760952285695235,"acc,none",1345423360,5.0,answer,0.34,0.04760952285695235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_anatomy,0.28888888888888886,0.0391545063041425,"acc,none",1345423360,5.0,answer,0.28888888888888886,0.0391545063041425,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_astronomy,0.19736842105263158,0.03238981601699397,"acc,none",1345423360,5.0,answer,0.19736842105263158,0.03238981601699397,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_business_ethics,0.23,0.04229525846816506,"acc,none",1345423360,5.0,answer,0.23,0.04229525846816506,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_clinical_knowledge,0.25660377358490566,0.026880647889051982,"acc,none",1345423360,5.0,answer,0.25660377358490566,0.026880647889051982,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_biology,0.25,0.03621034121889507,"acc,none",1345423360,5.0,answer,0.25,0.03621034121889507,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_chemistry,0.22,0.04163331998932269,"acc,none",1345423360,5.0,answer,0.22,0.04163331998932269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_computer_science,0.41,0.049431107042371025,"acc,none",1345423360,5.0,answer,0.41,0.049431107042371025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_mathematics,0.31,0.04648231987117316,"acc,none",1345423360,5.0,answer,0.31,0.04648231987117316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_medicine,0.24277456647398843,0.0326926380614177,"acc,none",1345423360,5.0,answer,0.24277456647398843,0.0326926380614177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_physics,0.2549019607843137,0.043364327079931785,"acc,none",1345423360,5.0,answer,0.2549019607843137,0.043364327079931785,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_computer_security,0.24,0.04292346959909284,"acc,none",1345423360,5.0,answer,0.24,0.04292346959909284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_conceptual_physics,0.2851063829787234,0.029513196625539355,"acc,none",1345423360,5.0,answer,0.2851063829787234,0.029513196625539355,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation,0.2940464321321749,0.0038097993776527953,"acc,none",1345423360,,,0.2940464321321749,0.0038097993776527953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_abstract_algebra,0.18,0.038612291966536955,"acc_norm,none",1345423360,0.0,{{answer}},0.22,0.041633319989322674,0.18,0.038612291966536955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_anatomy,0.2962962962962963,0.03944624162501116,"acc_norm,none",1345423360,0.0,{{answer}},0.3111111111111111,0.03999262876617723,0.2962962962962963,0.03944624162501116,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_astronomy,0.3881578947368421,0.03965842097512744,"acc_norm,none",1345423360,0.0,{{answer}},0.2894736842105263,0.036906779861372814,0.3881578947368421,0.03965842097512744,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_business_ethics,0.44,0.04988876515698589,"acc_norm,none",1345423360,0.0,{{answer}},0.52,0.050211673156867795,0.44,0.04988876515698589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_clinical_knowledge,0.3320754716981132,0.028985455652334395,"acc_norm,none",1345423360,0.0,{{answer}},0.2679245283018868,0.027257260322494845,0.3320754716981132,0.028985455652334395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_biology,0.3194444444444444,0.03899073687357335,"acc_norm,none",1345423360,0.0,{{answer}},0.3194444444444444,0.038990736873573344,0.3194444444444444,0.03899073687357335,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_chemistry,0.25,0.04351941398892446,"acc_norm,none",1345423360,0.0,{{answer}},0.25,0.04351941398892446,0.25,0.04351941398892446,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_computer_science,0.26,0.04408440022768079,"acc_norm,none",1345423360,0.0,{{answer}},0.28,0.04512608598542127,0.26,0.04408440022768079,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_mathematics,0.22,0.04163331998932269,"acc_norm,none",1345423360,0.0,{{answer}},0.16,0.03684529491774711,0.22,0.04163331998932269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_medicine,0.27167630057803466,0.03391750322321659,"acc_norm,none",1345423360,0.0,{{answer}},0.30057803468208094,0.03496101481191181,0.27167630057803466,0.03391750322321659,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_physics,0.2549019607843137,0.04336432707993177,"acc_norm,none",1345423360,0.0,{{answer}},0.2549019607843137,0.04336432707993177,0.2549019607843137,0.04336432707993177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_computer_security,0.37,0.048523658709391,"acc_norm,none",1345423360,0.0,{{answer}},0.34,0.047609522856952344,0.37,0.048523658709391,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_conceptual_physics,0.2680851063829787,0.028957342788342343,"acc_norm,none",1345423360,0.0,{{answer}},0.3404255319148936,0.030976692998534432,0.2680851063829787,0.028957342788342343,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_econometrics,0.2543859649122807,0.040969851398436716,"acc_norm,none",1345423360,0.0,{{answer}},0.21052631578947367,0.038351539543994194,0.2543859649122807,0.040969851398436716,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_electrical_engineering,0.27586206896551724,0.037245636197746325,"acc_norm,none",1345423360,0.0,{{answer}},0.2689655172413793,0.036951833116502325,0.27586206896551724,0.037245636197746325,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_elementary_mathematics,0.25132275132275134,0.022340482339643895,"acc_norm,none",1345423360,0.0,{{answer}},0.23015873015873015,0.021679219663693138,0.25132275132275134,0.022340482339643895,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_formal_logic,0.23809523809523808,0.038095238095238106,"acc_norm,none",1345423360,0.0,{{answer}},0.2857142857142857,0.0404061017820884,0.23809523809523808,0.038095238095238106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_global_facts,0.28,0.04512608598542127,"acc_norm,none",1345423360,0.0,{{answer}},0.34,0.04760952285695235,0.28,0.04512608598542127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_biology,0.31290322580645163,0.026377567028645858,"acc_norm,none",1345423360,0.0,{{answer}},0.29354838709677417,0.025906087021319295,0.31290322580645163,0.026377567028645858,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_chemistry,0.22660098522167488,0.02945486383529298,"acc_norm,none",1345423360,0.0,{{answer}},0.18719211822660098,0.027444924966882615,0.22660098522167488,0.02945486383529298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_computer_science,0.34,0.04760952285695235,"acc_norm,none",1345423360,0.0,{{answer}},0.28,0.04512608598542127,0.34,0.04760952285695235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_european_history,0.4484848484848485,0.038835659779569286,"acc_norm,none",1345423360,0.0,{{answer}},0.2727272727272727,0.0347769116216366,0.4484848484848485,0.038835659779569286,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_geography,0.3484848484848485,0.033948539651564025,"acc_norm,none",1345423360,0.0,{{answer}},0.3181818181818182,0.03318477333845331,0.3484848484848485,0.033948539651564025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_government_and_politics,0.40932642487046633,0.03548608168860806,"acc_norm,none",1345423360,0.0,{{answer}},0.32124352331606215,0.033699508685490674,0.40932642487046633,0.03548608168860806,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_macroeconomics,0.2717948717948718,0.02255655101013235,"acc_norm,none",1345423360,0.0,{{answer}},0.28205128205128205,0.02281581309889661,0.2717948717948718,0.02255655101013235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_mathematics,0.17037037037037037,0.02292255486307496,"acc_norm,none",1345423360,0.0,{{answer}},0.14814814814814814,0.02165977842211803,0.17037037037037037,0.02292255486307496,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_microeconomics,0.3319327731092437,0.030588697013783667,"acc_norm,none",1345423360,0.0,{{answer}},0.2605042016806723,0.028510251512341923,0.3319327731092437,0.030588697013783667,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_physics,0.271523178807947,0.03631329803969653,"acc_norm,none",1345423360,0.0,{{answer}},0.2582781456953642,0.035737053147634576,0.271523178807947,0.03631329803969653,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_psychology,0.3504587155963303,0.020456077599824457,"acc_norm,none",1345423360,0.0,{{answer}},0.3724770642201835,0.020728368457638494,0.3504587155963303,0.020456077599824457,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_statistics,0.2916666666666667,0.03099866630456053,"acc_norm,none",1345423360,0.0,{{answer}},0.26851851851851855,0.030225226160012407,0.2916666666666667,0.03099866630456053,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_us_history,0.36764705882352944,0.03384132045674118,"acc_norm,none",1345423360,0.0,{{answer}},0.3088235294117647,0.03242661719827218,0.36764705882352944,0.03384132045674118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_world_history,0.32489451476793246,0.030486039389105296,"acc_norm,none",1345423360,0.0,{{answer}},0.29535864978902954,0.029696338713422893,0.32489451476793246,0.030486039389105296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_human_aging,0.32286995515695066,0.03138147637575498,"acc_norm,none",1345423360,0.0,{{answer}},0.4170403587443946,0.03309266936071721,0.32286995515695066,0.03138147637575498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_human_sexuality,0.3816793893129771,0.042607351576445594,"acc_norm,none",1345423360,0.0,{{answer}},0.3893129770992366,0.04276486542814591,0.3816793893129771,0.042607351576445594,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_international_law,0.38016528925619836,0.04431324501968432,"acc_norm,none",1345423360,0.0,{{answer}},0.19008264462809918,0.035817969517092825,0.38016528925619836,0.04431324501968432,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_jurisprudence,0.3333333333333333,0.04557239513497751,"acc_norm,none",1345423360,0.0,{{answer}},0.24074074074074073,0.04133119440243839,0.3333333333333333,0.04557239513497751,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_logical_fallacies,0.3619631901840491,0.037757007291414416,"acc_norm,none",1345423360,0.0,{{answer}},0.294478527607362,0.03581165790474082,0.3619631901840491,0.037757007291414416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_machine_learning,0.30357142857142855,0.04364226155841044,"acc_norm,none",1345423360,0.0,{{answer}},0.22321428571428573,0.03952301967702511,0.30357142857142855,0.04364226155841044,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_management,0.36893203883495146,0.04777615181156739,"acc_norm,none",1345423360,0.0,{{answer}},0.3106796116504854,0.045821241601615506,0.36893203883495146,0.04777615181156739,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_marketing,0.42735042735042733,0.032408473935163266,"acc_norm,none",1345423360,0.0,{{answer}},0.44871794871794873,0.032583346493868806,0.42735042735042733,0.032408473935163266,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_medical_genetics,0.38,0.04878317312145632,"acc_norm,none",1345423360,0.0,{{answer}},0.27,0.0446196043338474,0.38,0.04878317312145632,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_miscellaneous,0.4112388250319285,0.017595971908056573,"acc_norm,none",1345423360,0.0,{{answer}},0.4342273307790549,0.01772458938967779,0.4112388250319285,0.017595971908056573,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_moral_disputes,0.2398843930635838,0.022989592543123567,"acc_norm,none",1345423360,0.0,{{answer}},0.2543352601156069,0.023445826276545546,0.2398843930635838,0.022989592543123567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_moral_scenarios,0.27262569832402234,0.014893391735249603,"acc_norm,none",1345423360,0.0,{{answer}},0.23798882681564246,0.014242630070574885,0.27262569832402234,0.014893391735249603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_nutrition,0.3464052287581699,0.02724561304721536,"acc_norm,none",1345423360,0.0,{{answer}},0.2777777777777778,0.025646863097137908,0.3464052287581699,0.02724561304721536,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_philosophy,0.2861736334405145,0.02567025924218894,"acc_norm,none",1345423360,0.0,{{answer}},0.2540192926045016,0.024723861504771696,0.2861736334405145,0.02567025924218894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_prehistory,0.2623456790123457,0.024477222856135114,"acc_norm,none",1345423360,0.0,{{answer}},0.3765432098765432,0.026959344518747787,0.2623456790123457,0.024477222856135114,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_professional_accounting,0.24113475177304963,0.025518731049537766,"acc_norm,none",1345423360,0.0,{{answer}},0.2872340425531915,0.026992199173064356,0.24113475177304963,0.025518731049537766,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_professional_law,0.26988265971316816,0.011337381084250408,"acc_norm,none",1345423360,0.0,{{answer}},0.24967405475880053,0.011054538377832329,0.26988265971316816,0.011337381084250408,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_professional_medicine,0.30514705882352944,0.0279715413701706,"acc_norm,none",1345423360,0.0,{{answer}},0.29044117647058826,0.027576468622740512,0.30514705882352944,0.0279715413701706,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_professional_psychology,0.29248366013071897,0.018403415710109797,"acc_norm,none",1345423360,0.0,{{answer}},0.2957516339869281,0.01846315413263281,0.29248366013071897,0.018403415710109797,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_public_relations,0.3090909090909091,0.044262946482000985,"acc_norm,none",1345423360,0.0,{{answer}},0.42727272727272725,0.04738198703545483,0.3090909090909091,0.044262946482000985,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_security_studies,0.2571428571428571,0.02797982353874455,"acc_norm,none",1345423360,0.0,{{answer}},0.3142857142857143,0.02971932942241748,0.2571428571428571,0.02797982353874455,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_sociology,0.2537313432835821,0.03076944496729601,"acc_norm,none",1345423360,0.0,{{answer}},0.27860696517412936,0.031700561834973086,0.2537313432835821,0.03076944496729601,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_us_foreign_policy,0.35,0.0479372485441102,"acc_norm,none",1345423360,0.0,{{answer}},0.3,0.046056618647183814,0.35,0.0479372485441102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_virology,0.3253012048192771,0.03647168523683227,"acc_norm,none",1345423360,0.0,{{answer}},0.25903614457831325,0.03410646614071856,0.3253012048192771,0.03647168523683227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_world_religions,0.4269005847953216,0.03793620616529918,"acc_norm,none",1345423360,0.0,{{answer}},0.38011695906432746,0.037229657413855394,0.4269005847953216,0.03793620616529918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_econometrics,0.21929824561403508,0.03892431106518753,"acc,none",1345423360,5.0,answer,0.21929824561403508,0.03892431106518753,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_electrical_engineering,0.25517241379310346,0.03632984052707842,"acc,none",1345423360,5.0,answer,0.25517241379310346,0.03632984052707842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_elementary_mathematics,0.24338624338624337,0.022101128787415422,"acc,none",1345423360,5.0,answer,0.24338624338624337,0.022101128787415422,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_formal_logic,0.1746031746031746,0.03395490020856112,"acc,none",1345423360,5.0,answer,0.1746031746031746,0.03395490020856112,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_global_facts,0.29,0.045604802157206845,"acc,none",1345423360,5.0,answer,0.29,0.045604802157206845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_biology,0.23870967741935484,0.024251071262208834,"acc,none",1345423360,5.0,answer,0.23870967741935484,0.024251071262208834,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_chemistry,0.2413793103448276,0.03010833071801162,"acc,none",1345423360,5.0,answer,0.2413793103448276,0.03010833071801162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_computer_science,0.34,0.04760952285695236,"acc,none",1345423360,5.0,answer,0.34,0.04760952285695236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_european_history,0.24242424242424243,0.03346409881055953,"acc,none",1345423360,5.0,answer,0.24242424242424243,0.03346409881055953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_geography,0.24242424242424243,0.030532892233932026,"acc,none",1345423360,5.0,answer,0.24242424242424243,0.030532892233932026,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_government_and_politics,0.18652849740932642,0.02811209121011746,"acc,none",1345423360,5.0,answer,0.18652849740932642,0.02811209121011746,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_macroeconomics,0.2153846153846154,0.020843034557462874,"acc,none",1345423360,5.0,answer,0.2153846153846154,0.020843034557462874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_mathematics,0.25555555555555554,0.026593939101844072,"acc,none",1345423360,5.0,answer,0.25555555555555554,0.026593939101844072,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_microeconomics,0.24789915966386555,0.028047967224176896,"acc,none",1345423360,5.0,answer,0.24789915966386555,0.028047967224176896,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_physics,0.26490066225165565,0.036030385453603826,"acc,none",1345423360,5.0,answer,0.26490066225165565,0.036030385453603826,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_psychology,0.25688073394495414,0.01873249292834245,"acc,none",1345423360,5.0,answer,0.25688073394495414,0.01873249292834245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_statistics,0.25,0.029531221160930918,"acc,none",1345423360,5.0,answer,0.25,0.029531221160930918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_us_history,0.24509803921568626,0.030190282453501954,"acc,none",1345423360,5.0,answer,0.24509803921568626,0.030190282453501954,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_world_history,0.2489451476793249,0.028146970599422644,"acc,none",1345423360,5.0,answer,0.2489451476793249,0.028146970599422644,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_human_aging,0.2914798206278027,0.030500283176545913,"acc,none",1345423360,5.0,answer,0.2914798206278027,0.030500283176545913,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_human_sexuality,0.2366412213740458,0.03727673575596918,"acc,none",1345423360,5.0,answer,0.2366412213740458,0.03727673575596918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_humanities,0.2512221041445271,0.006317885948846767,"acc,none",1345423360,,,0.2512221041445271,0.006317885948846767,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_international_law,0.3884297520661157,0.044492703500683836,"acc,none",1345423360,5.0,answer,0.3884297520661157,0.044492703500683836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_jurisprudence,0.3055555555555556,0.04453197507374984,"acc,none",1345423360,5.0,answer,0.3055555555555556,0.04453197507374984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_logical_fallacies,0.294478527607362,0.03581165790474082,"acc,none",1345423360,5.0,answer,0.294478527607362,0.03581165790474082,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_machine_learning,0.3392857142857143,0.04493949068613539,"acc,none",1345423360,5.0,answer,0.3392857142857143,0.04493949068613539,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_management,0.20388349514563106,0.03989139859531772,"acc,none",1345423360,5.0,answer,0.20388349514563106,0.03989139859531772,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_marketing,0.28205128205128205,0.02948036054954119,"acc,none",1345423360,5.0,answer,0.28205128205128205,0.02948036054954119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_medical_genetics,0.3,0.046056618647183814,"acc,none",1345423360,5.0,answer,0.3,0.046056618647183814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_miscellaneous,0.2413793103448276,0.015302380123542075,"acc,none",1345423360,5.0,answer,0.2413793103448276,0.015302380123542075,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_moral_disputes,0.23410404624277456,0.02279711027807113,"acc,none",1345423360,5.0,answer,0.23410404624277456,0.02279711027807113,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_moral_scenarios,0.2446927374301676,0.014378169884098433,"acc,none",1345423360,5.0,answer,0.2446927374301676,0.014378169884098433,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_nutrition,0.23529411764705882,0.02428861946604612,"acc,none",1345423360,5.0,answer,0.23529411764705882,0.02428861946604612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_other,0.2500804634695848,0.0077662237578896036,"acc,none",1345423360,,,0.2500804634695848,0.0077662237578896036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_philosophy,0.2572347266881029,0.024826171289250888,"acc,none",1345423360,5.0,answer,0.2572347266881029,0.024826171289250888,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_prehistory,0.24382716049382716,0.023891879541959607,"acc,none",1345423360,5.0,answer,0.24382716049382716,0.023891879541959607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_professional_accounting,0.26595744680851063,0.026358065698880592,"acc,none",1345423360,5.0,answer,0.26595744680851063,0.026358065698880592,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_professional_law,0.2470664928292047,0.011015752255279345,"acc,none",1345423360,5.0,answer,0.2470664928292047,0.011015752255279345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_professional_medicine,0.18382352941176472,0.02352924218519311,"acc,none",1345423360,5.0,answer,0.18382352941176472,0.02352924218519311,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_professional_psychology,0.25980392156862747,0.01774089950917779,"acc,none",1345423360,5.0,answer,0.25980392156862747,0.01774089950917779,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_public_relations,0.2909090909090909,0.04350271442923243,"acc,none",1345423360,5.0,answer,0.2909090909090909,0.04350271442923243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_security_studies,0.1673469387755102,0.02389714476891452,"acc,none",1345423360,5.0,answer,0.1673469387755102,0.02389714476891452,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_social_sciences,0.23951901202469938,0.0076870452380985044,"acc,none",1345423360,,,0.23951901202469938,0.0076870452380985044,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_sociology,0.263681592039801,0.03115715086935557,"acc,none",1345423360,5.0,answer,0.263681592039801,0.03115715086935557,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_stem,0.2654614652711703,0.007850468029483575,"acc,none",1345423360,,,0.2654614652711703,0.007850468029483575,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_us_foreign_policy,0.29,0.045604802157206845,"acc,none",1345423360,5.0,answer,0.29,0.045604802157206845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_virology,0.28313253012048195,0.03507295431370518,"acc,none",1345423360,5.0,answer,0.28313253012048195,0.03507295431370518,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_world_religions,0.2631578947368421,0.033773102522091945,"acc,none",1345423360,5.0,answer,0.2631578947368421,0.033773102522091945,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,other,0.352108142903122,0.008466119440621258,"acc,none",1345423360,,,0.352108142903122,0.008466119440621258,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,social sciences,0.31394215144621385,0.008339656991123474,"acc,none",1345423360,,,0.31394215144621385,0.008339656991123474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,stem,0.2562638756739613,0.0077352956242838136,"acc,none",1345423360,,,0.2562638756739613,0.0077352956242838136,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,truthfulqa_gen,0.20195838433292534,0.014053957441512353,"rouge1_acc,none",1345423360,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,3.5746537481769134,0.3204682059351352,0.1799265605875153,0.013447109235537557,-0.41536581002333867,0.15088790330809135,11.08653249926408,0.6426562325628733,0.20195838433292534,0.014053957441512353,-0.7437016904278017,0.27128164233606417,5.284406697114497,0.4846428197466845,0.09424724602203183,0.010228079300416427,-0.9424459257333404,0.24451124560783855,10.187513725393815,0.599412300063159,0.19583843329253367,0.01389234436774208,-0.655557180045168,0.2435752222722309
croissantllm/CroissantLLMBase,3000000000000,openllm,truthfulqa_mc1,0.23623011015911874,0.014869755015871093,"acc,none",1345423360,0.0,0,0.23623011015911874,0.014869755015871093,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,truthfulqa_mc2,0.37237821800712717,0.01366221421305425,"acc,none",1345423360,0.0,0,0.37237821800712717,0.01366221421305425,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,winogrande,0.55327545382794,0.01397248837161669,"acc,none",1345423360,5.0,,0.55327545382794,0.01397248837161669,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,french_bench,french_bench_arc_challenge,0.31565440547476475,0.01359947816343533,"acc_norm,none",1345431552,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.2737382378100941,0.013046466448429663,0.31565440547476475,0.01359947816343533,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,french_bench,french_bench_grammar,0.7815126050420168,0.038039971528894836,"acc,none",1345431552,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7815126050420168,0.038039971528894836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,french_bench,french_bench_hellaswag,0.5293424716213322,0.005165557449021644,"acc_norm,none",1345431552,5.0,{{label}},0.4171128721353609,0.005102879925256238,0.5293424716213322,0.005165557449021644,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,french_bench,french_bench_vocab,0.7899159663865546,0.037501269180121324,"acc,none",1345431552,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7899159663865546,0.037501269180121324,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,arc_challenge,0.32081911262798635,0.013640943091946524,"acc_norm,none",1345431552,25.0,{{choices.label.index(answerKey)}},0.2986348122866894,0.013374078615068745,0.32081911262798635,0.013640943091946524,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,gsm8k,0.014404852160727824,0.0032820559171369383,"exact_match,flexible-extract",1345431552,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.0075815011372251705,0.0023892815120772153,0.014404852160727824,0.0032820559171369383,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,hellaswag,0.5598486357299343,0.004953907062096602,"acc_norm,none",1345431552,10.0,{{label}},0.42859988050189207,0.004938643787869544,0.5598486357299343,0.004953907062096602,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,humanities,0.2771519659936238,0.0064903743030866965,"acc,none",1345431552,,,0.2771519659936238,0.0064903743030866965,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu,0.25096140150975643,0.003655512767962599,"acc,none",1345431552,,,0.25096140150975643,0.003655512767962599,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_abstract_algebra,0.31,0.04648231987117316,"acc,none",1345431552,5.0,answer,0.31,0.04648231987117316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_anatomy,0.3111111111111111,0.03999262876617723,"acc,none",1345431552,5.0,answer,0.3111111111111111,0.03999262876617723,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_astronomy,0.23684210526315788,0.03459777606810536,"acc,none",1345431552,5.0,answer,0.23684210526315788,0.03459777606810536,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_business_ethics,0.24,0.04292346959909282,"acc,none",1345431552,5.0,answer,0.24,0.04292346959909282,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_clinical_knowledge,0.24528301886792453,0.026480357179895702,"acc,none",1345431552,5.0,answer,0.24528301886792453,0.026480357179895702,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_college_biology,0.2569444444444444,0.03653946969442099,"acc,none",1345431552,5.0,answer,0.2569444444444444,0.03653946969442099,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_college_chemistry,0.27,0.044619604333847394,"acc,none",1345431552,5.0,answer,0.27,0.044619604333847394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_college_computer_science,0.35,0.047937248544110196,"acc,none",1345431552,5.0,answer,0.35,0.047937248544110196,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_college_mathematics,0.33,0.047258156262526045,"acc,none",1345431552,5.0,answer,0.33,0.047258156262526045,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_college_medicine,0.23699421965317918,0.032424147574830996,"acc,none",1345431552,5.0,answer,0.23699421965317918,0.032424147574830996,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_college_physics,0.29411764705882354,0.04533838195929775,"acc,none",1345431552,5.0,answer,0.29411764705882354,0.04533838195929775,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_computer_security,0.22,0.041633319989322695,"acc,none",1345431552,5.0,answer,0.22,0.041633319989322695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_conceptual_physics,0.26382978723404255,0.02880998985410298,"acc,none",1345431552,5.0,answer,0.26382978723404255,0.02880998985410298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation,0.2996011964107677,0.003828136093694429,"acc,none",1345431552,,,0.2996011964107677,0.003828136093694429,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_abstract_algebra,0.18,0.03861229196653697,"acc_norm,none",1345431552,0.0,{{answer}},0.16,0.036845294917747094,0.18,0.03861229196653697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_anatomy,0.2962962962962963,0.03944624162501116,"acc_norm,none",1345431552,0.0,{{answer}},0.2962962962962963,0.03944624162501116,0.2962962962962963,0.03944624162501116,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_astronomy,0.4144736842105263,0.04008973785779206,"acc_norm,none",1345431552,0.0,{{answer}},0.3223684210526316,0.03803510248351585,0.4144736842105263,0.04008973785779206,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_business_ethics,0.38,0.04878317312145633,"acc_norm,none",1345431552,0.0,{{answer}},0.45,0.05,0.38,0.04878317312145633,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_clinical_knowledge,0.3849056603773585,0.029946498567699955,"acc_norm,none",1345431552,0.0,{{answer}},0.30566037735849055,0.028353298073322666,0.3849056603773585,0.029946498567699955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_college_biology,0.3541666666666667,0.039994111357535424,"acc_norm,none",1345431552,0.0,{{answer}},0.3541666666666667,0.039994111357535424,0.3541666666666667,0.039994111357535424,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_college_chemistry,0.24,0.04292346959909283,"acc_norm,none",1345431552,0.0,{{answer}},0.18,0.03861229196653695,0.24,0.04292346959909283,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_college_computer_science,0.26,0.04408440022768078,"acc_norm,none",1345431552,0.0,{{answer}},0.28,0.045126085985421255,0.26,0.04408440022768078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_college_mathematics,0.22,0.041633319989322695,"acc_norm,none",1345431552,0.0,{{answer}},0.17,0.0377525168068637,0.22,0.041633319989322695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_college_medicine,0.3236994219653179,0.0356760379963917,"acc_norm,none",1345431552,0.0,{{answer}},0.28901734104046245,0.034564257450869995,0.3236994219653179,0.0356760379963917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_college_physics,0.2549019607843137,0.04336432707993176,"acc_norm,none",1345431552,0.0,{{answer}},0.20588235294117646,0.04023382273617747,0.2549019607843137,0.04336432707993176,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_computer_security,0.38,0.04878317312145634,"acc_norm,none",1345431552,0.0,{{answer}},0.32,0.04688261722621505,0.38,0.04878317312145634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_conceptual_physics,0.28085106382978725,0.029379170464124818,"acc_norm,none",1345431552,0.0,{{answer}},0.3574468085106383,0.03132941789476425,0.28085106382978725,0.029379170464124818,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_econometrics,0.2719298245614035,0.04185774424022058,"acc_norm,none",1345431552,0.0,{{answer}},0.22807017543859648,0.03947152782669415,0.2719298245614035,0.04185774424022058,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_electrical_engineering,0.2896551724137931,0.037800192304380135,"acc_norm,none",1345431552,0.0,{{answer}},0.30344827586206896,0.038312260488503336,0.2896551724137931,0.037800192304380135,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_elementary_mathematics,0.2671957671957672,0.022789673145776568,"acc_norm,none",1345431552,0.0,{{answer}},0.25132275132275134,0.022340482339643895,0.2671957671957672,0.022789673145776568,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_formal_logic,0.30158730158730157,0.04104947269903394,"acc_norm,none",1345431552,0.0,{{answer}},0.2698412698412698,0.03970158273235173,0.30158730158730157,0.04104947269903394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_global_facts,0.29,0.04560480215720683,"acc_norm,none",1345431552,0.0,{{answer}},0.29,0.045604802157206845,0.29,0.04560480215720683,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_biology,0.3258064516129032,0.026662010578567104,"acc_norm,none",1345431552,0.0,{{answer}},0.3225806451612903,0.026593084516572274,0.3258064516129032,0.026662010578567104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_chemistry,0.2561576354679803,0.0307127300709826,"acc_norm,none",1345431552,0.0,{{answer}},0.1724137931034483,0.026577672183036586,0.2561576354679803,0.0307127300709826,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_computer_science,0.34,0.047609522856952344,"acc_norm,none",1345431552,0.0,{{answer}},0.31,0.04648231987117316,0.34,0.047609522856952344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_european_history,0.4666666666666667,0.03895658065271847,"acc_norm,none",1345431552,0.0,{{answer}},0.3393939393939394,0.03697442205031595,0.4666666666666667,0.03895658065271847,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_geography,0.3838383838383838,0.0346488167501634,"acc_norm,none",1345431552,0.0,{{answer}},0.3484848484848485,0.033948539651564025,0.3838383838383838,0.0346488167501634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_government_and_politics,0.38341968911917096,0.03508984236295342,"acc_norm,none",1345431552,0.0,{{answer}},0.3005181347150259,0.0330881859441575,0.38341968911917096,0.03508984236295342,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_macroeconomics,0.30512820512820515,0.023346335293325887,"acc_norm,none",1345431552,0.0,{{answer}},0.28717948717948716,0.02293992541853062,0.30512820512820515,0.023346335293325887,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_mathematics,0.1814814814814815,0.023499264669407306,"acc_norm,none",1345431552,0.0,{{answer}},0.14444444444444443,0.021433761274104915,0.1814814814814815,0.023499264669407306,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_microeconomics,0.31932773109243695,0.030283995525884396,"acc_norm,none",1345431552,0.0,{{answer}},0.29411764705882354,0.029597329730978086,0.31932773109243695,0.030283995525884396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_physics,0.271523178807947,0.03631329803969653,"acc_norm,none",1345431552,0.0,{{answer}},0.24503311258278146,0.03511807571804724,0.271523178807947,0.03631329803969653,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_psychology,0.3522935779816514,0.020480568843998993,"acc_norm,none",1345431552,0.0,{{answer}},0.3853211009174312,0.020865850852794115,0.3522935779816514,0.020480568843998993,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_statistics,0.26851851851851855,0.030225226160012414,"acc_norm,none",1345431552,0.0,{{answer}},0.28703703703703703,0.03085199299325701,0.26851851851851855,0.030225226160012414,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_us_history,0.39705882352941174,0.03434131164719129,"acc_norm,none",1345431552,0.0,{{answer}},0.3333333333333333,0.03308611113236434,0.39705882352941174,0.03434131164719129,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_high_school_world_history,0.34177215189873417,0.030874537537553617,"acc_norm,none",1345431552,0.0,{{answer}},0.32489451476793246,0.030486039389105313,0.34177215189873417,0.030874537537553617,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_human_aging,0.34977578475336324,0.03200736719484503,"acc_norm,none",1345431552,0.0,{{answer}},0.4170403587443946,0.03309266936071721,0.34977578475336324,0.03200736719484503,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_human_sexuality,0.3893129770992366,0.04276486542814591,"acc_norm,none",1345431552,0.0,{{answer}},0.3969465648854962,0.04291135671009225,0.3893129770992366,0.04276486542814591,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_international_law,0.35537190082644626,0.04369236326573981,"acc_norm,none",1345431552,0.0,{{answer}},0.21487603305785125,0.03749492448709698,0.35537190082644626,0.04369236326573981,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_jurisprudence,0.37962962962962965,0.04691521224077742,"acc_norm,none",1345431552,0.0,{{answer}},0.24074074074074073,0.04133119440243838,0.37962962962962965,0.04691521224077742,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_logical_fallacies,0.3312883435582822,0.03697983910025588,"acc_norm,none",1345431552,0.0,{{answer}},0.27607361963190186,0.0351238528370505,0.3312883435582822,0.03697983910025588,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_machine_learning,0.25892857142857145,0.041577515398656284,"acc_norm,none",1345431552,0.0,{{answer}},0.2767857142857143,0.042466243366976256,0.25892857142857145,0.041577515398656284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_management,0.42718446601941745,0.04897957737781168,"acc_norm,none",1345431552,0.0,{{answer}},0.33980582524271846,0.04689765937278134,0.42718446601941745,0.04897957737781168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_marketing,0.42735042735042733,0.032408473935163266,"acc_norm,none",1345431552,0.0,{{answer}},0.4444444444444444,0.03255326307272486,0.42735042735042733,0.032408473935163266,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_medical_genetics,0.38,0.04878317312145633,"acc_norm,none",1345431552,0.0,{{answer}},0.29,0.04560480215720683,0.38,0.04878317312145633,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_miscellaneous,0.4278416347381865,0.017692787927803728,"acc_norm,none",1345431552,0.0,{{answer}},0.44189016602809705,0.017758800534214407,0.4278416347381865,0.017692787927803728,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_moral_disputes,0.24566473988439305,0.02317629820399201,"acc_norm,none",1345431552,0.0,{{answer}},0.2774566473988439,0.024105712607754307,0.24566473988439305,0.02317629820399201,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_moral_scenarios,0.25921787709497207,0.014655780837497722,"acc_norm,none",1345431552,0.0,{{answer}},0.23798882681564246,0.014242630070574885,0.25921787709497207,0.014655780837497722,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_nutrition,0.34967320261437906,0.027305308076274695,"acc_norm,none",1345431552,0.0,{{answer}},0.2908496732026144,0.026004800363952113,0.34967320261437906,0.027305308076274695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_philosophy,0.29260450160771706,0.02583989833487798,"acc_norm,none",1345431552,0.0,{{answer}},0.2572347266881029,0.02482617128925089,0.29260450160771706,0.02583989833487798,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_prehistory,0.3055555555555556,0.025630824975621344,"acc_norm,none",1345431552,0.0,{{answer}},0.38580246913580246,0.027085401226132143,0.3055555555555556,0.025630824975621344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_professional_accounting,0.23404255319148937,0.025257861359432414,"acc_norm,none",1345431552,0.0,{{answer}},0.26595744680851063,0.026358065698880596,0.23404255319148937,0.025257861359432414,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_professional_law,0.2685788787483703,0.011320056629121722,"acc_norm,none",1345431552,0.0,{{answer}},0.25097783572359844,0.011073730299187226,0.2685788787483703,0.011320056629121722,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_professional_medicine,0.27205882352941174,0.027033041151681456,"acc_norm,none",1345431552,0.0,{{answer}},0.2757352941176471,0.027146271936625162,0.27205882352941174,0.027033041151681456,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_professional_psychology,0.3022875816993464,0.018579232711113874,"acc_norm,none",1345431552,0.0,{{answer}},0.28431372549019607,0.018249024411207657,0.3022875816993464,0.018579232711113874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_public_relations,0.2818181818181818,0.043091187099464585,"acc_norm,none",1345431552,0.0,{{answer}},0.37272727272727274,0.04631381319425463,0.2818181818181818,0.043091187099464585,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_security_studies,0.23673469387755103,0.027212835884073142,"acc_norm,none",1345431552,0.0,{{answer}},0.33877551020408164,0.030299506562154185,0.23673469387755103,0.027212835884073142,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_sociology,0.27860696517412936,0.0317005618349731,"acc_norm,none",1345431552,0.0,{{answer}},0.2885572139303483,0.03203841040213321,0.27860696517412936,0.0317005618349731,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_us_foreign_policy,0.4,0.04923659639173309,"acc_norm,none",1345431552,0.0,{{answer}},0.3,0.046056618647183814,0.4,0.04923659639173309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_virology,0.2891566265060241,0.035294868015111155,"acc_norm,none",1345431552,0.0,{{answer}},0.23493975903614459,0.03300533186128922,0.2891566265060241,0.035294868015111155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_continuation_world_religions,0.47953216374269003,0.03831610532821932,"acc_norm,none",1345431552,0.0,{{answer}},0.4269005847953216,0.03793620616529918,0.47953216374269003,0.03831610532821932,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_econometrics,0.22807017543859648,0.03947152782669415,"acc,none",1345431552,5.0,answer,0.22807017543859648,0.03947152782669415,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_electrical_engineering,0.21379310344827587,0.03416520447747548,"acc,none",1345431552,5.0,answer,0.21379310344827587,0.03416520447747548,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_elementary_mathematics,0.24603174603174602,0.022182037202948368,"acc,none",1345431552,5.0,answer,0.24603174603174602,0.022182037202948368,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_formal_logic,0.23015873015873015,0.03764950879790606,"acc,none",1345431552,5.0,answer,0.23015873015873015,0.03764950879790606,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_global_facts,0.27,0.044619604333847394,"acc,none",1345431552,5.0,answer,0.27,0.044619604333847394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_biology,0.2161290322580645,0.023415293433568525,"acc,none",1345431552,5.0,answer,0.2161290322580645,0.023415293433568525,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_chemistry,0.23645320197044334,0.029896114291733552,"acc,none",1345431552,5.0,answer,0.23645320197044334,0.029896114291733552,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_computer_science,0.23,0.04229525846816506,"acc,none",1345431552,5.0,answer,0.23,0.04229525846816506,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_european_history,0.21212121212121213,0.03192271569548299,"acc,none",1345431552,5.0,answer,0.21212121212121213,0.03192271569548299,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_geography,0.21212121212121213,0.029126522834586808,"acc,none",1345431552,5.0,answer,0.21212121212121213,0.029126522834586808,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_government_and_politics,0.21243523316062177,0.02951928261681726,"acc,none",1345431552,5.0,answer,0.21243523316062177,0.02951928261681726,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_macroeconomics,0.26666666666666666,0.022421273612923707,"acc,none",1345431552,5.0,answer,0.26666666666666666,0.022421273612923707,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_mathematics,0.24444444444444444,0.026202766534652148,"acc,none",1345431552,5.0,answer,0.24444444444444444,0.026202766534652148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_microeconomics,0.25630252100840334,0.02835962087053395,"acc,none",1345431552,5.0,answer,0.25630252100840334,0.02835962087053395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_physics,0.17880794701986755,0.03128744850600724,"acc,none",1345431552,5.0,answer,0.17880794701986755,0.03128744850600724,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_psychology,0.25137614678899084,0.018599206360287415,"acc,none",1345431552,5.0,answer,0.25137614678899084,0.018599206360287415,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_statistics,0.32407407407407407,0.03191923445686186,"acc,none",1345431552,5.0,answer,0.32407407407407407,0.03191923445686186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_us_history,0.23039215686274508,0.029554292605695053,"acc,none",1345431552,5.0,answer,0.23039215686274508,0.029554292605695053,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_high_school_world_history,0.270042194092827,0.028900721906293433,"acc,none",1345431552,5.0,answer,0.270042194092827,0.028900721906293433,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_human_aging,0.27802690582959644,0.030069584874494033,"acc,none",1345431552,5.0,answer,0.27802690582959644,0.030069584874494033,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_human_sexuality,0.2595419847328244,0.03844876139785271,"acc,none",1345431552,5.0,answer,0.2595419847328244,0.03844876139785271,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_humanities,0.2499468650371945,0.0063046845127181445,"acc,none",1345431552,,,0.2499468650371945,0.0063046845127181445,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_international_law,0.3305785123966942,0.04294340845212093,"acc,none",1345431552,5.0,answer,0.3305785123966942,0.04294340845212093,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_jurisprudence,0.3425925925925926,0.045879047413018105,"acc,none",1345431552,5.0,answer,0.3425925925925926,0.045879047413018105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_logical_fallacies,0.3128834355828221,0.036429145782924055,"acc,none",1345431552,5.0,answer,0.3128834355828221,0.036429145782924055,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_machine_learning,0.3125,0.043994650575715215,"acc,none",1345431552,5.0,answer,0.3125,0.043994650575715215,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_management,0.20388349514563106,0.039891398595317706,"acc,none",1345431552,5.0,answer,0.20388349514563106,0.039891398595317706,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_marketing,0.23504273504273504,0.02777883590493543,"acc,none",1345431552,5.0,answer,0.23504273504273504,0.02777883590493543,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_medical_genetics,0.25,0.04351941398892446,"acc,none",1345431552,5.0,answer,0.25,0.04351941398892446,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_miscellaneous,0.27458492975734355,0.01595982993308404,"acc,none",1345431552,5.0,answer,0.27458492975734355,0.01595982993308404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_moral_disputes,0.22254335260115607,0.02239421566194282,"acc,none",1345431552,5.0,answer,0.22254335260115607,0.02239421566194282,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_moral_scenarios,0.2435754189944134,0.01435591196476786,"acc,none",1345431552,5.0,answer,0.2435754189944134,0.01435591196476786,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_nutrition,0.2222222222222222,0.02380518652488814,"acc,none",1345431552,5.0,answer,0.2222222222222222,0.02380518652488814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_other,0.2529771483746379,0.007804679578523687,"acc,none",1345431552,,,0.2529771483746379,0.007804679578523687,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_philosophy,0.18971061093247588,0.022268196258783225,"acc,none",1345431552,5.0,answer,0.18971061093247588,0.022268196258783225,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_prehistory,0.25617283950617287,0.024288533637726095,"acc,none",1345431552,5.0,answer,0.25617283950617287,0.024288533637726095,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_professional_accounting,0.2375886524822695,0.025389512552729903,"acc,none",1345431552,5.0,answer,0.2375886524822695,0.025389512552729903,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_professional_law,0.2516297262059974,0.011083276280441909,"acc,none",1345431552,5.0,answer,0.2516297262059974,0.011083276280441909,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_professional_medicine,0.24632352941176472,0.02617343857052,"acc,none",1345431552,5.0,answer,0.24632352941176472,0.02617343857052,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_professional_psychology,0.2369281045751634,0.017201662169789786,"acc,none",1345431552,5.0,answer,0.2369281045751634,0.017201662169789786,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_public_relations,0.33636363636363636,0.04525393596302505,"acc,none",1345431552,5.0,answer,0.33636363636363636,0.04525393596302505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_security_studies,0.17142857142857143,0.024127463462650153,"acc,none",1345431552,5.0,answer,0.17142857142857143,0.024127463462650153,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_social_sciences,0.2427689307767306,0.007722694185606258,"acc,none",1345431552,,,0.2427689307767306,0.007722694185606258,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_sociology,0.24378109452736318,0.03036049015401466,"acc,none",1345431552,5.0,answer,0.24378109452736318,0.03036049015401466,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_stem,0.25848398350777035,0.007783347572668495,"acc,none",1345431552,,,0.25848398350777035,0.007783347572668495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_us_foreign_policy,0.29,0.045604802157206845,"acc,none",1345431552,5.0,answer,0.29,0.045604802157206845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_virology,0.29518072289156627,0.035509201856896294,"acc,none",1345431552,5.0,answer,0.29518072289156627,0.035509201856896294,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,mmlu_world_religions,0.29239766081871343,0.03488647713457923,"acc,none",1345431552,5.0,answer,0.29239766081871343,0.03488647713457923,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,other,0.35082072738976505,0.008461409610699211,"acc,none",1345431552,,,0.35082072738976505,0.008461409610699211,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,social sciences,0.31946701332466687,0.008382466484875282,"acc,none",1345431552,,,0.31946701332466687,0.008382466484875282,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,stem,0.26324135743736127,0.007772535800510438,"acc,none",1345431552,,,0.26324135743736127,0.007772535800510438,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,truthfulqa_gen,0.20807833537331702,0.01421050347357663,"rouge1_acc,none",1345431552,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,11.193674777331966,0.607307326558056,0.193390452876377,0.013826240752599066,-2.424012069656661,0.4349561181592294,24.473951748486126,0.9201352966728208,0.20807833537331702,0.01421050347357663,-4.286956508516791,0.5281068146813853,15.172197878226205,0.8088182769433488,0.12362301101591187,0.011522588757335618,-3.980774220192735,0.5815737218725309,22.673455560331426,0.8843259753177398,0.19951040391676866,0.013989929967559657,-4.0344813396922286,0.5272948572429987
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,truthfulqa_mc1,0.24357405140758873,0.01502635482491078,"acc,none",1345431552,0.0,0,0.24357405140758873,0.01502635482491078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,truthfulqa_mc2,0.39654281490156906,0.014744773128129874,"acc,none",1345431552,0.0,0,0.39654281490156906,0.014744773128129874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMChat-v0.1,3000000000000,openllm,winogrande,0.5603788476716653,0.013949649776015706,"acc,none",1345431552,5.0,,0.5603788476716653,0.013949649776015706,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_compréhension_de_la_date,0.476,0.03164968895968781,"acc_norm,none",8030261248,3.0,{{target}},,,0.476,0.03164968895968781,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_compréhension_des_sports,0.652,0.030186568464511697,"acc_norm,none",8030261248,3.0,{{target}},,,0.652,0.030186568464511697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_comptage_d_objets,0.392,0.030938207620401195,"acc_norm,none",8030261248,3.0,{{target}},,,0.392,0.030938207620401195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_cinq_objets,0.328,0.029752391824475383,"acc_norm,none",8030261248,3.0,{{target}},,,0.328,0.029752391824475383,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_sept_objets,0.236,0.026909337594953838,"acc_norm,none",8030261248,3.0,{{target}},,,0.236,0.026909337594953838,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_trois_objets,0.48,0.03166085340849519,"acc_norm,none",8030261248,3.0,{{target}},,,0.48,0.03166085340849519,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_désambiguïsation_qa,0.4,0.031046021028253233,"acc_norm,none",8030261248,3.0,{{target}},,,0.4,0.031046021028253233,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_expressions_booléennes,0.46,0.031584653891499004,"acc_norm,none",8030261248,3.0,{{target}},,,0.46,0.031584653891499004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_formes_géométriques,0.348,0.030186568464511707,"acc_norm,none",8030261248,3.0,{{target}},,,0.348,0.030186568464511707,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_hyperbate,0.528,0.0316364895315444,"acc_norm,none",8030261248,3.0,{{target}},,,0.528,0.0316364895315444,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_jugement_causal,0.48128342245989303,0.03663608375537842,"acc_norm,none",8030261248,3.0,{{target}},,,0.48128342245989303,0.03663608375537842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_naviguer,0.576,0.03131803437491614,"acc_norm,none",8030261248,3.0,{{target}},,,0.576,0.03131803437491614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_pingouins_sur_une_table,0.3835616438356164,0.04038112474853564,"acc_norm,none",8030261248,3.0,{{target}},,,0.3835616438356164,0.04038112474853564,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_raisonnement_sur_les_objets_colorés,0.372,0.030630325944558313,"acc_norm,none",8030261248,3.0,{{target}},,,0.372,0.030630325944558313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_recommandation_de_film,0.556,0.03148684942554574,"acc_norm,none",8030261248,3.0,{{target}},,,0.556,0.03148684942554574,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_sarcasmes,0.5056179775280899,0.03757992900475981,"acc_norm,none",8030261248,3.0,{{target}},,,0.5056179775280899,0.03757992900475981,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_sophismes_formels,0.504,0.031685198551199154,"acc_norm,none",8030261248,3.0,{{target}},,,0.504,0.031685198551199154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_cinq_objets,0.144,0.02224940773545021,"acc_norm,none",8030261248,3.0,{{target}},,,0.144,0.02224940773545021,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_sept_objets,0.124,0.02088638225867326,"acc_norm,none",8030261248,3.0,{{target}},,,0.124,0.02088638225867326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_trois_objets,0.328,0.029752391824475376,"acc_norm,none",8030261248,3.0,{{target}},,,0.328,0.029752391824475376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_séquences_temporelles,0.196,0.025156857313255936,"acc_norm,none",8030261248,3.0,{{target}},,,0.196,0.025156857313255936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_toile_de_mensonges,0.508,0.031682156431413803,"acc_norm,none",8030261248,3.0,{{target}},,,0.508,0.031682156431413803,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_gpqa_diamond_fr,0.36548223350253806,0.03439750899385474,"acc_norm,none",8030261248,0.0,answer,,,0.36548223350253806,0.03439750899385474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_gpqa_extended_fr,0.289134438305709,0.019473499586525275,"acc_norm,none",8030261248,0.0,answer,,,0.289134438305709,0.019473499586525275,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_gpqa_main_fr,0.29603729603729606,0.022066129127119836,"acc_norm,none",8030261248,0.0,answer,,,0.29603729603729606,0.022066129127119836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_ifeval_fr,0.019417475728155338,0.006086349682701494,"prompt_level_loose_acc,none",8030261248,0.0,0,,,,,,,,,,,,,0.02330097087378641,0.006654046431364158,0.15765422696115766,N/A,0.019417475728155338,0.006086349682701494,0.1667936024371668,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_algebra_hard_fr,0.07714285714285714,0.014282439248008285,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.07714285714285714,0.014282439248008285,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_counting_and_prob_hard_fr,0.015306122448979591,0.008791559199116599,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.015306122448979591,0.008791559199116599,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_geometry_hard_fr,0.019417475728155338,0.00963743649866822,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.019417475728155338,0.00963743649866822,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_num_theory_hard_fr,0.013824884792626729,0.007944762237164794,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.013824884792626729,0.007944762237164794,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_prealgebra_hard_fr,0.09734513274336283,0.028009733301017123,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.09734513274336283,0.028009733301017123,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_precalculus_hard_fr,0.011904761904761904,0.011904761904761902,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.011904761904761904,0.011904761904761902,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_mmlu_fr,0.5263495228599915,0.004213733711006387,"acc,none",8030261248,5.0,Answer,0.5263495228599915,0.004213733711006387,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_musr_murder_mysteries_fr,0.5,0.031686212526223896,"acc_norm,none",8030261248,0.0,{{answer_choice}},,,0.5,0.031686212526223896,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_musr_object_placements_fr,0.2890625,0.02838843806999465,"acc_norm,none",8030261248,0.0,{{answer_choice}},,,0.2890625,0.02838843806999465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_musr_team_allocation_fr,0.264,0.027934518957690908,"acc_norm,none",8030261248,0.0,{{answer_choice}},,,0.264,0.027934518957690908,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench,french_bench_arc_challenge,0.5089820359281437,0.01462778257777394,"acc_norm,none",8030261248,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.4533789563729683,0.01456640669542,0.5089820359281437,0.01462778257777394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench,french_bench_grammar,0.7647058823529411,0.03904916456144797,"acc,none",8030261248,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7647058823529411,0.03904916456144797,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench,french_bench_hellaswag,0.6718783465410152,0.0048591363823387666,"acc_norm,none",8030261248,5.0,{{label}},0.4937888198757764,0.005174076114991718,0.6718783465410152,0.0048591363823387666,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench,french_bench_vocab,0.8487394957983193,0.032984429309406524,"acc,none",8030261248,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.8487394957983193,0.032984429309406524,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench_gen,french_bench_fquadv2_genq,0.3490946664074175,0.0,"rouge1,none",8030261248,5.0,{{question}},,,,,0.3490946664074175,N/A,0.3290418688272903,0.011775499775611025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench_gen,french_bench_fquadv2_hasAns,0.8090964538890904,0.0,"rouge1,none",8030261248,5.0,{% if answers.text| length > 0 %}{{answers.text[0]}}{% else %}{{['Impossible']}}{% endif %},,,,,0.8090964538890904,N/A,0.800287864883104,0.014893714494017513,0.545,0.024929725792117705,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench_gen,french_bench_multifquad,0.6628420045258809,0.0,"rouge1,none",8030261248,5.0,"{{', '.join(answers.text)}}",,,,,0.6628420045258809,N/A,0.6564214069666591,0.014634477373571372,0.19,0.019639610121239274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench_gen,french_bench_orangesum_abstract,0.2872868740021828,0.0,"rouge1,none",8030261248,5.0,{{summary}},,,,,0.2872868740021828,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench_gen,french_bench_trivia,0.7430472232445917,0.0,"rouge1,none",8030261248,5.0,{{Answer}},,,,,0.7430472232445917,N/A,0.728627025709069,0.020261345600661663,0.6078947368421053,0.025078174648458913,0.5842105263157895,0.02531639301514226,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,arc_de,0.4636441402908469,0.01459141740443866,"acc_norm,none",8030261248,25.0,gold,0.41659538066723695,0.01442516320371223,0.4636441402908469,0.01459141740443866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,arc_es,0.5153846153846153,0.014616960326221319,"acc_norm,none",8030261248,25.0,gold,0.46324786324786327,0.014584325475224508,0.5153846153846153,0.014616960326221319,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,arc_fr,0.5004277159965783,0.014630138046609936,"acc_norm,none",8030261248,25.0,gold,0.4593669803250642,0.014581753402377686,0.5004277159965783,0.014630138046609936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,arc_it,0.5072711719418306,0.014628596328069858,"acc_norm,none",8030261248,25.0,gold,0.4636441402908469,0.014591417404438664,0.5072711719418306,0.014628596328069858,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,m_mmlu_de,0.561547744757882,0.0043095494629396104,"acc,none",8030261248,25.0,answer,0.561547744757882,0.0043095494629396104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,m_mmlu_es,0.5777711114444278,0.0042774795718552505,"acc,none",8030261248,25.0,answer,0.5777711114444278,0.0042774795718552505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,m_mmlu_fr,0.5744404552746162,0.004321483442731279,"acc,none",8030261248,25.0,answer,0.5744404552746162,0.004321483442731279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,m_mmlu_it,0.5712019339729546,0.004301727187947193,"acc,none",8030261248,25.0,answer,0.5712019339729546,0.004301727187947193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,arc_de,0.4636441402908469,0.01459141740443866,"acc_norm,none",8030261248,25.0,gold,0.41659538066723695,0.01442516320371223,0.4636441402908469,0.01459141740443866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,arc_es,0.5153846153846153,0.014616960326221319,"acc_norm,none",8030261248,25.0,gold,0.46324786324786327,0.014584325475224508,0.5153846153846153,0.014616960326221319,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,arc_fr,0.5004277159965783,0.014630138046609936,"acc_norm,none",8030261248,25.0,gold,0.4593669803250642,0.014581753402377686,0.5004277159965783,0.014630138046609936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,arc_it,0.5072711719418306,0.014628596328069858,"acc_norm,none",8030261248,25.0,gold,0.4636441402908469,0.014591417404438664,0.5072711719418306,0.014628596328069858,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,m_mmlu_de,0.561547744757882,0.0043095494629396104,"acc,none",8030261248,25.0,answer,0.561547744757882,0.0043095494629396104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,m_mmlu_es,0.5777711114444278,0.0042774795718552505,"acc,none",8030261248,25.0,answer,0.5777711114444278,0.0042774795718552505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,m_mmlu_fr,0.5744404552746162,0.004321483442731279,"acc,none",8030261248,25.0,answer,0.5744404552746162,0.004321483442731279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,m_mmlu_it,0.5712019339729546,0.004301727187947193,"acc,none",8030261248,25.0,answer,0.5712019339729546,0.004301727187947193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,arc_challenge,0.5819112627986348,0.014413988396996074,"acc_norm,none",8030261248,25.0,{{choices.label.index(answerKey)}},0.5477815699658704,0.014544519880633827,0.5819112627986348,0.014413988396996074,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,gsm8k,0.514783927217589,0.013766463050787601,"exact_match,flexible-extract",8030261248,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.511751326762699,0.013768680408142796,0.514783927217589,0.013766463050787601,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,hellaswag,0.8170683130850428,0.0038582038518200175,"acc_norm,none",8030261248,10.0,{{label}},0.6170085640310695,0.004851227527070909,0.8170683130850428,0.0038582038518200175,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,humanities,0.35515409139213605,0.006773666936701515,"acc,none",8030261248,,,0.35515409139213605,0.006773666936701515,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu,0.6519726534681669,0.00379623139201471,"acc,none",8030261248,,,0.6519726534681669,0.00379623139201471,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_abstract_algebra,0.3,0.046056618647183814,"acc,none",8030261248,5.0,answer,0.3,0.046056618647183814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_anatomy,0.6222222222222222,0.04188307537595853,"acc,none",8030261248,5.0,answer,0.6222222222222222,0.04188307537595853,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_astronomy,0.7039473684210527,0.03715062154998904,"acc,none",8030261248,5.0,answer,0.7039473684210527,0.03715062154998904,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_business_ethics,0.65,0.0479372485441102,"acc,none",8030261248,5.0,answer,0.65,0.0479372485441102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_clinical_knowledge,0.7584905660377359,0.026341480371118362,"acc,none",8030261248,5.0,answer,0.7584905660377359,0.026341480371118362,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_biology,0.7569444444444444,0.03586879280080342,"acc,none",8030261248,5.0,answer,0.7569444444444444,0.03586879280080342,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_chemistry,0.43,0.04975698519562428,"acc,none",8030261248,5.0,answer,0.43,0.04975698519562428,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_computer_science,0.46,0.05009082659620332,"acc,none",8030261248,5.0,answer,0.46,0.05009082659620332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_mathematics,0.3,0.046056618647183814,"acc,none",8030261248,5.0,answer,0.3,0.046056618647183814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_medicine,0.653179190751445,0.03629146670159663,"acc,none",8030261248,5.0,answer,0.653179190751445,0.03629146670159663,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_physics,0.49019607843137253,0.04974229460422817,"acc,none",8030261248,5.0,answer,0.49019607843137253,0.04974229460422817,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_computer_security,0.83,0.0377525168068637,"acc,none",8030261248,5.0,answer,0.83,0.0377525168068637,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_conceptual_physics,0.6,0.03202563076101735,"acc,none",8030261248,5.0,answer,0.6,0.03202563076101735,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation,0.44609030052699045,0.004028669044988495,"acc,none",8030261248,,,0.44609030052699045,0.004028669044988495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_abstract_algebra,0.28,0.045126085985421276,"acc_norm,none",8030261248,0.0,{{answer}},0.24,0.04292346959909283,0.28,0.045126085985421276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_anatomy,0.5777777777777777,0.042667634040995814,"acc_norm,none",8030261248,0.0,{{answer}},0.5555555555555556,0.04292596718256981,0.5777777777777777,0.042667634040995814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_astronomy,0.5460526315789473,0.04051646342874143,"acc_norm,none",8030261248,0.0,{{answer}},0.47368421052631576,0.04063302731486671,0.5460526315789473,0.04051646342874143,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_business_ethics,0.63,0.04852365870939098,"acc_norm,none",8030261248,0.0,{{answer}},0.72,0.045126085985421276,0.63,0.04852365870939098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_clinical_knowledge,0.5358490566037736,0.030693675018458006,"acc_norm,none",8030261248,0.0,{{answer}},0.43018867924528303,0.030471445867183235,0.5358490566037736,0.030693675018458006,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_biology,0.5902777777777778,0.04112490974670787,"acc_norm,none",8030261248,0.0,{{answer}},0.5625,0.04148415739394154,0.5902777777777778,0.04112490974670787,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_chemistry,0.42,0.049604496374885836,"acc_norm,none",8030261248,0.0,{{answer}},0.37,0.04852365870939098,0.42,0.049604496374885836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_computer_science,0.44,0.04988876515698589,"acc_norm,none",8030261248,0.0,{{answer}},0.36,0.048241815132442176,0.44,0.04988876515698589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_mathematics,0.29,0.045604802157206845,"acc_norm,none",8030261248,0.0,{{answer}},0.22,0.041633319989322695,0.29,0.045604802157206845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_medicine,0.48554913294797686,0.03810871630454764,"acc_norm,none",8030261248,0.0,{{answer}},0.43352601156069365,0.03778621079092055,0.48554913294797686,0.03810871630454764,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_physics,0.3431372549019608,0.04724007352383889,"acc_norm,none",8030261248,0.0,{{answer}},0.3333333333333333,0.04690650298201943,0.3431372549019608,0.04724007352383889,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_computer_security,0.56,0.049888765156985884,"acc_norm,none",8030261248,0.0,{{answer}},0.53,0.050161355804659205,0.56,0.049888765156985884,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_conceptual_physics,0.5234042553191489,0.03265019475033582,"acc_norm,none",8030261248,0.0,{{answer}},0.5574468085106383,0.03246956919789958,0.5234042553191489,0.03265019475033582,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_econometrics,0.3508771929824561,0.04489539350270699,"acc_norm,none",8030261248,0.0,{{answer}},0.37719298245614036,0.04559522141958216,0.3508771929824561,0.04489539350270699,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_electrical_engineering,0.4413793103448276,0.04137931034482758,"acc_norm,none",8030261248,0.0,{{answer}},0.3931034482758621,0.040703290137070705,0.4413793103448276,0.04137931034482758,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_elementary_mathematics,0.6957671957671958,0.02369541500946309,"acc_norm,none",8030261248,0.0,{{answer}},0.6878306878306878,0.023865206836972578,0.6957671957671958,0.02369541500946309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_formal_logic,0.42857142857142855,0.04426266681379909,"acc_norm,none",8030261248,0.0,{{answer}},0.4523809523809524,0.044518079590553275,0.42857142857142855,0.04426266681379909,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_global_facts,0.54,0.05009082659620332,"acc_norm,none",8030261248,0.0,{{answer}},0.55,0.049999999999999996,0.54,0.05009082659620332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_biology,0.5709677419354838,0.028156036538233193,"acc_norm,none",8030261248,0.0,{{answer}},0.4870967741935484,0.028434533152681848,0.5709677419354838,0.028156036538233193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_chemistry,0.3645320197044335,0.0338640574606209,"acc_norm,none",8030261248,0.0,{{answer}},0.3251231527093596,0.032957975663112704,0.3645320197044335,0.0338640574606209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_computer_science,0.48,0.050211673156867795,"acc_norm,none",8030261248,0.0,{{answer}},0.47,0.050161355804659205,0.48,0.050211673156867795,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_european_history,0.503030303030303,0.03904272341431857,"acc_norm,none",8030261248,0.0,{{answer}},0.38181818181818183,0.03793713171165633,0.503030303030303,0.03904272341431857,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_geography,0.6060606060606061,0.03481285338232963,"acc_norm,none",8030261248,0.0,{{answer}},0.5303030303030303,0.03555804051763929,0.6060606060606061,0.03481285338232963,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_government_and_politics,0.5958549222797928,0.0354150857888402,"acc_norm,none",8030261248,0.0,{{answer}},0.5492227979274611,0.035909109522355244,0.5958549222797928,0.0354150857888402,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_macroeconomics,0.5051282051282051,0.02534967290683866,"acc_norm,none",8030261248,0.0,{{answer}},0.44358974358974357,0.025189149894764205,0.5051282051282051,0.02534967290683866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_mathematics,0.35555555555555557,0.02918571494985741,"acc_norm,none",8030261248,0.0,{{answer}},0.3037037037037037,0.02803792996911499,0.35555555555555557,0.02918571494985741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_microeconomics,0.5672268907563025,0.03218358107742613,"acc_norm,none",8030261248,0.0,{{answer}},0.46638655462184875,0.03240501447690071,0.5672268907563025,0.03218358107742613,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_physics,0.33112582781456956,0.038425817186598696,"acc_norm,none",8030261248,0.0,{{answer}},0.3509933774834437,0.03896981964257374,0.33112582781456956,0.038425817186598696,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_psychology,0.6385321100917432,0.02059808200993737,"acc_norm,none",8030261248,0.0,{{answer}},0.6752293577981652,0.02007772910931033,0.6385321100917432,0.02059808200993737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_statistics,0.4166666666666667,0.03362277436608043,"acc_norm,none",8030261248,0.0,{{answer}},0.32407407407407407,0.03191923445686186,0.4166666666666667,0.03362277436608043,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_us_history,0.5343137254901961,0.03501038327635897,"acc_norm,none",8030261248,0.0,{{answer}},0.44607843137254904,0.03488845451304974,0.5343137254901961,0.03501038327635897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_world_history,0.510548523206751,0.032539983791662855,"acc_norm,none",8030261248,0.0,{{answer}},0.4345991561181435,0.03226759995510145,0.510548523206751,0.032539983791662855,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_human_aging,0.5291479820627802,0.03350073248773404,"acc_norm,none",8030261248,0.0,{{answer}},0.5336322869955157,0.033481800170603065,0.5291479820627802,0.03350073248773404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_human_sexuality,0.5114503816793893,0.04384140024078016,"acc_norm,none",8030261248,0.0,{{answer}},0.5190839694656488,0.04382094705550988,0.5114503816793893,0.04384140024078016,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_international_law,0.512396694214876,0.045629515481807666,"acc_norm,none",8030261248,0.0,{{answer}},0.2809917355371901,0.04103203830514512,0.512396694214876,0.045629515481807666,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_jurisprudence,0.5092592592592593,0.04832853553437055,"acc_norm,none",8030261248,0.0,{{answer}},0.3888888888888889,0.0471282125742677,0.5092592592592593,0.04832853553437055,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_logical_fallacies,0.49079754601226994,0.03927705600787443,"acc_norm,none",8030261248,0.0,{{answer}},0.4785276073619632,0.03924746876751129,0.49079754601226994,0.03927705600787443,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_machine_learning,0.375,0.04595091388086298,"acc_norm,none",8030261248,0.0,{{answer}},0.4375,0.04708567521880525,0.375,0.04595091388086298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_management,0.6213592233009708,0.04802694698258974,"acc_norm,none",8030261248,0.0,{{answer}},0.5242718446601942,0.049449010929737795,0.6213592233009708,0.04802694698258974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_marketing,0.6581196581196581,0.031075028526507762,"acc_norm,none",8030261248,0.0,{{answer}},0.717948717948718,0.029480360549541187,0.6581196581196581,0.031075028526507762,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_medical_genetics,0.59,0.04943110704237101,"acc_norm,none",8030261248,0.0,{{answer}},0.53,0.050161355804659205,0.59,0.04943110704237101,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_miscellaneous,0.698595146871009,0.016409091097268787,"acc_norm,none",8030261248,0.0,{{answer}},0.70242656449553,0.01634911191290942,0.698595146871009,0.016409091097268787,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_moral_disputes,0.41329479768786126,0.026511261369409244,"acc_norm,none",8030261248,0.0,{{answer}},0.3583815028901734,0.025816756791584204,0.41329479768786126,0.026511261369409244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_moral_scenarios,0.27262569832402234,0.014893391735249603,"acc_norm,none",8030261248,0.0,{{answer}},0.23798882681564246,0.014242630070574885,0.27262569832402234,0.014893391735249603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_nutrition,0.4738562091503268,0.028590752958852394,"acc_norm,none",8030261248,0.0,{{answer}},0.38235294117647056,0.027826109307283697,0.4738562091503268,0.028590752958852394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_philosophy,0.4983922829581994,0.028397944907806612,"acc_norm,none",8030261248,0.0,{{answer}},0.4662379421221865,0.02833327710956279,0.4983922829581994,0.028397944907806612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_prehistory,0.5154320987654321,0.02780749004427619,"acc_norm,none",8030261248,0.0,{{answer}},0.5123456790123457,0.027812262269327242,0.5154320987654321,0.02780749004427619,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_professional_accounting,0.35815602836879434,0.02860208586275943,"acc_norm,none",8030261248,0.0,{{answer}},0.3723404255319149,0.028838921471251448,0.35815602836879434,0.02860208586275943,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_professional_law,0.30834419817470665,0.011794833789715322,"acc_norm,none",8030261248,0.0,{{answer}},0.2803129074315515,0.011471555944958616,0.30834419817470665,0.011794833789715322,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_professional_medicine,0.5073529411764706,0.030369552523902173,"acc_norm,none",8030261248,0.0,{{answer}},0.5330882352941176,0.03030625772246831,0.5073529411764706,0.030369552523902173,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_professional_psychology,0.49836601307189543,0.020227726838150117,"acc_norm,none",8030261248,0.0,{{answer}},0.4395424836601307,0.020079420408087918,0.49836601307189543,0.020227726838150117,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_public_relations,0.43636363636363634,0.04750185058907297,"acc_norm,none",8030261248,0.0,{{answer}},0.4727272727272727,0.04782001791380063,0.43636363636363634,0.04750185058907297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_security_studies,0.3020408163265306,0.029393609319879818,"acc_norm,none",8030261248,0.0,{{answer}},0.33877551020408164,0.030299506562154185,0.3020408163265306,0.029393609319879818,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_sociology,0.5174129353233831,0.03533389234739245,"acc_norm,none",8030261248,0.0,{{answer}},0.4228855721393035,0.034932317774212816,0.5174129353233831,0.03533389234739245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_us_foreign_policy,0.51,0.05024183937956912,"acc_norm,none",8030261248,0.0,{{answer}},0.46,0.05009082659620332,0.51,0.05024183937956912,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_virology,0.42168674698795183,0.03844453181770917,"acc_norm,none",8030261248,0.0,{{answer}},0.3433734939759036,0.03696584317010601,0.42168674698795183,0.03844453181770917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_world_religions,0.7719298245614035,0.03218093795602357,"acc_norm,none",8030261248,0.0,{{answer}},0.7309941520467836,0.034010526201040885,0.7719298245614035,0.03218093795602357,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_econometrics,0.5,0.047036043419179864,"acc,none",8030261248,5.0,answer,0.5,0.047036043419179864,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_electrical_engineering,0.6344827586206897,0.04013124195424385,"acc,none",8030261248,5.0,answer,0.6344827586206897,0.04013124195424385,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_elementary_mathematics,0.43386243386243384,0.025525034382474894,"acc,none",8030261248,5.0,answer,0.43386243386243384,0.025525034382474894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_formal_logic,0.47619047619047616,0.04467062628403273,"acc,none",8030261248,5.0,answer,0.47619047619047616,0.04467062628403273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_global_facts,0.32,0.04688261722621505,"acc,none",8030261248,5.0,answer,0.32,0.04688261722621505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_biology,0.7741935483870968,0.02378557788418101,"acc,none",8030261248,5.0,answer,0.7741935483870968,0.02378557788418101,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_chemistry,0.541871921182266,0.03505630140785741,"acc,none",8030261248,5.0,answer,0.541871921182266,0.03505630140785741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_computer_science,0.67,0.04725815626252607,"acc,none",8030261248,5.0,answer,0.67,0.04725815626252607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_european_history,0.7818181818181819,0.03225078108306289,"acc,none",8030261248,5.0,answer,0.7818181818181819,0.03225078108306289,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_geography,0.8131313131313131,0.02777253333421899,"acc,none",8030261248,5.0,answer,0.8131313131313131,0.02777253333421899,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_government_and_politics,0.8911917098445595,0.022473253332768756,"acc,none",8030261248,5.0,answer,0.8911917098445595,0.022473253332768756,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_macroeconomics,0.6435897435897436,0.0242831405294673,"acc,none",8030261248,5.0,answer,0.6435897435897436,0.0242831405294673,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_mathematics,0.4074074074074074,0.029958249250082118,"acc,none",8030261248,5.0,answer,0.4074074074074074,0.029958249250082118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_microeconomics,0.7394957983193278,0.028510251512341933,"acc,none",8030261248,5.0,answer,0.7394957983193278,0.028510251512341933,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_physics,0.4503311258278146,0.04062290018683775,"acc,none",8030261248,5.0,answer,0.4503311258278146,0.04062290018683775,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_psychology,0.8532110091743119,0.015173141845126267,"acc,none",8030261248,5.0,answer,0.8532110091743119,0.015173141845126267,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_statistics,0.5416666666666666,0.03398110890294636,"acc,none",8030261248,5.0,answer,0.5416666666666666,0.03398110890294636,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_us_history,0.8284313725490197,0.02646056956124065,"acc,none",8030261248,5.0,answer,0.8284313725490197,0.02646056956124065,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_world_history,0.8227848101265823,0.024856364184503234,"acc,none",8030261248,5.0,answer,0.8227848101265823,0.024856364184503234,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_human_aging,0.6905829596412556,0.03102441174057221,"acc,none",8030261248,5.0,answer,0.6905829596412556,0.03102441174057221,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_human_sexuality,0.7786259541984732,0.036412970813137296,"acc,none",8030261248,5.0,answer,0.7786259541984732,0.036412970813137296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_humanities,0.5997874601487779,0.006787659513650209,"acc,none",8030261248,,,0.5997874601487779,0.006787659513650209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_international_law,0.8181818181818182,0.03520893951097653,"acc,none",8030261248,5.0,answer,0.8181818181818182,0.03520893951097653,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_jurisprudence,0.7314814814814815,0.042844679680521934,"acc,none",8030261248,5.0,answer,0.7314814814814815,0.042844679680521934,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_logical_fallacies,0.7423312883435583,0.03436150827846917,"acc,none",8030261248,5.0,answer,0.7423312883435583,0.03436150827846917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_machine_learning,0.44642857142857145,0.04718471485219588,"acc,none",8030261248,5.0,answer,0.44642857142857145,0.04718471485219588,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_management,0.8543689320388349,0.03492606476623789,"acc,none",8030261248,5.0,answer,0.8543689320388349,0.03492606476623789,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_marketing,0.8632478632478633,0.0225090339370778,"acc,none",8030261248,5.0,answer,0.8632478632478633,0.0225090339370778,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_medical_genetics,0.84,0.0368452949177471,"acc,none",8030261248,5.0,answer,0.84,0.0368452949177471,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_miscellaneous,0.8109833971902938,0.014000791294406999,"acc,none",8030261248,5.0,answer,0.8109833971902938,0.014000791294406999,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_moral_disputes,0.7283236994219653,0.023948512905468348,"acc,none",8030261248,5.0,answer,0.7283236994219653,0.023948512905468348,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_moral_scenarios,0.4223463687150838,0.016519594275297114,"acc,none",8030261248,5.0,answer,0.4223463687150838,0.016519594275297114,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_nutrition,0.8071895424836601,0.022589318888176686,"acc,none",8030261248,5.0,answer,0.8071895424836601,0.022589318888176686,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_other,0.7215963952365626,0.007705274282282836,"acc,none",8030261248,,,0.7215963952365626,0.007705274282282836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_philosophy,0.729903536977492,0.025218040373410622,"acc,none",8030261248,5.0,answer,0.729903536977492,0.025218040373410622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_prehistory,0.7129629629629629,0.02517104191530968,"acc,none",8030261248,5.0,answer,0.7129629629629629,0.02517104191530968,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_professional_accounting,0.4929078014184397,0.02982449855912901,"acc,none",8030261248,5.0,answer,0.4929078014184397,0.02982449855912901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_professional_law,0.48435462842242505,0.012763982838120958,"acc,none",8030261248,5.0,answer,0.48435462842242505,0.012763982838120958,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_professional_medicine,0.6911764705882353,0.028064998167040094,"acc,none",8030261248,5.0,answer,0.6911764705882353,0.028064998167040094,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_professional_psychology,0.7222222222222222,0.018120224251484587,"acc,none",8030261248,5.0,answer,0.7222222222222222,0.018120224251484587,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_public_relations,0.7363636363636363,0.04220224692971987,"acc,none",8030261248,5.0,answer,0.7363636363636363,0.04220224692971987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_security_studies,0.726530612244898,0.028535560337128445,"acc,none",8030261248,5.0,answer,0.726530612244898,0.028535560337128445,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_social_sciences,0.7637309067273318,0.007481993756042584,"acc,none",8030261248,,,0.7637309067273318,0.007481993756042584,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_sociology,0.8805970149253731,0.02292879327721974,"acc,none",8030261248,5.0,answer,0.8805970149253731,0.02292879327721974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_stem,0.5521725340945132,0.00850210540398227,"acc,none",8030261248,,,0.5521725340945132,0.00850210540398227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_us_foreign_policy,0.88,0.03265986323710905,"acc,none",8030261248,5.0,answer,0.88,0.03265986323710905,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_virology,0.5662650602409639,0.03858158940685517,"acc,none",8030261248,5.0,answer,0.5662650602409639,0.03858158940685517,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_world_religions,0.8128654970760234,0.029913127232368036,"acc,none",8030261248,5.0,answer,0.8128654970760234,0.029913127232368036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,other,0.54200193112327,0.008610989594701456,"acc,none",8030261248,,,0.54200193112327,0.008610989594701456,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,social sciences,0.49041273968150795,0.008849322702152353,"acc,none",8030261248,,,0.49041273968150795,0.008849322702152353,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,stem,0.44402156676181415,0.008553742667352173,"acc,none",8030261248,,,0.44402156676181415,0.008553742667352173,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,truthfulqa_gen,0.34149326805385555,0.016600688619950833,"rouge1_acc,none",8030261248,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,21.008720522711336,0.7336018872891364,0.3561811505507956,0.01676379072844632,-3.124722099823706,0.7213074992957418,44.93669655271626,0.8843149537731662,0.34149326805385555,0.016600688619950833,-5.462640969860286,0.8755065045791989,29.898162940041654,0.9679408518061355,0.28151774785801714,0.01574402724825605,-5.757788559988837,0.9913047201873912,42.14288698905598,0.883642650692877,0.3427172582619339,0.016614949385347015,-5.687997367703529,0.8829608433181637
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,truthfulqa_mc1,0.2839657282741738,0.01578537085839674,"acc,none",8030261248,0.0,0,0.2839657282741738,0.01578537085839674,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,truthfulqa_mc2,0.45216712850729296,0.014324862042820553,"acc,none",8030261248,0.0,0,0.45216712850729296,0.014324862042820553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,winogrande,0.7774269928966061,0.011690933809712669,"acc,none",8030261248,5.0,,0.7774269928966061,0.011690933809712669,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,french_bench,french_bench_arc_challenge,0.525235243798118,0.014611498054709715,"acc_norm,none",8030261248,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.46535500427716,0.014594980731782444,0.525235243798118,0.014611498054709715,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,french_bench,french_bench_grammar,0.7394957983193278,0.04040491690453788,"acc,none",8030261248,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7394957983193278,0.04040491690453788,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,french_bench,french_bench_hellaswag,0.6658813450417649,0.0048814079808672155,"acc_norm,none",8030261248,5.0,{{label}},0.49411008781323623,0.005174116351482882,0.6658813450417649,0.0048814079808672155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,french_bench,french_bench_vocab,0.8319327731092437,0.03442267607655235,"acc,none",8030261248,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.8319327731092437,0.03442267607655235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,french_bench_gen,french_bench_fquadv2_genq,0.3726495465090799,0.0,"rouge1,none",8030261248,5.0,{{question}},,,,,0.3726495465090799,N/A,0.3524072146704473,0.011134748673265946,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,french_bench_gen,french_bench_fquadv2_hasAns,0.8284015217056384,0.0,"rouge1,none",8030261248,5.0,{% if answers.text| length > 0 %}{{answers.text[0]}}{% else %}{{['Impossible']}}{% endif %},,,,,0.8284015217056384,N/A,0.8223405423545546,0.01339507852265242,0.56,0.02485042976789578,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,french_bench_gen,french_bench_multifquad,0.6954563959446698,0.0,"rouge1,none",8030261248,5.0,"{{', '.join(answers.text)}}",,,,,0.6954563959446698,N/A,0.6882215717341086,0.01349046405980234,0.18,0.019233429544157698,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,french_bench_gen,french_bench_orangesum_abstract,0.3358299543247494,0.0,"rouge1,none",8030261248,5.0,{{summary}},,,,,0.3358299543247494,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,french_bench_gen,french_bench_trivia,0.7268367860209966,0.0,"rouge1,none",8030261248,5.0,{{Answer}},,,,,0.7268367860209966,N/A,0.7179602853287065,0.020128258774905962,0.5842105263157895,0.02531639301514226,0.5657894736842105,0.025459976617683008,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,okapi,arc_de,0.48417450812660395,0.014622813435858917,"acc_norm,none",8030261248,25.0,gold,0.4508126603934987,0.014559179118156088,0.48417450812660395,0.014622813435858917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,okapi,arc_es,0.5307692307692308,0.014596168053496518,"acc_norm,none",8030261248,25.0,gold,0.49572649572649574,0.014623350357307396,0.5307692307692308,0.014596168053496518,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,okapi,arc_fr,0.5183917878528657,0.014620242527326811,"acc_norm,none",8030261248,25.0,gold,0.47904191616766467,0.014617285423513595,0.5183917878528657,0.014620242527326811,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,okapi,arc_it,0.5355004277159966,0.0145932206426823,"acc_norm,none",8030261248,25.0,gold,0.4875962360992301,0.01462564091328053,0.5355004277159966,0.0145932206426823,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,okapi,m_mmlu_de,0.5807059888369287,0.004285631781324015,"acc,none",8030261248,25.0,answer,0.5807059888369287,0.004285631781324015,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,okapi,m_mmlu_es,0.6059697015149242,0.004231811595853265,"acc,none",8030261248,25.0,answer,0.6059697015149242,0.004231811595853265,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,okapi,m_mmlu_fr,0.6033916431135895,0.004275734722885748,"acc,none",8030261248,25.0,answer,0.6033916431135895,0.004275734722885748,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,okapi,m_mmlu_it,0.5923547631638589,0.004271237780310981,"acc,none",8030261248,25.0,answer,0.5923547631638589,0.004271237780310981,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,arc_challenge,0.5921501706484642,0.014361097288449712,"acc_norm,none",8030261248,25.0,{{choices.label.index(answerKey)}},0.5571672354948806,0.0145155738733489,0.5921501706484642,0.014361097288449712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,gsm8k,0.7808946171341926,0.01139370663497807,"exact_match,flexible-extract",8030261248,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.7634571645185747,0.011705488202961672,0.7808946171341926,0.01139370663497807,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,hellaswag,0.8027285401314479,0.003971257040386402,"acc_norm,none",8030261248,10.0,{{label}},0.5945030870344553,0.00489984508718311,0.8027285401314479,0.003971257040386402,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,humanities,0.3846971307120085,0.0069596962377956074,"acc,none",8030261248,,,0.3846971307120085,0.0069596962377956074,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu,0.6819541375872383,0.003747255876305419,"acc,none",8030261248,,,0.6819541375872383,0.003747255876305419,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_abstract_algebra,0.32,0.046882617226215034,"acc,none",8030261248,5.0,answer,0.32,0.046882617226215034,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_anatomy,0.6962962962962963,0.039725528847851375,"acc,none",8030261248,5.0,answer,0.6962962962962963,0.039725528847851375,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_astronomy,0.756578947368421,0.034923496688842384,"acc,none",8030261248,5.0,answer,0.756578947368421,0.034923496688842384,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_business_ethics,0.72,0.04512608598542128,"acc,none",8030261248,5.0,answer,0.72,0.04512608598542128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_clinical_knowledge,0.7547169811320755,0.026480357179895688,"acc,none",8030261248,5.0,answer,0.7547169811320755,0.026480357179895688,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_college_biology,0.8055555555555556,0.033096151770590054,"acc,none",8030261248,5.0,answer,0.8055555555555556,0.033096151770590054,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_college_chemistry,0.48,0.05021167315686779,"acc,none",8030261248,5.0,answer,0.48,0.05021167315686779,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_college_computer_science,0.58,0.04960449637488583,"acc,none",8030261248,5.0,answer,0.58,0.04960449637488583,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_college_mathematics,0.39,0.04902071300001974,"acc,none",8030261248,5.0,answer,0.39,0.04902071300001974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_college_medicine,0.653179190751445,0.03629146670159663,"acc,none",8030261248,5.0,answer,0.653179190751445,0.03629146670159663,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_college_physics,0.45098039215686275,0.04951218252396262,"acc,none",8030261248,5.0,answer,0.45098039215686275,0.04951218252396262,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_computer_security,0.81,0.03942772444036625,"acc,none",8030261248,5.0,answer,0.81,0.03942772444036625,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_conceptual_physics,0.625531914893617,0.031639106653672915,"acc,none",8030261248,5.0,answer,0.625531914893617,0.031639106653672915,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation,0.46531833072211937,0.004064900045188249,"acc,none",8030261248,,,0.46531833072211937,0.004064900045188249,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_abstract_algebra,0.26,0.044084400227680794,"acc_norm,none",8030261248,0.0,{{answer}},0.29,0.04560480215720683,0.26,0.044084400227680794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_anatomy,0.5777777777777777,0.042667634040995814,"acc_norm,none",8030261248,0.0,{{answer}},0.5925925925925926,0.04244633238353228,0.5777777777777777,0.042667634040995814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_astronomy,0.5723684210526315,0.04026097083296564,"acc_norm,none",8030261248,0.0,{{answer}},0.4605263157894737,0.04056242252249034,0.5723684210526315,0.04026097083296564,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_business_ethics,0.66,0.04760952285695237,"acc_norm,none",8030261248,0.0,{{answer}},0.73,0.044619604333847394,0.66,0.04760952285695237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_clinical_knowledge,0.5433962264150943,0.030656748696739435,"acc_norm,none",8030261248,0.0,{{answer}},0.4528301886792453,0.030635627957961823,0.5433962264150943,0.030656748696739435,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_college_biology,0.5833333333333334,0.04122728707651281,"acc_norm,none",8030261248,0.0,{{answer}},0.6041666666666666,0.04089465449325582,0.5833333333333334,0.04122728707651281,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_college_chemistry,0.4,0.04923659639173309,"acc_norm,none",8030261248,0.0,{{answer}},0.36,0.04824181513244218,0.4,0.04923659639173309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_college_computer_science,0.46,0.05009082659620333,"acc_norm,none",8030261248,0.0,{{answer}},0.4,0.04923659639173309,0.46,0.05009082659620333,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_college_mathematics,0.31,0.04648231987117316,"acc_norm,none",8030261248,0.0,{{answer}},0.29,0.04560480215720684,0.31,0.04648231987117316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_college_medicine,0.4797687861271676,0.03809342081273956,"acc_norm,none",8030261248,0.0,{{answer}},0.44508670520231214,0.03789401760283647,0.4797687861271676,0.03809342081273956,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_college_physics,0.3137254901960784,0.04617034827006718,"acc_norm,none",8030261248,0.0,{{answer}},0.3137254901960784,0.04617034827006718,0.3137254901960784,0.04617034827006718,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_computer_security,0.6,0.04923659639173309,"acc_norm,none",8030261248,0.0,{{answer}},0.55,0.05,0.6,0.04923659639173309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_conceptual_physics,0.5319148936170213,0.03261936918467382,"acc_norm,none",8030261248,0.0,{{answer}},0.5659574468085107,0.03240038086792747,0.5319148936170213,0.03261936918467382,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_econometrics,0.34210526315789475,0.04462917535336936,"acc_norm,none",8030261248,0.0,{{answer}},0.43859649122807015,0.04668000738510455,0.34210526315789475,0.04462917535336936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_electrical_engineering,0.43448275862068964,0.04130740879555498,"acc_norm,none",8030261248,0.0,{{answer}},0.41379310344827586,0.04104269211806232,0.43448275862068964,0.04130740879555498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_elementary_mathematics,0.7486772486772487,0.0223404823396439,"acc_norm,none",8030261248,0.0,{{answer}},0.7433862433862434,0.022494510767503154,0.7486772486772487,0.0223404823396439,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_formal_logic,0.4444444444444444,0.04444444444444449,"acc_norm,none",8030261248,0.0,{{answer}},0.4365079365079365,0.04435932892851466,0.4444444444444444,0.04444444444444449,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_global_facts,0.55,0.04999999999999999,"acc_norm,none",8030261248,0.0,{{answer}},0.56,0.04988876515698589,0.55,0.04999999999999999,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_biology,0.5838709677419355,0.028040981380761536,"acc_norm,none",8030261248,0.0,{{answer}},0.5064516129032258,0.028441638233540505,0.5838709677419355,0.028040981380761536,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_chemistry,0.3891625615763547,0.03430462416103872,"acc_norm,none",8030261248,0.0,{{answer}},0.3399014778325123,0.0333276906841079,0.3891625615763547,0.03430462416103872,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_computer_science,0.55,0.049999999999999996,"acc_norm,none",8030261248,0.0,{{answer}},0.51,0.05024183937956911,0.55,0.049999999999999996,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_european_history,0.5272727272727272,0.03898531605579418,"acc_norm,none",8030261248,0.0,{{answer}},0.37575757575757573,0.03781887353205982,0.5272727272727272,0.03898531605579418,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_geography,0.6060606060606061,0.03481285338232963,"acc_norm,none",8030261248,0.0,{{answer}},0.5656565656565656,0.035315058793591834,0.6060606060606061,0.03481285338232963,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_government_and_politics,0.5803108808290155,0.035615873276858834,"acc_norm,none",8030261248,0.0,{{answer}},0.5647668393782384,0.03578038165008586,0.5803108808290155,0.035615873276858834,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_macroeconomics,0.47435897435897434,0.02531764972644866,"acc_norm,none",8030261248,0.0,{{answer}},0.4461538461538462,0.02520357177302833,0.47435897435897434,0.02531764972644866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_mathematics,0.44814814814814813,0.030321167196316286,"acc_norm,none",8030261248,0.0,{{answer}},0.3962962962962963,0.029822619458533997,0.44814814814814813,0.030321167196316286,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_microeconomics,0.5798319327731093,0.03206183783236153,"acc_norm,none",8030261248,0.0,{{answer}},0.47478991596638653,0.03243718055137411,0.5798319327731093,0.03206183783236153,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_physics,0.3443708609271523,0.03879687024073327,"acc_norm,none",8030261248,0.0,{{answer}},0.32450331125827814,0.03822746937658754,0.3443708609271523,0.03879687024073327,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_psychology,0.6256880733944954,0.020748959408988313,"acc_norm,none",8030261248,0.0,{{answer}},0.6807339449541284,0.01998782906975001,0.6256880733944954,0.020748959408988313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_statistics,0.4074074074074074,0.03350991604696043,"acc_norm,none",8030261248,0.0,{{answer}},0.35185185185185186,0.03256850570293647,0.4074074074074074,0.03350991604696043,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_us_history,0.5294117647058824,0.03503235296367993,"acc_norm,none",8030261248,0.0,{{answer}},0.4411764705882353,0.034849415144292316,0.5294117647058824,0.03503235296367993,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_high_school_world_history,0.5316455696202531,0.032481974005110756,"acc_norm,none",8030261248,0.0,{{answer}},0.45147679324894513,0.032393600173974704,0.5316455696202531,0.032481974005110756,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_human_aging,0.5515695067264574,0.03337883736255098,"acc_norm,none",8030261248,0.0,{{answer}},0.5381165919282511,0.033460150119732274,0.5515695067264574,0.03337883736255098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_human_sexuality,0.44274809160305345,0.04356447202665069,"acc_norm,none",8030261248,0.0,{{answer}},0.5267175572519084,0.04379024936553893,0.44274809160305345,0.04356447202665069,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_international_law,0.4793388429752066,0.04560456086387235,"acc_norm,none",8030261248,0.0,{{answer}},0.2975206611570248,0.04173349148083499,0.4793388429752066,0.04560456086387235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_jurisprudence,0.5185185185185185,0.04830366024635331,"acc_norm,none",8030261248,0.0,{{answer}},0.42592592592592593,0.0478034362693679,0.5185185185185185,0.04830366024635331,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_logical_fallacies,0.5030674846625767,0.03928297078179662,"acc_norm,none",8030261248,0.0,{{answer}},0.49079754601226994,0.03927705600787443,0.5030674846625767,0.03928297078179662,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_machine_learning,0.39285714285714285,0.046355501356099754,"acc_norm,none",8030261248,0.0,{{answer}},0.42857142857142855,0.04697113923010212,0.39285714285714285,0.046355501356099754,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_management,0.6213592233009708,0.04802694698258974,"acc_norm,none",8030261248,0.0,{{answer}},0.5436893203883495,0.049318019942204146,0.6213592233009708,0.04802694698258974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_marketing,0.6666666666666666,0.030882736974138642,"acc_norm,none",8030261248,0.0,{{answer}},0.7008547008547008,0.029996951858349486,0.6666666666666666,0.030882736974138642,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_medical_genetics,0.63,0.04852365870939099,"acc_norm,none",8030261248,0.0,{{answer}},0.55,0.049999999999999996,0.63,0.04852365870939099,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_miscellaneous,0.7100893997445722,0.016225017944770975,"acc_norm,none",8030261248,0.0,{{answer}},0.7139208173690932,0.016160871405127543,0.7100893997445722,0.016225017944770975,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_moral_disputes,0.3872832369942196,0.026226158605124655,"acc_norm,none",8030261248,0.0,{{answer}},0.37283236994219654,0.026033890613576288,0.3872832369942196,0.026226158605124655,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_moral_scenarios,0.27262569832402234,0.014893391735249603,"acc_norm,none",8030261248,0.0,{{answer}},0.33743016759776534,0.015813901283913048,0.27262569832402234,0.014893391735249603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_nutrition,0.49673202614379086,0.02862930519400354,"acc_norm,none",8030261248,0.0,{{answer}},0.39869281045751637,0.028036092273891762,0.49673202614379086,0.02862930519400354,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_philosophy,0.5048231511254019,0.028396770444111298,"acc_norm,none",8030261248,0.0,{{answer}},0.4694533762057878,0.02834504586484068,0.5048231511254019,0.028396770444111298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_prehistory,0.5339506172839507,0.027756535257347663,"acc_norm,none",8030261248,0.0,{{answer}},0.5308641975308642,0.027767689606833935,0.5339506172839507,0.027756535257347663,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_professional_accounting,0.32978723404255317,0.02804594694204241,"acc_norm,none",8030261248,0.0,{{answer}},0.3617021276595745,0.02866382014719949,0.32978723404255317,0.02804594694204241,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_professional_law,0.3239895697522816,0.011952840809646568,"acc_norm,none",8030261248,0.0,{{answer}},0.30247718383311606,0.0117315242341657,0.3239895697522816,0.011952840809646568,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_professional_medicine,0.5,0.030372836961539352,"acc_norm,none",8030261248,0.0,{{answer}},0.5257352941176471,0.030332578094555033,0.5,0.030372836961539352,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_professional_psychology,0.477124183006536,0.020206653187884782,"acc_norm,none",8030261248,0.0,{{answer}},0.434640522875817,0.02005426920072646,0.477124183006536,0.020206653187884782,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_public_relations,0.4090909090909091,0.047093069786618966,"acc_norm,none",8030261248,0.0,{{answer}},0.4818181818181818,0.04785964010794916,0.4090909090909091,0.047093069786618966,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_security_studies,0.30612244897959184,0.029504896454595957,"acc_norm,none",8030261248,0.0,{{answer}},0.3346938775510204,0.03020923522624231,0.30612244897959184,0.029504896454595957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_sociology,0.5174129353233831,0.03533389234739245,"acc_norm,none",8030261248,0.0,{{answer}},0.4079601990049751,0.034751163651940926,0.5174129353233831,0.03533389234739245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_us_foreign_policy,0.53,0.050161355804659205,"acc_norm,none",8030261248,0.0,{{answer}},0.49,0.05024183937956912,0.53,0.050161355804659205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_virology,0.40963855421686746,0.038284011150790206,"acc_norm,none",8030261248,0.0,{{answer}},0.3493975903614458,0.0371172519074075,0.40963855421686746,0.038284011150790206,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_continuation_world_religions,0.7602339181286549,0.03274485211946956,"acc_norm,none",8030261248,0.0,{{answer}},0.7076023391812866,0.03488647713457923,0.7602339181286549,0.03274485211946956,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_econometrics,0.5,0.047036043419179864,"acc,none",8030261248,5.0,answer,0.5,0.047036043419179864,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_electrical_engineering,0.7172413793103448,0.03752833958003336,"acc,none",8030261248,5.0,answer,0.7172413793103448,0.03752833958003336,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_elementary_mathematics,0.4894179894179894,0.02574554227604549,"acc,none",8030261248,5.0,answer,0.4894179894179894,0.02574554227604549,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_formal_logic,0.5555555555555556,0.04444444444444449,"acc,none",8030261248,5.0,answer,0.5555555555555556,0.04444444444444449,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_global_facts,0.39,0.04902071300001975,"acc,none",8030261248,5.0,answer,0.39,0.04902071300001975,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_biology,0.8193548387096774,0.021886178567172537,"acc,none",8030261248,5.0,answer,0.8193548387096774,0.021886178567172537,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_chemistry,0.6206896551724138,0.034139638059062345,"acc,none",8030261248,5.0,answer,0.6206896551724138,0.034139638059062345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_computer_science,0.76,0.042923469599092816,"acc,none",8030261248,5.0,answer,0.76,0.042923469599092816,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_european_history,0.7575757575757576,0.03346409881055953,"acc,none",8030261248,5.0,answer,0.7575757575757576,0.03346409881055953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_geography,0.8535353535353535,0.02519092111460393,"acc,none",8030261248,5.0,answer,0.8535353535353535,0.02519092111460393,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_government_and_politics,0.917098445595855,0.01989934131572178,"acc,none",8030261248,5.0,answer,0.917098445595855,0.01989934131572178,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_macroeconomics,0.6743589743589744,0.02375966576741229,"acc,none",8030261248,5.0,answer,0.6743589743589744,0.02375966576741229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_mathematics,0.43703703703703706,0.030242862397654002,"acc,none",8030261248,5.0,answer,0.43703703703703706,0.030242862397654002,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_microeconomics,0.7773109243697479,0.027025433498882367,"acc,none",8030261248,5.0,answer,0.7773109243697479,0.027025433498882367,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_physics,0.47019867549668876,0.04075224992216979,"acc,none",8030261248,5.0,answer,0.47019867549668876,0.04075224992216979,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_psychology,0.8660550458715597,0.014602811435592635,"acc,none",8030261248,5.0,answer,0.8660550458715597,0.014602811435592635,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_statistics,0.5740740740740741,0.033723432716530624,"acc,none",8030261248,5.0,answer,0.5740740740740741,0.033723432716530624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_us_history,0.8382352941176471,0.025845017986926927,"acc,none",8030261248,5.0,answer,0.8382352941176471,0.025845017986926927,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_high_school_world_history,0.8396624472573839,0.023884380925965676,"acc,none",8030261248,5.0,answer,0.8396624472573839,0.023884380925965676,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_human_aging,0.6860986547085202,0.031146796482972465,"acc,none",8030261248,5.0,answer,0.6860986547085202,0.031146796482972465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_human_sexuality,0.8015267175572519,0.03498149385462472,"acc,none",8030261248,5.0,answer,0.8015267175572519,0.03498149385462472,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_humanities,0.6431455897980871,0.006718399953583552,"acc,none",8030261248,,,0.6431455897980871,0.006718399953583552,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_international_law,0.8347107438016529,0.03390780612972776,"acc,none",8030261248,5.0,answer,0.8347107438016529,0.03390780612972776,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_jurisprudence,0.7685185185185185,0.04077494709252628,"acc,none",8030261248,5.0,answer,0.7685185185185185,0.04077494709252628,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_logical_fallacies,0.7975460122699386,0.031570650789119005,"acc,none",8030261248,5.0,answer,0.7975460122699386,0.031570650789119005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_machine_learning,0.5714285714285714,0.04697113923010212,"acc,none",8030261248,5.0,answer,0.5714285714285714,0.04697113923010212,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_management,0.8155339805825242,0.03840423627288276,"acc,none",8030261248,5.0,answer,0.8155339805825242,0.03840423627288276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_marketing,0.8760683760683761,0.02158649400128138,"acc,none",8030261248,5.0,answer,0.8760683760683761,0.02158649400128138,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_medical_genetics,0.85,0.03588702812826371,"acc,none",8030261248,5.0,answer,0.85,0.03588702812826371,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_miscellaneous,0.8263090676883781,0.01354741565866226,"acc,none",8030261248,5.0,answer,0.8263090676883781,0.01354741565866226,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_moral_disputes,0.7687861271676301,0.022698657167855716,"acc,none",8030261248,5.0,answer,0.7687861271676301,0.022698657167855716,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_moral_scenarios,0.5441340782122905,0.01665722942458631,"acc,none",8030261248,5.0,answer,0.5441340782122905,0.01665722942458631,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_nutrition,0.7712418300653595,0.024051029739912258,"acc,none",8030261248,5.0,answer,0.7712418300653595,0.024051029739912258,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_other,0.7286771805600257,0.00766091945566121,"acc,none",8030261248,,,0.7286771805600257,0.00766091945566121,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_philosophy,0.7234726688102894,0.025403832978179622,"acc,none",8030261248,5.0,answer,0.7234726688102894,0.025403832978179622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_prehistory,0.75,0.02409347123262133,"acc,none",8030261248,5.0,answer,0.75,0.02409347123262133,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_professional_accounting,0.5070921985815603,0.02982449855912901,"acc,none",8030261248,5.0,answer,0.5070921985815603,0.02982449855912901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_professional_law,0.5104302477183833,0.012767457253930655,"acc,none",8030261248,5.0,answer,0.5104302477183833,0.012767457253930655,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_professional_medicine,0.7426470588235294,0.02655651947004152,"acc,none",8030261248,5.0,answer,0.7426470588235294,0.02655651947004152,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_professional_psychology,0.7238562091503268,0.018087276935663137,"acc,none",8030261248,5.0,answer,0.7238562091503268,0.018087276935663137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_public_relations,0.7,0.04389311454644287,"acc,none",8030261248,5.0,answer,0.7,0.04389311454644287,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_security_studies,0.7306122448979592,0.02840125202902294,"acc,none",8030261248,5.0,answer,0.7306122448979592,0.02840125202902294,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_social_sciences,0.7760805979850504,0.007333229464721674,"acc,none",8030261248,,,0.7760805979850504,0.007333229464721674,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_sociology,0.8656716417910447,0.024112678240900826,"acc,none",8030261248,5.0,answer,0.8656716417910447,0.024112678240900826,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_stem,0.6019663812242309,0.008354642145800634,"acc,none",8030261248,,,0.6019663812242309,0.008354642145800634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_us_foreign_policy,0.87,0.03379976689896309,"acc,none",8030261248,5.0,answer,0.87,0.03379976689896309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_virology,0.5120481927710844,0.03891364495835817,"acc,none",8030261248,5.0,answer,0.5120481927710844,0.03891364495835817,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,mmlu_world_religions,0.8362573099415205,0.028380919596145866,"acc,none",8030261248,5.0,answer,0.8362573099415205,0.028380919596145866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,other,0.548760862568394,0.008602933271808609,"acc,none",8030261248,,,0.548760862568394,0.008602933271808609,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,social sciences,0.4972375690607735,0.008839575967843693,"acc,none",8030261248,,,0.4972375690607735,0.008839575967843693,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,stem,0.47224865207738664,0.008578226645812165,"acc,none",8030261248,,,0.47224865207738664,0.008578226645812165,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,truthfulqa_gen,0.605875152998776,0.01710658814070032,"rouge1_acc,none",8030261248,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,35.96708193335139,0.8807760917926095,0.6230110159118727,0.016965517578930348,15.352993467603595,1.0859688429596,60.642045853324674,0.9968424200475688,0.605875152998776,0.01710658814070032,21.18701206512606,1.5887238945812316,48.13955173949256,1.2082639806260551,0.5581395348837209,0.017384767478986218,22.792093084483824,1.6366087528058764,58.79266771839251,1.0293840621250667,0.6107711138310894,0.01706855268069031,21.364754522511923,1.6009676896420084
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,truthfulqa_mc1,0.3708690330477356,0.01690969358024883,"acc,none",8030261248,0.0,0,0.3708690330477356,0.01690969358024883,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,truthfulqa_mc2,0.541343747007086,0.014967576972440048,"acc,none",8030261248,0.0,0,0.541343747007086,0.014967576972440048,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B-Instruct,15000000000000,openllm,winogrande,0.7742699289660616,0.011749626260902556,"acc,none",8030261248,5.0,,0.7742699289660616,0.011749626260902556,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,french_bench,french_bench_arc_challenge,0.5406330196749358,0.01458175340237769,"acc_norm,none",7241732096,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.48674080410607357,0.014624998352776109,0.5406330196749358,0.01458175340237769,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,french_bench,french_bench_grammar,0.7647058823529411,0.039049164561447976,"acc,none",7241732096,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7647058823529411,0.039049164561447976,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,french_bench,french_bench_hellaswag,0.6770186335403726,0.004839331416795899,"acc_norm,none",7241732096,5.0,{{label}},0.5125294495609338,0.0051728504731054035,0.6770186335403726,0.004839331416795899,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,french_bench,french_bench_vocab,0.7563025210084033,0.039521397092913316,"acc,none",7241732096,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7563025210084033,0.039521397092913316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,french_bench_gen,french_bench_fquadv2_genq,0.3294025986705468,0.0,"rouge1,none",7241732096,5.0,{{question}},,,,,0.3294025986705468,N/A,0.30709088687496194,0.011231271737798575,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,french_bench_gen,french_bench_fquadv2_hasAns,0.7153423942653181,0.0,"rouge1,none",7241732096,5.0,{% if answers.text| length > 0 %}{{answers.text[0]}}{% else %}{{['Impossible']}}{% endif %},,,,,0.7153423942653181,N/A,0.6999137156420211,0.01670329292122933,0.405,0.024575340657273674,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,french_bench_gen,french_bench_multifquad,0.6213534039118727,0.0,"rouge1,none",7241732096,5.0,"{{', '.join(answers.text)}}",,,,,0.6213534039118727,N/A,0.5991413946148872,0.01405122265298688,0.0975,0.014850444918779895,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,french_bench_gen,french_bench_orangesum_abstract,0.3283365529580934,0.0,"rouge1,none",7241732096,5.0,{{summary}},,,,,0.3283365529580934,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,french_bench_gen,french_bench_trivia,0.68062645841608,0.0,"rouge1,none",7241732096,5.0,{{Answer}},,,,,0.68062645841608,N/A,0.6701604846223912,0.02152039244174793,0.5578947368421052,0.025510523434968492,0.55,0.02555453581639776,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,arc_challenge,0.6382252559726962,0.014041957945038078,"acc_norm,none",7241732096,25.0,{{choices.label.index(answerKey)}},0.6023890784982935,0.014301752223279543,0.6382252559726962,0.014041957945038078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,gsm8k,0.44200151630022744,0.013679514492814581,"exact_match,flexible-extract",7241732096,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.44124336618650495,0.01367705947859264,0.44200151630022744,0.013679514492814581,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,hellaswag,0.8451503684524995,0.003610219413061354,"acc_norm,none",7241732096,10.0,{{label}},0.6620195180242979,0.004720551323547124,0.8451503684524995,0.003610219413061354,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,humanities,0.383634431455898,0.006906357251881411,"acc,none",7241732096,,,0.383634431455898,0.006906357251881411,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu,0.5910126762569434,0.003936938905881521,"acc,none",7241732096,,,0.5910126762569434,0.003936938905881521,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_abstract_algebra,0.31,0.04648231987117316,"acc,none",7241732096,5.0,answer,0.31,0.04648231987117316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_anatomy,0.5777777777777777,0.04266763404099582,"acc,none",7241732096,5.0,answer,0.5777777777777777,0.04266763404099582,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_astronomy,0.6052631578947368,0.039777499346220734,"acc,none",7241732096,5.0,answer,0.6052631578947368,0.039777499346220734,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_business_ethics,0.58,0.049604496374885836,"acc,none",7241732096,5.0,answer,0.58,0.049604496374885836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_clinical_knowledge,0.6754716981132075,0.028815615713432118,"acc,none",7241732096,5.0,answer,0.6754716981132075,0.028815615713432118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_college_biology,0.6736111111111112,0.03921067198982266,"acc,none",7241732096,5.0,answer,0.6736111111111112,0.03921067198982266,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_college_chemistry,0.41,0.04943110704237102,"acc,none",7241732096,5.0,answer,0.41,0.04943110704237102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_college_computer_science,0.51,0.05024183937956912,"acc,none",7241732096,5.0,answer,0.51,0.05024183937956912,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_college_mathematics,0.4,0.04923659639173309,"acc,none",7241732096,5.0,answer,0.4,0.04923659639173309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_college_medicine,0.6011560693641619,0.037336266553835096,"acc,none",7241732096,5.0,answer,0.6011560693641619,0.037336266553835096,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_college_physics,0.43137254901960786,0.04928099597287534,"acc,none",7241732096,5.0,answer,0.43137254901960786,0.04928099597287534,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_computer_security,0.68,0.046882617226215034,"acc,none",7241732096,5.0,answer,0.68,0.046882617226215034,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_conceptual_physics,0.5404255319148936,0.03257901482099834,"acc,none",7241732096,5.0,answer,0.5404255319148936,0.03257901482099834,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation,0.45769833357071643,0.004059492201312392,"acc,none",7241732096,,,0.45769833357071643,0.004059492201312392,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_abstract_algebra,0.3,0.046056618647183814,"acc_norm,none",7241732096,0.0,{{answer}},0.27,0.04461960433384741,0.3,0.046056618647183814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_anatomy,0.5555555555555556,0.04292596718256981,"acc_norm,none",7241732096,0.0,{{answer}},0.562962962962963,0.04284958639753401,0.5555555555555556,0.04292596718256981,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_astronomy,0.5723684210526315,0.040260970832965634,"acc_norm,none",7241732096,0.0,{{answer}},0.4934210526315789,0.04068590050224971,0.5723684210526315,0.040260970832965634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_business_ethics,0.61,0.04902071300001974,"acc_norm,none",7241732096,0.0,{{answer}},0.65,0.047937248544110196,0.61,0.04902071300001974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_clinical_knowledge,0.569811320754717,0.03047144586718324,"acc_norm,none",7241732096,0.0,{{answer}},0.4830188679245283,0.030755120364119905,0.569811320754717,0.03047144586718324,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_college_biology,0.5069444444444444,0.04180806750294938,"acc_norm,none",7241732096,0.0,{{answer}},0.5208333333333334,0.041775789507399935,0.5069444444444444,0.04180806750294938,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_college_chemistry,0.38,0.04878317312145633,"acc_norm,none",7241732096,0.0,{{answer}},0.38,0.04878317312145633,0.38,0.04878317312145633,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_college_computer_science,0.41,0.04943110704237102,"acc_norm,none",7241732096,0.0,{{answer}},0.37,0.048523658709391,0.41,0.04943110704237102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_college_mathematics,0.33,0.04725815626252604,"acc_norm,none",7241732096,0.0,{{answer}},0.24,0.04292346959909283,0.33,0.04725815626252604,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_college_medicine,0.48554913294797686,0.03810871630454764,"acc_norm,none",7241732096,0.0,{{answer}},0.44508670520231214,0.03789401760283648,0.48554913294797686,0.03810871630454764,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_college_physics,0.3627450980392157,0.04784060704105654,"acc_norm,none",7241732096,0.0,{{answer}},0.35294117647058826,0.04755129616062949,0.3627450980392157,0.04784060704105654,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_computer_security,0.62,0.04878317312145633,"acc_norm,none",7241732096,0.0,{{answer}},0.56,0.049888765156985884,0.62,0.04878317312145633,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_conceptual_physics,0.5531914893617021,0.032500536843658404,"acc_norm,none",7241732096,0.0,{{answer}},0.548936170212766,0.032529096196131965,0.5531914893617021,0.032500536843658404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_econometrics,0.32456140350877194,0.044045561573747664,"acc_norm,none",7241732096,0.0,{{answer}},0.35964912280701755,0.04514496132873633,0.32456140350877194,0.044045561573747664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_electrical_engineering,0.38620689655172413,0.04057324734419036,"acc_norm,none",7241732096,0.0,{{answer}},0.38620689655172413,0.04057324734419036,0.38620689655172413,0.04057324734419036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_elementary_mathematics,0.6402116402116402,0.024718075944129277,"acc_norm,none",7241732096,0.0,{{answer}},0.6216931216931217,0.024976954053155236,0.6402116402116402,0.024718075944129277,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_formal_logic,0.3968253968253968,0.0437588849272706,"acc_norm,none",7241732096,0.0,{{answer}},0.4126984126984127,0.04403438954768176,0.3968253968253968,0.0437588849272706,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_global_facts,0.48,0.050211673156867795,"acc_norm,none",7241732096,0.0,{{answer}},0.5,0.050251890762960605,0.48,0.050211673156867795,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_biology,0.5290322580645161,0.028396016402761005,"acc_norm,none",7241732096,0.0,{{answer}},0.4935483870967742,0.028441638233540515,0.5290322580645161,0.028396016402761005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_chemistry,0.3251231527093596,0.03295797566311271,"acc_norm,none",7241732096,0.0,{{answer}},0.3251231527093596,0.03295797566311271,0.3251231527093596,0.03295797566311271,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_computer_science,0.6,0.049236596391733084,"acc_norm,none",7241732096,0.0,{{answer}},0.52,0.050211673156867795,0.6,0.049236596391733084,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_european_history,0.5818181818181818,0.03851716319398394,"acc_norm,none",7241732096,0.0,{{answer}},0.4666666666666667,0.03895658065271846,0.5818181818181818,0.03851716319398394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_geography,0.5656565656565656,0.03531505879359183,"acc_norm,none",7241732096,0.0,{{answer}},0.5606060606060606,0.0353608594752948,0.5656565656565656,0.03531505879359183,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_government_and_politics,0.6269430051813472,0.03490205592048574,"acc_norm,none",7241732096,0.0,{{answer}},0.5803108808290155,0.03561587327685884,0.6269430051813472,0.03490205592048574,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_macroeconomics,0.46923076923076923,0.025302958890850154,"acc_norm,none",7241732096,0.0,{{answer}},0.4358974358974359,0.025141801511177495,0.46923076923076923,0.025302958890850154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_mathematics,0.337037037037037,0.028820884666253255,"acc_norm,none",7241732096,0.0,{{answer}},0.2851851851851852,0.027528599210340492,0.337037037037037,0.028820884666253255,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_microeconomics,0.6176470588235294,0.03156663099215416,"acc_norm,none",7241732096,0.0,{{answer}},0.5126050420168067,0.03246816765752174,0.6176470588235294,0.03156663099215416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_physics,0.31125827814569534,0.03780445850526733,"acc_norm,none",7241732096,0.0,{{answer}},0.32450331125827814,0.038227469376587525,0.31125827814569534,0.03780445850526733,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_psychology,0.6697247706422018,0.02016446633634298,"acc_norm,none",7241732096,0.0,{{answer}},0.673394495412844,0.020106990889937303,0.6697247706422018,0.02016446633634298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_statistics,0.3888888888888889,0.03324708911809117,"acc_norm,none",7241732096,0.0,{{answer}},0.375,0.033016908987210894,0.3888888888888889,0.03324708911809117,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_us_history,0.5686274509803921,0.034760990605016355,"acc_norm,none",7241732096,0.0,{{answer}},0.4852941176470588,0.03507793834791324,0.5686274509803921,0.034760990605016355,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_high_school_world_history,0.510548523206751,0.032539983791662855,"acc_norm,none",7241732096,0.0,{{answer}},0.4641350210970464,0.03246338898055659,0.510548523206751,0.032539983791662855,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_human_aging,0.5426008968609866,0.033435777055830646,"acc_norm,none",7241732096,0.0,{{answer}},0.5291479820627802,0.03350073248773403,0.5426008968609866,0.033435777055830646,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_human_sexuality,0.44274809160305345,0.04356447202665069,"acc_norm,none",7241732096,0.0,{{answer}},0.4732824427480916,0.04379024936553894,0.44274809160305345,0.04356447202665069,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_international_law,0.5785123966942148,0.04507732278775088,"acc_norm,none",7241732096,0.0,{{answer}},0.33884297520661155,0.043207678075366705,0.5785123966942148,0.04507732278775088,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_jurisprudence,0.5833333333333334,0.04766075165356461,"acc_norm,none",7241732096,0.0,{{answer}},0.4074074074074074,0.047500773411999854,0.5833333333333334,0.04766075165356461,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_logical_fallacies,0.50920245398773,0.03927705600787443,"acc_norm,none",7241732096,0.0,{{answer}},0.44171779141104295,0.03901591825836184,0.50920245398773,0.03927705600787443,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_machine_learning,0.41964285714285715,0.046840993210771065,"acc_norm,none",7241732096,0.0,{{answer}},0.42857142857142855,0.04697113923010212,0.41964285714285715,0.046840993210771065,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_management,0.6407766990291263,0.047504583990416946,"acc_norm,none",7241732096,0.0,{{answer}},0.5631067961165048,0.04911147107365777,0.6407766990291263,0.047504583990416946,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_marketing,0.6923076923076923,0.030236389942173092,"acc_norm,none",7241732096,0.0,{{answer}},0.688034188034188,0.030351527323344948,0.6923076923076923,0.030236389942173092,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_medical_genetics,0.62,0.04878317312145633,"acc_norm,none",7241732096,0.0,{{answer}},0.53,0.050161355804659205,0.62,0.04878317312145633,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_miscellaneous,0.719029374201788,0.016073127851221225,"acc_norm,none",7241732096,0.0,{{answer}},0.722860791826309,0.016005636294122418,0.719029374201788,0.016073127851221225,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_moral_disputes,0.45375722543352603,0.026803720583206188,"acc_norm,none",7241732096,0.0,{{answer}},0.4277456647398844,0.026636539741116082,0.45375722543352603,0.026803720583206188,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_moral_scenarios,0.27262569832402234,0.014893391735249603,"acc_norm,none",7241732096,0.0,{{answer}},0.2681564245810056,0.014816119635317006,0.27262569832402234,0.014893391735249603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_nutrition,0.46078431372549017,0.028541722692618874,"acc_norm,none",7241732096,0.0,{{answer}},0.3758169934640523,0.027732834353363944,0.46078431372549017,0.028541722692618874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_philosophy,0.5241157556270096,0.028365041542564584,"acc_norm,none",7241732096,0.0,{{answer}},0.4983922829581994,0.02839794490780661,0.5241157556270096,0.028365041542564584,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_prehistory,0.5,0.02782074420373286,"acc_norm,none",7241732096,0.0,{{answer}},0.5493827160493827,0.027684721415656203,0.5,0.02782074420373286,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_professional_accounting,0.35815602836879434,0.02860208586275942,"acc_norm,none",7241732096,0.0,{{answer}},0.375886524822695,0.028893955412115882,0.35815602836879434,0.02860208586275942,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_professional_law,0.3409387222946545,0.01210681720306721,"acc_norm,none",7241732096,0.0,{{answer}},0.3044328552803129,0.011752877592597565,0.3409387222946545,0.01210681720306721,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_professional_medicine,0.5661764705882353,0.030105636570016626,"acc_norm,none",7241732096,0.0,{{answer}},0.5514705882352942,0.030211479609121596,0.5661764705882353,0.030105636570016626,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_professional_psychology,0.4852941176470588,0.020219083895133924,"acc_norm,none",7241732096,0.0,{{answer}},0.4493464052287582,0.020123766528027266,0.4852941176470588,0.020219083895133924,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_public_relations,0.39090909090909093,0.046737523336702363,"acc_norm,none",7241732096,0.0,{{answer}},0.5,0.04789131426105757,0.39090909090909093,0.046737523336702363,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_security_studies,0.34285714285714286,0.030387262919547728,"acc_norm,none",7241732096,0.0,{{answer}},0.3346938775510204,0.03020923522624231,0.34285714285714286,0.030387262919547728,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_sociology,0.5323383084577115,0.03528131472933607,"acc_norm,none",7241732096,0.0,{{answer}},0.4527363184079602,0.03519702717576915,0.5323383084577115,0.03528131472933607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_us_foreign_policy,0.51,0.05024183937956911,"acc_norm,none",7241732096,0.0,{{answer}},0.44,0.049888765156985884,0.51,0.05024183937956911,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_virology,0.37349397590361444,0.037658451171688624,"acc_norm,none",7241732096,0.0,{{answer}},0.3192771084337349,0.03629335329947861,0.37349397590361444,0.037658451171688624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_continuation_world_religions,0.6783625730994152,0.03582529442573122,"acc_norm,none",7241732096,0.0,{{answer}},0.7134502923976608,0.03467826685703826,0.6783625730994152,0.03582529442573122,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_econometrics,0.41228070175438597,0.046306532033665956,"acc,none",7241732096,5.0,answer,0.41228070175438597,0.046306532033665956,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_electrical_engineering,0.6137931034482759,0.04057324734419036,"acc,none",7241732096,5.0,answer,0.6137931034482759,0.04057324734419036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_elementary_mathematics,0.38095238095238093,0.025010749116137595,"acc,none",7241732096,5.0,answer,0.38095238095238093,0.025010749116137595,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_formal_logic,0.3968253968253968,0.04375888492727062,"acc,none",7241732096,5.0,answer,0.3968253968253968,0.04375888492727062,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_global_facts,0.36,0.04824181513244218,"acc,none",7241732096,5.0,answer,0.36,0.04824181513244218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_biology,0.6516129032258065,0.027104826328100944,"acc,none",7241732096,5.0,answer,0.6516129032258065,0.027104826328100944,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_chemistry,0.5024630541871922,0.03517945038691063,"acc,none",7241732096,5.0,answer,0.5024630541871922,0.03517945038691063,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_computer_science,0.62,0.048783173121456316,"acc,none",7241732096,5.0,answer,0.62,0.048783173121456316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_european_history,0.7333333333333333,0.03453131801885417,"acc,none",7241732096,5.0,answer,0.7333333333333333,0.03453131801885417,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_geography,0.7575757575757576,0.030532892233932036,"acc,none",7241732096,5.0,answer,0.7575757575757576,0.030532892233932036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_government_and_politics,0.844559585492228,0.02614848346915332,"acc,none",7241732096,5.0,answer,0.844559585492228,0.02614848346915332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_macroeconomics,0.5666666666666667,0.025124653525885124,"acc,none",7241732096,5.0,answer,0.5666666666666667,0.025124653525885124,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_mathematics,0.3111111111111111,0.028226446749683515,"acc,none",7241732096,5.0,answer,0.3111111111111111,0.028226446749683515,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_microeconomics,0.6386554621848739,0.031204691225150013,"acc,none",7241732096,5.0,answer,0.6386554621848739,0.031204691225150013,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_physics,0.3443708609271523,0.03879687024073327,"acc,none",7241732096,5.0,answer,0.3443708609271523,0.03879687024073327,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_psychology,0.7944954128440367,0.01732435232501601,"acc,none",7241732096,5.0,answer,0.7944954128440367,0.01732435232501601,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_statistics,0.4537037037037037,0.03395322726375798,"acc,none",7241732096,5.0,answer,0.4537037037037037,0.03395322726375798,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_us_history,0.7450980392156863,0.030587591351604243,"acc,none",7241732096,5.0,answer,0.7450980392156863,0.030587591351604243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_high_school_world_history,0.7552742616033755,0.027985699387036416,"acc,none",7241732096,5.0,answer,0.7552742616033755,0.027985699387036416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_human_aging,0.6188340807174888,0.03259625118416827,"acc,none",7241732096,5.0,answer,0.6188340807174888,0.03259625118416827,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_human_sexuality,0.732824427480916,0.03880848301082395,"acc,none",7241732096,5.0,answer,0.732824427480916,0.03880848301082395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_humanities,0.5462274176408076,0.006827531911397889,"acc,none",7241732096,,,0.5462274176408076,0.006827531911397889,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_international_law,0.7851239669421488,0.037494924487096966,"acc,none",7241732096,5.0,answer,0.7851239669421488,0.037494924487096966,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_jurisprudence,0.7314814814814815,0.042844679680521934,"acc,none",7241732096,5.0,answer,0.7314814814814815,0.042844679680521934,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_logical_fallacies,0.7177914110429447,0.03536117886664743,"acc,none",7241732096,5.0,answer,0.7177914110429447,0.03536117886664743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_machine_learning,0.42857142857142855,0.04697113923010213,"acc,none",7241732096,5.0,answer,0.42857142857142855,0.04697113923010213,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_management,0.7378640776699029,0.04354631077260595,"acc,none",7241732096,5.0,answer,0.7378640776699029,0.04354631077260595,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_marketing,0.8632478632478633,0.022509033937077805,"acc,none",7241732096,5.0,answer,0.8632478632478633,0.022509033937077805,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_medical_genetics,0.66,0.04760952285695237,"acc,none",7241732096,5.0,answer,0.66,0.04760952285695237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_miscellaneous,0.776500638569604,0.014897235229450708,"acc,none",7241732096,5.0,answer,0.776500638569604,0.014897235229450708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_moral_disputes,0.6791907514450867,0.025131000233647907,"acc,none",7241732096,5.0,answer,0.6791907514450867,0.025131000233647907,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_moral_scenarios,0.33519553072625696,0.015788007190185884,"acc,none",7241732096,5.0,answer,0.33519553072625696,0.015788007190185884,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_nutrition,0.6797385620915033,0.02671611838015684,"acc,none",7241732096,5.0,answer,0.6797385620915033,0.02671611838015684,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_other,0.660122304473769,0.008209641961260119,"acc,none",7241732096,,,0.660122304473769,0.008209641961260119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_philosophy,0.6977491961414791,0.026082700695399662,"acc,none",7241732096,5.0,answer,0.6977491961414791,0.026082700695399662,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_prehistory,0.6820987654320988,0.02591006352824089,"acc,none",7241732096,5.0,answer,0.6820987654320988,0.02591006352824089,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_professional_accounting,0.45390070921985815,0.029700453247291477,"acc,none",7241732096,5.0,answer,0.45390070921985815,0.029700453247291477,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_professional_law,0.4302477183833116,0.012645361435115231,"acc,none",7241732096,5.0,answer,0.4302477183833116,0.012645361435115231,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_professional_medicine,0.6139705882352942,0.029573269134411127,"acc,none",7241732096,5.0,answer,0.6139705882352942,0.029573269134411127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_professional_psychology,0.630718954248366,0.01952431674486635,"acc,none",7241732096,5.0,answer,0.630718954248366,0.01952431674486635,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_public_relations,0.7272727272727273,0.04265792110940588,"acc,none",7241732096,5.0,answer,0.7272727272727273,0.04265792110940588,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_security_studies,0.710204081632653,0.029043088683304328,"acc,none",7241732096,5.0,answer,0.710204081632653,0.029043088683304328,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_social_sciences,0.6915827104322392,0.008140865973458164,"acc,none",7241732096,,,0.6915827104322392,0.008140865973458164,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_sociology,0.7164179104477612,0.03187187537919797,"acc,none",7241732096,5.0,answer,0.7164179104477612,0.03187187537919797,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_stem,0.4915953060577228,0.00866396328652113,"acc,none",7241732096,,,0.4915953060577228,0.00866396328652113,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_us_foreign_policy,0.82,0.03861229196653693,"acc,none",7241732096,5.0,answer,0.82,0.03861229196653693,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_virology,0.4879518072289157,0.038913644958358196,"acc,none",7241732096,5.0,answer,0.4879518072289157,0.038913644958358196,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,mmlu_world_religions,0.8421052631578947,0.02796678585916087,"acc,none",7241732096,5.0,answer,0.8421052631578947,0.02796678585916087,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,other,0.5471515931766978,0.008599168783945866,"acc,none",7241732096,,,0.5471515931766978,0.008599168783945866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,social sciences,0.49788755281117975,0.008845077793960711,"acc,none",7241732096,,,0.49788755281117975,0.008845077793960711,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,stem,0.4408499841420869,0.008621791105072649,"acc,none",7241732096,,,0.4408499841420869,0.008621791105072649,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,truthfulqa_gen,0.5703794369645043,0.017329234580409116,"rouge1_acc,none",7241732096,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,21.296353748830672,0.7524448061840159,0.543451652386781,0.017437280953183695,4.329100197094927,0.6122626478232078,46.35212763888087,0.8559335647777255,0.5703794369645043,0.017329234580409116,5.623905847899481,0.8649746722634263,31.105014079851745,0.9810686014573766,0.44920440636474906,0.017412941986115277,5.411842407379108,0.9286417964475384,43.24486273497977,0.8715249404647539,0.5569155446756426,0.017389730346877092,5.434889811171831,0.8713347891213943
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,truthfulqa_mc1,0.5238678090575275,0.017483547156961574,"acc,none",7241732096,0.0,0,0.5238678090575275,0.017483547156961574,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,truthfulqa_mc2,0.6686944584519211,0.015238513394215607,"acc,none",7241732096,0.0,0,0.6686944584519211,0.015238513394215607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-Instruct-v0.2,7000000000000,openllm,winogrande,0.7742699289660616,0.011749626260902543,"acc,none",7241732096,5.0,,0.7742699289660616,0.011749626260902543,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,french_bench,french_bench_arc_challenge,0.5115483319076134,0.01462624061094917,"acc_norm,none",7241732096,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.4627887082976903,0.014589571001051863,0.5115483319076134,0.01462624061094917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,french_bench,french_bench_grammar,0.7815126050420168,0.038039971528894836,"acc,none",7241732096,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7815126050420168,0.038039971528894836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,french_bench,french_bench_hellaswag,0.6608481473548939,0.004899415320445792,"acc_norm,none",7241732096,5.0,{{label}},0.48704219318911973,0.005172737450145475,0.6608481473548939,0.004899415320445792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,french_bench,french_bench_vocab,0.7815126050420168,0.038039971528894836,"acc,none",7241732096,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7815126050420168,0.038039971528894836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,arc_de,0.4798973481608212,0.014618314049208825,"acc_norm,none",7241732096,25.0,gold,0.4508126603934987,0.014559179118156086,0.4798973481608212,0.014618314049208825,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,arc_es,0.5213675213675214,0.014610524729617743,"acc_norm,none",7241732096,25.0,gold,0.4752136752136752,0.014605904746627946,0.5213675213675214,0.014610524729617743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,arc_fr,0.5106928999144568,0.01462679745105401,"acc_norm,none",7241732096,25.0,gold,0.4739093242087254,0.014610211661428528,0.5106928999144568,0.01462679745105401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,arc_it,0.48845166809238666,0.014626240610949168,"acc_norm,none",7241732096,25.0,gold,0.4525235243798118,0.014564040919200123,0.48845166809238666,0.014626240610949168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,m_mmlu_de,0.5220244380751244,0.0043383604809106545,"acc,none",7241732096,25.0,answer,0.5220244380751244,0.0043383604809106545,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,m_mmlu_es,0.24126293685315733,0.0037053310443897194,"acc,none",7241732096,25.0,answer,0.24126293685315733,0.0037053310443897194,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,m_mmlu_fr,0.5424337330990757,0.004354422058827953,"acc,none",7241732096,25.0,answer,0.5424337330990757,0.004354422058827953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,m_mmlu_it,0.48779935030596056,0.004344724991105597,"acc,none",7241732096,25.0,answer,0.48779935030596056,0.004344724991105597,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,arc_de,0.4798973481608212,0.014618314049208825,"acc_norm,none",7241732096,25.0,gold,0.4508126603934987,0.014559179118156086,0.4798973481608212,0.014618314049208825,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,arc_es,0.5213675213675214,0.014610524729617743,"acc_norm,none",7241732096,25.0,gold,0.4752136752136752,0.014605904746627946,0.5213675213675214,0.014610524729617743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,arc_fr,0.5106928999144568,0.01462679745105401,"acc_norm,none",7241732096,25.0,gold,0.4739093242087254,0.014610211661428528,0.5106928999144568,0.01462679745105401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,arc_it,0.48845166809238666,0.014626240610949168,"acc_norm,none",7241732096,25.0,gold,0.4525235243798118,0.014564040919200123,0.48845166809238666,0.014626240610949168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,m_mmlu_de,0.5220244380751244,0.0043383604809106545,"acc,none",7241732096,25.0,answer,0.5220244380751244,0.0043383604809106545,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,m_mmlu_es,0.24126293685315733,0.0037053310443897194,"acc,none",7241732096,25.0,answer,0.24126293685315733,0.0037053310443897194,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,m_mmlu_fr,0.5424337330990757,0.004354422058827953,"acc,none",7241732096,25.0,answer,0.5424337330990757,0.004354422058827953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,m_mmlu_it,0.48779935030596056,0.004344724991105597,"acc,none",7241732096,25.0,answer,0.48779935030596056,0.004344724991105597,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,arc_challenge,0.6015358361774744,0.014306946052735567,"acc_norm,none",7241732096,25.0,{{choices.label.index(answerKey)}},0.5622866894197952,0.014497573881108282,0.6015358361774744,0.014306946052735567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,gsm8k,0.38286580742987114,0.013389223491820465,"exact_match,flexible-extract",7241732096,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.379833206974981,0.013368818096960498,0.38286580742987114,0.013389223491820465,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,hellaswag,0.832105158334993,0.0037300899105376854,"acc_norm,none",7241732096,10.0,{{label}},0.6294562836088429,0.004819633668832527,0.832105158334993,0.0037300899105376854,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,humanities,0.357066950053135,0.006768857003008058,"acc,none",7241732096,,,0.357066950053135,0.006768857003008058,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu,0.6239851872952571,0.003840358304043326,"acc,none",7241732096,,,0.6239851872952571,0.003840358304043326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_abstract_algebra,0.26,0.044084400227680794,"acc,none",7241732096,5.0,answer,0.26,0.044084400227680794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_anatomy,0.6,0.04232073695151589,"acc,none",7241732096,5.0,answer,0.6,0.04232073695151589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_astronomy,0.6578947368421053,0.03860731599316091,"acc,none",7241732096,5.0,answer,0.6578947368421053,0.03860731599316091,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_business_ethics,0.57,0.04975698519562428,"acc,none",7241732096,5.0,answer,0.57,0.04975698519562428,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_clinical_knowledge,0.690566037735849,0.028450154794118634,"acc,none",7241732096,5.0,answer,0.690566037735849,0.028450154794118634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_biology,0.7222222222222222,0.037455547914624576,"acc,none",7241732096,5.0,answer,0.7222222222222222,0.037455547914624576,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_chemistry,0.49,0.05024183937956912,"acc,none",7241732096,5.0,answer,0.49,0.05024183937956912,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_computer_science,0.54,0.05009082659620332,"acc,none",7241732096,5.0,answer,0.54,0.05009082659620332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_mathematics,0.38,0.04878317312145633,"acc,none",7241732096,5.0,answer,0.38,0.04878317312145633,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_medicine,0.6242774566473989,0.03692820767264867,"acc,none",7241732096,5.0,answer,0.6242774566473989,0.03692820767264867,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_physics,0.3235294117647059,0.046550104113196177,"acc,none",7241732096,5.0,answer,0.3235294117647059,0.046550104113196177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_computer_security,0.78,0.041633319989322626,"acc,none",7241732096,5.0,answer,0.78,0.041633319989322626,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_conceptual_physics,0.5872340425531914,0.03218471141400351,"acc,none",7241732096,5.0,answer,0.5872340425531914,0.03218471141400351,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation,0.4346246973365617,0.004019493865828048,"acc,none",7241732096,,,0.4346246973365617,0.004019493865828048,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_abstract_algebra,0.24,0.04292346959909284,"acc_norm,none",7241732096,0.0,{{answer}},0.23,0.04229525846816506,0.24,0.04292346959909284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_anatomy,0.562962962962963,0.04284958639753401,"acc_norm,none",7241732096,0.0,{{answer}},0.5703703703703704,0.042763494943765995,0.562962962962963,0.04284958639753401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_astronomy,0.5328947368421053,0.040601270352363966,"acc_norm,none",7241732096,0.0,{{answer}},0.46710526315789475,0.040601270352363966,0.5328947368421053,0.040601270352363966,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_business_ethics,0.63,0.048523658709390974,"acc_norm,none",7241732096,0.0,{{answer}},0.67,0.04725815626252609,0.63,0.048523658709390974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_clinical_knowledge,0.539622641509434,0.030676096599389184,"acc_norm,none",7241732096,0.0,{{answer}},0.42641509433962266,0.03043779434298305,0.539622641509434,0.030676096599389184,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_biology,0.5138888888888888,0.04179596617581002,"acc_norm,none",7241732096,0.0,{{answer}},0.4930555555555556,0.04180806750294938,0.5138888888888888,0.04179596617581002,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_chemistry,0.36,0.04824181513244218,"acc_norm,none",7241732096,0.0,{{answer}},0.36,0.04824181513244218,0.36,0.04824181513244218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_computer_science,0.37,0.048523658709391,"acc_norm,none",7241732096,0.0,{{answer}},0.36,0.04824181513244218,0.37,0.048523658709391,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_mathematics,0.26,0.04408440022768077,"acc_norm,none",7241732096,0.0,{{answer}},0.2,0.040201512610368445,0.26,0.04408440022768077,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_medicine,0.41040462427745666,0.037507570448955356,"acc_norm,none",7241732096,0.0,{{answer}},0.3988439306358382,0.03733626655383509,0.41040462427745666,0.037507570448955356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_physics,0.4019607843137255,0.048786087144669976,"acc_norm,none",7241732096,0.0,{{answer}},0.35294117647058826,0.04755129616062949,0.4019607843137255,0.048786087144669976,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_computer_security,0.56,0.049888765156985884,"acc_norm,none",7241732096,0.0,{{answer}},0.49,0.05024183937956913,0.56,0.049888765156985884,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_conceptual_physics,0.5106382978723404,0.03267862331014063,"acc_norm,none",7241732096,0.0,{{answer}},0.574468085106383,0.03232146916224468,0.5106382978723404,0.03267862331014063,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_econometrics,0.3333333333333333,0.044346007015849245,"acc_norm,none",7241732096,0.0,{{answer}},0.3333333333333333,0.04434600701584925,0.3333333333333333,0.044346007015849245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_electrical_engineering,0.4068965517241379,0.04093793981266237,"acc_norm,none",7241732096,0.0,{{answer}},0.35172413793103446,0.03979236637497409,0.4068965517241379,0.04093793981266237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_elementary_mathematics,0.6111111111111112,0.025107425481137296,"acc_norm,none",7241732096,0.0,{{answer}},0.6111111111111112,0.025107425481137296,0.6111111111111112,0.025107425481137296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_formal_logic,0.38095238095238093,0.04343525428949098,"acc_norm,none",7241732096,0.0,{{answer}},0.40476190476190477,0.04390259265377563,0.38095238095238093,0.04343525428949098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_global_facts,0.58,0.049604496374885836,"acc_norm,none",7241732096,0.0,{{answer}},0.57,0.04975698519562428,0.58,0.049604496374885836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_biology,0.567741935483871,0.028181739720019416,"acc_norm,none",7241732096,0.0,{{answer}},0.4645161290322581,0.028372287797962956,0.567741935483871,0.028181739720019416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_chemistry,0.33497536945812806,0.033208527423483104,"acc_norm,none",7241732096,0.0,{{answer}},0.30049261083743845,0.03225799476233484,0.33497536945812806,0.033208527423483104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_computer_science,0.49,0.05024183937956911,"acc_norm,none",7241732096,0.0,{{answer}},0.42,0.049604496374885836,0.49,0.05024183937956911,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_european_history,0.5333333333333333,0.03895658065271846,"acc_norm,none",7241732096,0.0,{{answer}},0.36363636363636365,0.03756335775187896,0.5333333333333333,0.03895658065271846,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_geography,0.5555555555555556,0.035402943770953675,"acc_norm,none",7241732096,0.0,{{answer}},0.51010101010101,0.035616254886737454,0.5555555555555556,0.035402943770953675,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_government_and_politics,0.5751295336787565,0.035674713352125395,"acc_norm,none",7241732096,0.0,{{answer}},0.5544041450777202,0.03587014986075659,0.5751295336787565,0.035674713352125395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_macroeconomics,0.44871794871794873,0.025217315184846486,"acc_norm,none",7241732096,0.0,{{answer}},0.41794871794871796,0.025007329882461217,0.44871794871794873,0.025217315184846486,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_mathematics,0.32222222222222224,0.028493465091028597,"acc_norm,none",7241732096,0.0,{{answer}},0.24074074074074073,0.026067159222275794,0.32222222222222224,0.028493465091028597,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_microeconomics,0.5336134453781513,0.03240501447690071,"acc_norm,none",7241732096,0.0,{{answer}},0.453781512605042,0.032339434681820885,0.5336134453781513,0.03240501447690071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_physics,0.3576158940397351,0.03913453431177258,"acc_norm,none",7241732096,0.0,{{answer}},0.3509933774834437,0.03896981964257375,0.3576158940397351,0.03913453431177258,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_psychology,0.653211009174312,0.020406097104093027,"acc_norm,none",7241732096,0.0,{{answer}},0.6623853211009174,0.02027526598663891,0.653211009174312,0.020406097104093027,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_statistics,0.4074074074074074,0.033509916046960436,"acc_norm,none",7241732096,0.0,{{answer}},0.3611111111111111,0.03275773486100999,0.4074074074074074,0.033509916046960436,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_us_history,0.5196078431372549,0.03506612560524866,"acc_norm,none",7241732096,0.0,{{answer}},0.46078431372549017,0.03498501649369527,0.5196078431372549,0.03506612560524866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_world_history,0.4767932489451477,0.032512152011410174,"acc_norm,none",7241732096,0.0,{{answer}},0.43037974683544306,0.03223017195937599,0.4767932489451477,0.032512152011410174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_human_aging,0.5067264573991032,0.03355476596234354,"acc_norm,none",7241732096,0.0,{{answer}},0.5336322869955157,0.033481800170603065,0.5067264573991032,0.03355476596234354,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_human_sexuality,0.4580152671755725,0.04369802690578756,"acc_norm,none",7241732096,0.0,{{answer}},0.5190839694656488,0.04382094705550988,0.4580152671755725,0.04369802690578756,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_international_law,0.5289256198347108,0.04556710331269498,"acc_norm,none",7241732096,0.0,{{answer}},0.3140495867768595,0.04236964753041018,0.5289256198347108,0.04556710331269498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_jurisprudence,0.5277777777777778,0.04826217294139894,"acc_norm,none",7241732096,0.0,{{answer}},0.4351851851851852,0.04792898170907061,0.5277777777777778,0.04826217294139894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_logical_fallacies,0.4785276073619632,0.0392474687675113,"acc_norm,none",7241732096,0.0,{{answer}},0.4049079754601227,0.03856672163548913,0.4785276073619632,0.0392474687675113,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_machine_learning,0.35714285714285715,0.04547960999764376,"acc_norm,none",7241732096,0.0,{{answer}},0.33035714285714285,0.04464285714285713,0.35714285714285715,0.04547960999764376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_management,0.6116504854368932,0.04825729337356389,"acc_norm,none",7241732096,0.0,{{answer}},0.4854368932038835,0.049486373240266376,0.6116504854368932,0.04825729337356389,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_marketing,0.6623931623931624,0.03098029699261856,"acc_norm,none",7241732096,0.0,{{answer}},0.7136752136752137,0.029614323690456648,0.6623931623931624,0.03098029699261856,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_medical_genetics,0.57,0.04975698519562428,"acc_norm,none",7241732096,0.0,{{answer}},0.49,0.05024183937956912,0.57,0.04975698519562428,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_miscellaneous,0.7279693486590039,0.015913367447500503,"acc_norm,none",7241732096,0.0,{{answer}},0.7062579821200511,0.016287759388491658,0.7279693486590039,0.015913367447500503,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_moral_disputes,0.4190751445086705,0.026564178111422625,"acc_norm,none",7241732096,0.0,{{answer}},0.3815028901734104,0.0261521986197268,0.4190751445086705,0.026564178111422625,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_moral_scenarios,0.2737430167597765,0.014912413096372432,"acc_norm,none",7241732096,0.0,{{answer}},0.23798882681564246,0.014242630070574885,0.2737430167597765,0.014912413096372432,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_nutrition,0.4542483660130719,0.028509807802626567,"acc_norm,none",7241732096,0.0,{{answer}},0.37254901960784315,0.02768418188330289,0.4542483660130719,0.028509807802626567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_philosophy,0.5144694533762058,0.028386198084177687,"acc_norm,none",7241732096,0.0,{{answer}},0.48231511254019294,0.02838032284907713,0.5144694533762058,0.028386198084177687,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_prehistory,0.49074074074074076,0.027815973433878014,"acc_norm,none",7241732096,0.0,{{answer}},0.5493827160493827,0.0276847214156562,0.49074074074074076,0.027815973433878014,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_professional_accounting,0.32269503546099293,0.027889139300534792,"acc_norm,none",7241732096,0.0,{{answer}},0.3262411347517731,0.02796845304356316,0.32269503546099293,0.027889139300534792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_professional_law,0.3076923076923077,0.01178791025166459,"acc_norm,none",7241732096,0.0,{{answer}},0.27640156453715775,0.011422153194553567,0.3076923076923077,0.01178791025166459,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_professional_medicine,0.4889705882352941,0.030365446477275675,"acc_norm,none",7241732096,0.0,{{answer}},0.4852941176470588,0.03035969707904612,0.4889705882352941,0.030365446477275675,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_professional_psychology,0.46078431372549017,0.020165523313907904,"acc_norm,none",7241732096,0.0,{{answer}},0.4215686274509804,0.01997742260022747,0.46078431372549017,0.020165523313907904,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_public_relations,0.4,0.0469237132203465,"acc_norm,none",7241732096,0.0,{{answer}},0.509090909090909,0.04788339768702861,0.4,0.0469237132203465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_security_studies,0.3306122448979592,0.030116426296540596,"acc_norm,none",7241732096,0.0,{{answer}},0.3306122448979592,0.030116426296540596,0.3306122448979592,0.030116426296540596,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_sociology,0.46766169154228854,0.03528131472933607,"acc_norm,none",7241732096,0.0,{{answer}},0.4079601990049751,0.034751163651940926,0.46766169154228854,0.03528131472933607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_us_foreign_policy,0.51,0.05024183937956911,"acc_norm,none",7241732096,0.0,{{answer}},0.45,0.05,0.51,0.05024183937956911,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_virology,0.42771084337349397,0.03851597683718533,"acc_norm,none",7241732096,0.0,{{answer}},0.3433734939759036,0.03696584317010601,0.42771084337349397,0.03851597683718533,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_world_religions,0.7543859649122807,0.0330140594698725,"acc_norm,none",7241732096,0.0,{{answer}},0.7309941520467836,0.03401052620104089,0.7543859649122807,0.0330140594698725,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_econometrics,0.49122807017543857,0.04702880432049615,"acc,none",7241732096,5.0,answer,0.49122807017543857,0.04702880432049615,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_electrical_engineering,0.5655172413793104,0.04130740879555498,"acc,none",7241732096,5.0,answer,0.5655172413793104,0.04130740879555498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_elementary_mathematics,0.3862433862433862,0.025075981767601688,"acc,none",7241732096,5.0,answer,0.3862433862433862,0.025075981767601688,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_formal_logic,0.3968253968253968,0.04375888492727062,"acc,none",7241732096,5.0,answer,0.3968253968253968,0.04375888492727062,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_global_facts,0.33,0.047258156262526045,"acc,none",7241732096,5.0,answer,0.33,0.047258156262526045,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_biology,0.7709677419354839,0.023904914311782648,"acc,none",7241732096,5.0,answer,0.7709677419354839,0.023904914311782648,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_chemistry,0.49261083743842365,0.035176035403610084,"acc,none",7241732096,5.0,answer,0.49261083743842365,0.035176035403610084,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_computer_science,0.68,0.04688261722621505,"acc,none",7241732096,5.0,answer,0.68,0.04688261722621505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_european_history,0.7696969696969697,0.0328766675860349,"acc,none",7241732096,5.0,answer,0.7696969696969697,0.0328766675860349,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_geography,0.7727272727272727,0.0298575156733864,"acc,none",7241732096,5.0,answer,0.7727272727272727,0.0298575156733864,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_government_and_politics,0.8601036269430051,0.025033870583015174,"acc,none",7241732096,5.0,answer,0.8601036269430051,0.025033870583015174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_macroeconomics,0.6512820512820513,0.02416278028401772,"acc,none",7241732096,5.0,answer,0.6512820512820513,0.02416278028401772,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_mathematics,0.37407407407407406,0.02950286112895529,"acc,none",7241732096,5.0,answer,0.37407407407407406,0.02950286112895529,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_microeconomics,0.6596638655462185,0.030778057422931673,"acc,none",7241732096,5.0,answer,0.6596638655462185,0.030778057422931673,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_physics,0.2980132450331126,0.03734535676787198,"acc,none",7241732096,5.0,answer,0.2980132450331126,0.03734535676787198,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_psychology,0.8238532110091743,0.016332882393431412,"acc,none",7241732096,5.0,answer,0.8238532110091743,0.016332882393431412,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_statistics,0.5416666666666666,0.033981108902946366,"acc,none",7241732096,5.0,answer,0.5416666666666666,0.033981108902946366,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_us_history,0.803921568627451,0.02786594228663933,"acc,none",7241732096,5.0,answer,0.803921568627451,0.02786594228663933,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_world_history,0.7637130801687764,0.027652153144159274,"acc,none",7241732096,5.0,answer,0.7637130801687764,0.027652153144159274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_human_aging,0.6905829596412556,0.03102441174057222,"acc,none",7241732096,5.0,answer,0.6905829596412556,0.03102441174057222,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_human_sexuality,0.8015267175572519,0.03498149385462472,"acc,none",7241732096,5.0,answer,0.8015267175572519,0.03498149385462472,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_humanities,0.5674814027630181,0.006756112143898837,"acc,none",7241732096,,,0.5674814027630181,0.006756112143898837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_international_law,0.7851239669421488,0.03749492448709699,"acc,none",7241732096,5.0,answer,0.7851239669421488,0.03749492448709699,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_jurisprudence,0.7407407407407407,0.04236511258094632,"acc,none",7241732096,5.0,answer,0.7407407407407407,0.04236511258094632,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_logical_fallacies,0.7668711656441718,0.0332201579577674,"acc,none",7241732096,5.0,answer,0.7668711656441718,0.0332201579577674,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_machine_learning,0.5,0.04745789978762494,"acc,none",7241732096,5.0,answer,0.5,0.04745789978762494,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_management,0.8155339805825242,0.03840423627288276,"acc,none",7241732096,5.0,answer,0.8155339805825242,0.03840423627288276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_marketing,0.8760683760683761,0.021586494001281382,"acc,none",7241732096,5.0,answer,0.8760683760683761,0.021586494001281382,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_medical_genetics,0.73,0.0446196043338474,"acc,none",7241732096,5.0,answer,0.73,0.0446196043338474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_miscellaneous,0.8160919540229885,0.013853724170922531,"acc,none",7241732096,5.0,answer,0.8160919540229885,0.013853724170922531,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_moral_disputes,0.7109826589595376,0.02440517393578323,"acc,none",7241732096,5.0,answer,0.7109826589595376,0.02440517393578323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_moral_scenarios,0.33631284916201115,0.015801003729145897,"acc,none",7241732096,5.0,answer,0.33631284916201115,0.015801003729145897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_nutrition,0.7581699346405228,0.024518195641879334,"acc,none",7241732096,5.0,answer,0.7581699346405228,0.024518195641879334,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_other,0.7019633086578694,0.007890986670550425,"acc,none",7241732096,,,0.7019633086578694,0.007890986670550425,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_philosophy,0.7138263665594855,0.025670259242188957,"acc,none",7241732096,5.0,answer,0.7138263665594855,0.025670259242188957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_prehistory,0.7253086419753086,0.024836057868294677,"acc,none",7241732096,5.0,answer,0.7253086419753086,0.024836057868294677,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_professional_accounting,0.5106382978723404,0.02982074719142244,"acc,none",7241732096,5.0,answer,0.5106382978723404,0.02982074719142244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_professional_law,0.45697522816166886,0.012722869501611419,"acc,none",7241732096,5.0,answer,0.45697522816166886,0.012722869501611419,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_professional_medicine,0.6580882352941176,0.028814722422254177,"acc,none",7241732096,5.0,answer,0.6580882352941176,0.028814722422254177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_professional_psychology,0.6781045751633987,0.018901015322093095,"acc,none",7241732096,5.0,answer,0.6781045751633987,0.018901015322093095,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_public_relations,0.6454545454545455,0.04582004841505417,"acc,none",7241732096,5.0,answer,0.6454545454545455,0.04582004841505417,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_security_studies,0.7224489795918367,0.028666857790274648,"acc,none",7241732096,5.0,answer,0.7224489795918367,0.028666857790274648,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_social_sciences,0.7331816704582386,0.007818618394580918,"acc,none",7241732096,,,0.7331816704582386,0.007818618394580918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_sociology,0.8258706467661692,0.026814951200421603,"acc,none",7241732096,5.0,answer,0.8258706467661692,0.026814951200421603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_stem,0.5248969235648588,0.008497277469390123,"acc,none",7241732096,,,0.5248969235648588,0.008497277469390123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_us_foreign_policy,0.87,0.033799766898963086,"acc,none",7241732096,5.0,answer,0.87,0.033799766898963086,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_virology,0.5421686746987951,0.038786267710023595,"acc,none",7241732096,5.0,answer,0.5421686746987951,0.038786267710023595,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_world_religions,0.8362573099415205,0.028380919596145866,"acc,none",7241732096,5.0,answer,0.8362573099415205,0.028380919596145866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,other,0.5275185065980045,0.008583586441129502,"acc,none",7241732096,,,0.5275185065980045,0.008583586441129502,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,social sciences,0.47708807279818005,0.00882914630032474,"acc,none",7241732096,,,0.47708807279818005,0.00882914630032474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,stem,0.4173802727561053,0.008523106433407465,"acc,none",7241732096,,,0.4173802727561053,0.008523106433407465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,truthfulqa_gen,0.397796817625459,0.017133934248559652,"rouge1_acc,none",7241732096,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,30.6279834548563,0.8547319050508423,0.412484700122399,0.017233299399571203,-1.5509407922441087,1.0402223690206764,55.94312022053465,0.9274292142352155,0.397796817625459,0.017133934248559652,-1.8253353029993267,1.257419367486379,41.155472362892276,1.0988337199287672,0.3488372093023256,0.0166844198599869,-2.680860077292509,1.4170725702460802,53.26292829659245,0.94472775094152,0.397796817625459,0.017133934248559655,-2.2005251181239904,1.2743301643103866
mistralai/Mistral-7B-v0.1,7000000000000,openllm,truthfulqa_mc1,0.28151774785801714,0.01574402724825605,"acc,none",7241732096,0.0,0,0.28151774785801714,0.01574402724825605,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,truthfulqa_mc2,0.4261059629785501,0.014205676389190985,"acc,none",7241732096,0.0,0,0.4261059629785501,0.014205676389190985,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,winogrande,0.7790055248618785,0.011661223637643417,"acc,none",7241732096,5.0,,0.7790055248618785,0.011661223637643417,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,french_bench,french_bench_arc_challenge,0.40290846877673225,0.014351663146567203,"acc_norm,none",6921720704,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.3473053892215569,0.013931226499492359,0.40290846877673225,0.014351663146567203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,french_bench,french_bench_grammar,0.8235294117647058,0.0350941493654996,"acc,none",6921720704,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.8235294117647058,0.0350941493654996,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,french_bench,french_bench_hellaswag,0.6530306275433712,0.00492616173150174,"acc_norm,none",6921720704,5.0,{{label}},0.48650674662668664,0.005172590825294664,0.6530306275433712,0.00492616173150174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,french_bench,french_bench_vocab,0.8319327731092437,0.03442267607655234,"acc,none",6921720704,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.8319327731092437,0.03442267607655234,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,arc_de,0.31394354148845166,0.013579515768185788,"acc_norm,none",6921720704,25.0,gold,0.2805816937553465,0.013146162224654298,0.31394354148845166,0.013579515768185788,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,arc_es,0.3803418803418803,0.014198938935522067,"acc_norm,none",6921720704,25.0,gold,0.335042735042735,0.013805105015816038,0.3803418803418803,0.014198938935522067,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,arc_fr,0.40889649272882805,0.01438523759817326,"acc_norm,none",6921720704,25.0,gold,0.3627031650983747,0.014067765882432828,0.40889649272882805,0.01438523759817326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,arc_it,0.38922155688622756,0.014266547006095372,"acc_norm,none",6921720704,25.0,gold,0.3481608212147134,0.013939229153926038,0.38922155688622756,0.014266547006095372,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,m_mmlu_de,0.24732237139840096,0.003747257839320172,"acc,none",6921720704,25.0,answer,0.24732237139840096,0.003747257839320172,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,m_mmlu_es,0.2521373931303435,0.0037606732343109465,"acc,none",6921720704,25.0,answer,0.2521373931303435,0.0037606732343109465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,m_mmlu_fr,0.2501718738064319,0.0037855612802412102,"acc,none",6921720704,25.0,answer,0.2501718738064319,0.0037855612802412102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,m_mmlu_it,0.24922565535997582,0.0037598689325997773,"acc,none",6921720704,25.0,answer,0.24922565535997582,0.0037598689325997773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,arc_de,0.31394354148845166,0.013579515768185788,"acc_norm,none",6921720704,25.0,gold,0.2805816937553465,0.013146162224654298,0.31394354148845166,0.013579515768185788,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,arc_es,0.3803418803418803,0.014198938935522067,"acc_norm,none",6921720704,25.0,gold,0.335042735042735,0.013805105015816038,0.3803418803418803,0.014198938935522067,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,arc_fr,0.40889649272882805,0.01438523759817326,"acc_norm,none",6921720704,25.0,gold,0.3627031650983747,0.014067765882432828,0.40889649272882805,0.01438523759817326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,arc_it,0.38922155688622756,0.014266547006095372,"acc_norm,none",6921720704,25.0,gold,0.3481608212147134,0.013939229153926038,0.38922155688622756,0.014266547006095372,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,m_mmlu_de,0.24732237139840096,0.003747257839320172,"acc,none",6921720704,25.0,answer,0.24732237139840096,0.003747257839320172,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,m_mmlu_es,0.2521373931303435,0.0037606732343109465,"acc,none",6921720704,25.0,answer,0.2521373931303435,0.0037606732343109465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,m_mmlu_fr,0.2501718738064319,0.0037855612802412102,"acc,none",6921720704,25.0,answer,0.2501718738064319,0.0037855612802412102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,m_mmlu_it,0.24922565535997582,0.0037598689325997773,"acc,none",6921720704,25.0,answer,0.24922565535997582,0.0037598689325997773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,arc_challenge,0.4658703071672355,0.014577311315231097,"acc_norm,none",6921720704,25.0,{{choices.label.index(answerKey)}},0.431740614334471,0.014474591427196202,0.4658703071672355,0.014577311315231097,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,gsm8k,0.043214556482183475,0.005600987515237861,"exact_match,flexible-extract",6921720704,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.037149355572403335,0.005209516283073793,0.043214556482183475,0.005600987515237861,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,hellaswag,0.7811192989444333,0.004126424809818356,"acc_norm,none",6921720704,10.0,{{label}},0.5843457478589922,0.004918272352137557,0.7811192989444333,0.004126424809818356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,humanities,0.32157279489904356,0.006690785751445344,"acc,none",6921720704,,,0.32157279489904356,0.006690785751445344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu,0.2723258795043441,0.0037419871393619217,"acc,none",6921720704,,,0.2723258795043441,0.0037419871393619217,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_abstract_algebra,0.25,0.04351941398892446,"acc,none",6921720704,5.0,answer,0.25,0.04351941398892446,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_anatomy,0.21481481481481482,0.035478541985608236,"acc,none",6921720704,5.0,answer,0.21481481481481482,0.035478541985608236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_astronomy,0.25,0.03523807393012047,"acc,none",6921720704,5.0,answer,0.25,0.03523807393012047,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_business_ethics,0.21,0.04093601807403326,"acc,none",6921720704,5.0,answer,0.21,0.04093601807403326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_clinical_knowledge,0.30566037735849055,0.028353298073322666,"acc,none",6921720704,5.0,answer,0.30566037735849055,0.028353298073322666,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_biology,0.22916666666666666,0.035146974678623884,"acc,none",6921720704,5.0,answer,0.22916666666666666,0.035146974678623884,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_chemistry,0.22,0.041633319989322695,"acc,none",6921720704,5.0,answer,0.22,0.041633319989322695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_computer_science,0.28,0.04512608598542127,"acc,none",6921720704,5.0,answer,0.28,0.04512608598542127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_mathematics,0.28,0.04512608598542126,"acc,none",6921720704,5.0,answer,0.28,0.04512608598542126,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_medicine,0.24277456647398843,0.0326926380614177,"acc,none",6921720704,5.0,answer,0.24277456647398843,0.0326926380614177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_physics,0.22549019607843138,0.04158307533083286,"acc,none",6921720704,5.0,answer,0.22549019607843138,0.04158307533083286,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_computer_security,0.26,0.044084400227680794,"acc,none",6921720704,5.0,answer,0.26,0.044084400227680794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_conceptual_physics,0.28936170212765955,0.029644006577009618,"acc,none",6921720704,5.0,answer,0.28936170212765955,0.029644006577009618,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation,0.37772397094430993,0.00397353272197644,"acc,none",6921720704,,,0.37772397094430993,0.00397353272197644,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_abstract_algebra,0.17,0.03775251680686371,"acc_norm,none",6921720704,0.0,{{answer}},0.16,0.03684529491774708,0.17,0.03775251680686371,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_anatomy,0.45925925925925926,0.04304979692464243,"acc_norm,none",6921720704,0.0,{{answer}},0.4666666666666667,0.043097329010363554,0.45925925925925926,0.04304979692464243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_astronomy,0.4934210526315789,0.04068590050224971,"acc_norm,none",6921720704,0.0,{{answer}},0.4144736842105263,0.04008973785779206,0.4934210526315789,0.04068590050224971,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_business_ethics,0.6,0.049236596391733084,"acc_norm,none",6921720704,0.0,{{answer}},0.59,0.04943110704237102,0.6,0.049236596391733084,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_clinical_knowledge,0.5056603773584906,0.030770900763851302,"acc_norm,none",6921720704,0.0,{{answer}},0.39245283018867927,0.030052580579557845,0.5056603773584906,0.030770900763851302,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_biology,0.4236111111111111,0.04132125019723368,"acc_norm,none",6921720704,0.0,{{answer}},0.4166666666666667,0.04122728707651282,0.4236111111111111,0.04132125019723368,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_chemistry,0.29,0.045604802157206845,"acc_norm,none",6921720704,0.0,{{answer}},0.26,0.04408440022768077,0.29,0.045604802157206845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_computer_science,0.3,0.046056618647183814,"acc_norm,none",6921720704,0.0,{{answer}},0.27,0.044619604333847415,0.3,0.046056618647183814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_mathematics,0.17,0.0377525168068637,"acc_norm,none",6921720704,0.0,{{answer}},0.14,0.03487350880197772,0.17,0.0377525168068637,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_medicine,0.3872832369942196,0.03714325906302065,"acc_norm,none",6921720704,0.0,{{answer}},0.3468208092485549,0.036291466701596636,0.3872832369942196,0.03714325906302065,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_physics,0.2647058823529412,0.0438986995680878,"acc_norm,none",6921720704,0.0,{{answer}},0.20588235294117646,0.04023382273617747,0.2647058823529412,0.0438986995680878,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_computer_security,0.51,0.05024183937956912,"acc_norm,none",6921720704,0.0,{{answer}},0.47,0.050161355804659205,0.51,0.05024183937956912,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_conceptual_physics,0.42127659574468085,0.03227834510146267,"acc_norm,none",6921720704,0.0,{{answer}},0.4425531914893617,0.032469569197899575,0.42127659574468085,0.03227834510146267,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_econometrics,0.24561403508771928,0.04049339297748141,"acc_norm,none",6921720704,0.0,{{answer}},0.2543859649122807,0.040969851398436695,0.24561403508771928,0.04049339297748141,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_electrical_engineering,0.3586206896551724,0.039966295748767186,"acc_norm,none",6921720704,0.0,{{answer}},0.32413793103448274,0.03900432069185555,0.3586206896551724,0.039966295748767186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_elementary_mathematics,0.4126984126984127,0.025355741263055284,"acc_norm,none",6921720704,0.0,{{answer}},0.3915343915343915,0.025138091388851095,0.4126984126984127,0.025355741263055284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_formal_logic,0.3253968253968254,0.041905964388711366,"acc_norm,none",6921720704,0.0,{{answer}},0.30952380952380953,0.04134913018303316,0.3253968253968254,0.041905964388711366,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_global_facts,0.53,0.050161355804659205,"acc_norm,none",6921720704,0.0,{{answer}},0.51,0.05024183937956912,0.53,0.050161355804659205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_biology,0.45483870967741935,0.028327743091561074,"acc_norm,none",6921720704,0.0,{{answer}},0.3870967741935484,0.02770935967503249,0.45483870967741935,0.028327743091561074,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_chemistry,0.270935960591133,0.031270907132976984,"acc_norm,none",6921720704,0.0,{{answer}},0.28078817733990147,0.0316185633535861,0.270935960591133,0.031270907132976984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_computer_science,0.44,0.04988876515698589,"acc_norm,none",6921720704,0.0,{{answer}},0.38,0.04878317312145633,0.44,0.04988876515698589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_european_history,0.49696969696969695,0.03904272341431855,"acc_norm,none",6921720704,0.0,{{answer}},0.3212121212121212,0.03646204963253811,0.49696969696969695,0.03904272341431855,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_geography,0.4797979797979798,0.03559443565563918,"acc_norm,none",6921720704,0.0,{{answer}},0.4797979797979798,0.035594435655639196,0.4797979797979798,0.03559443565563918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_government_and_politics,0.5544041450777202,0.03587014986075659,"acc_norm,none",6921720704,0.0,{{answer}},0.48704663212435234,0.03607228061047749,0.5544041450777202,0.03587014986075659,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_macroeconomics,0.38974358974358975,0.024726967886647074,"acc_norm,none",6921720704,0.0,{{answer}},0.3487179487179487,0.024162780284017724,0.38974358974358975,0.024726967886647074,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_mathematics,0.21481481481481482,0.025040443877000683,"acc_norm,none",6921720704,0.0,{{answer}},0.1962962962962963,0.02421742132741715,0.21481481481481482,0.025040443877000683,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_microeconomics,0.47058823529411764,0.03242225027115007,"acc_norm,none",6921720704,0.0,{{answer}},0.4117647058823529,0.031968769891957786,0.47058823529411764,0.03242225027115007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_physics,0.2582781456953642,0.035737053147634576,"acc_norm,none",6921720704,0.0,{{answer}},0.2582781456953642,0.035737053147634576,0.2582781456953642,0.035737053147634576,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_psychology,0.5486238532110091,0.021335714711268782,"acc_norm,none",6921720704,0.0,{{answer}},0.5798165137614679,0.021162420048273504,0.5486238532110091,0.021335714711268782,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_statistics,0.3101851851851852,0.03154696285656628,"acc_norm,none",6921720704,0.0,{{answer}},0.2916666666666667,0.030998666304560534,0.3101851851851852,0.03154696285656628,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_us_history,0.44607843137254904,0.03488845451304974,"acc_norm,none",6921720704,0.0,{{answer}},0.43137254901960786,0.03476099060501636,0.44607843137254904,0.03488845451304974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_world_history,0.39662447257383965,0.03184399873811224,"acc_norm,none",6921720704,0.0,{{answer}},0.38396624472573837,0.031658678064106674,0.39662447257383965,0.03184399873811224,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_human_aging,0.47085201793721976,0.03350073248773404,"acc_norm,none",6921720704,0.0,{{answer}},0.47533632286995514,0.03351695167652628,0.47085201793721976,0.03350073248773404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_human_sexuality,0.44274809160305345,0.04356447202665069,"acc_norm,none",6921720704,0.0,{{answer}},0.4732824427480916,0.04379024936553894,0.44274809160305345,0.04356447202665069,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_international_law,0.4132231404958678,0.04495087843548408,"acc_norm,none",6921720704,0.0,{{answer}},0.2644628099173554,0.04026187527591206,0.4132231404958678,0.04495087843548408,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_jurisprudence,0.4074074074074074,0.047500773411999854,"acc_norm,none",6921720704,0.0,{{answer}},0.3148148148148148,0.04489931073591312,0.4074074074074074,0.047500773411999854,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_logical_fallacies,0.43558282208588955,0.038956324641389366,"acc_norm,none",6921720704,0.0,{{answer}},0.3558282208588957,0.03761521380046734,0.43558282208588955,0.038956324641389366,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_machine_learning,0.25892857142857145,0.04157751539865629,"acc_norm,none",6921720704,0.0,{{answer}},0.29464285714285715,0.04327040932578728,0.25892857142857145,0.04157751539865629,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_management,0.5922330097087378,0.048657775704107675,"acc_norm,none",6921720704,0.0,{{answer}},0.49514563106796117,0.04950504382128919,0.5922330097087378,0.048657775704107675,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_marketing,0.6367521367521367,0.03150712523091264,"acc_norm,none",6921720704,0.0,{{answer}},0.6410256410256411,0.03142616993791925,0.6367521367521367,0.03150712523091264,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_medical_genetics,0.52,0.050211673156867795,"acc_norm,none",6921720704,0.0,{{answer}},0.43,0.049756985195624284,0.52,0.050211673156867795,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_miscellaneous,0.6181353767560664,0.01737373273667759,"acc_norm,none",6921720704,0.0,{{answer}},0.6296296296296297,0.017268607560005776,0.6181353767560664,0.01737373273667759,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_moral_disputes,0.37572254335260113,0.02607431485165708,"acc_norm,none",6921720704,0.0,{{answer}},0.3352601156069364,0.025416003773165555,0.37572254335260113,0.02607431485165708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_moral_scenarios,0.2759776536312849,0.014950103002475353,"acc_norm,none",6921720704,0.0,{{answer}},0.23798882681564246,0.014242630070574885,0.2759776536312849,0.014950103002475353,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_nutrition,0.4411764705882353,0.02843109544417664,"acc_norm,none",6921720704,0.0,{{answer}},0.3431372549019608,0.027184498909941613,0.4411764705882353,0.02843109544417664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_philosophy,0.40836012861736337,0.027917050748484634,"acc_norm,none",6921720704,0.0,{{answer}},0.3954983922829582,0.027770918531427838,0.40836012861736337,0.027917050748484634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_prehistory,0.43209876543209874,0.02756301097160668,"acc_norm,none",6921720704,0.0,{{answer}},0.4783950617283951,0.027794760105008736,0.43209876543209874,0.02756301097160668,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_professional_accounting,0.2765957446808511,0.026684564340460997,"acc_norm,none",6921720704,0.0,{{answer}},0.2907801418439716,0.027090664368353178,0.2765957446808511,0.026684564340460997,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_professional_law,0.2966101694915254,0.011665946586082832,"acc_norm,none",6921720704,0.0,{{answer}},0.26597131681877445,0.011285033165551277,0.2966101694915254,0.011665946586082832,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_professional_medicine,0.40441176470588236,0.029812630701569743,"acc_norm,none",6921720704,0.0,{{answer}},0.36764705882352944,0.029289413409403196,0.40441176470588236,0.029812630701569743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_professional_psychology,0.4117647058823529,0.019910377463105935,"acc_norm,none",6921720704,0.0,{{answer}},0.36764705882352944,0.019506291693954843,0.4117647058823529,0.019910377463105935,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_public_relations,0.37272727272727274,0.04631381319425463,"acc_norm,none",6921720704,0.0,{{answer}},0.45454545454545453,0.04769300568972744,0.37272727272727274,0.04631381319425463,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_security_studies,0.2938775510204082,0.029162738410249765,"acc_norm,none",6921720704,0.0,{{answer}},0.3224489795918367,0.02992310056368391,0.2938775510204082,0.029162738410249765,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_sociology,0.43283582089552236,0.03503490923673281,"acc_norm,none",6921720704,0.0,{{answer}},0.34328358208955223,0.03357379665433432,0.43283582089552236,0.03503490923673281,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_us_foreign_policy,0.46,0.05009082659620332,"acc_norm,none",6921720704,0.0,{{answer}},0.44,0.049888765156985884,0.46,0.05009082659620332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_virology,0.4036144578313253,0.03819486140758398,"acc_norm,none",6921720704,0.0,{{answer}},0.3072289156626506,0.03591566797824662,0.4036144578313253,0.03819486140758398,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_world_religions,0.6374269005847953,0.03687130615562059,"acc_norm,none",6921720704,0.0,{{answer}},0.6023391812865497,0.03753638955761691,0.6374269005847953,0.03687130615562059,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_econometrics,0.23684210526315788,0.03999423879281338,"acc,none",6921720704,5.0,answer,0.23684210526315788,0.03999423879281338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_electrical_engineering,0.2896551724137931,0.03780019230438014,"acc,none",6921720704,5.0,answer,0.2896551724137931,0.03780019230438014,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_elementary_mathematics,0.24338624338624337,0.02210112878741543,"acc,none",6921720704,5.0,answer,0.24338624338624337,0.02210112878741543,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_formal_logic,0.23809523809523808,0.0380952380952381,"acc,none",6921720704,5.0,answer,0.23809523809523808,0.0380952380952381,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_global_facts,0.31,0.04648231987117316,"acc,none",6921720704,5.0,answer,0.31,0.04648231987117316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_biology,0.25161290322580643,0.024685979286239963,"acc,none",6921720704,5.0,answer,0.25161290322580643,0.024685979286239963,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_chemistry,0.2561576354679803,0.030712730070982592,"acc,none",6921720704,5.0,answer,0.2561576354679803,0.030712730070982592,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_computer_science,0.31,0.04648231987117316,"acc,none",6921720704,5.0,answer,0.31,0.04648231987117316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_european_history,0.24848484848484848,0.03374402644139404,"acc,none",6921720704,5.0,answer,0.24848484848484848,0.03374402644139404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_geography,0.17676767676767677,0.027178752639044915,"acc,none",6921720704,5.0,answer,0.17676767676767677,0.027178752639044915,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_government_and_politics,0.2538860103626943,0.03141024780565318,"acc,none",6921720704,5.0,answer,0.2538860103626943,0.03141024780565318,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_macroeconomics,0.24102564102564103,0.021685546665333174,"acc,none",6921720704,5.0,answer,0.24102564102564103,0.021685546665333174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_mathematics,0.24814814814814815,0.0263357394040558,"acc,none",6921720704,5.0,answer,0.24814814814814815,0.0263357394040558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_microeconomics,0.2689075630252101,0.02880139219363127,"acc,none",6921720704,5.0,answer,0.2689075630252101,0.02880139219363127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_physics,0.304635761589404,0.03757949922943343,"acc,none",6921720704,5.0,answer,0.304635761589404,0.03757949922943343,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_psychology,0.23669724770642203,0.018224078117299067,"acc,none",6921720704,5.0,answer,0.23669724770642203,0.018224078117299067,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_statistics,0.13425925925925927,0.023251277590545894,"acc,none",6921720704,5.0,answer,0.13425925925925927,0.023251277590545894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_us_history,0.31862745098039214,0.0327028718148208,"acc,none",6921720704,5.0,answer,0.31862745098039214,0.0327028718148208,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_world_history,0.28270042194092826,0.029312814153955914,"acc,none",6921720704,5.0,answer,0.28270042194092826,0.029312814153955914,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_human_aging,0.4618834080717489,0.033460150119732274,"acc,none",6921720704,5.0,answer,0.4618834080717489,0.033460150119732274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_human_sexuality,0.2900763358778626,0.039800662464677665,"acc,none",6921720704,5.0,answer,0.2900763358778626,0.039800662464677665,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_humanities,0.2688629117959617,0.006451202014659573,"acc,none",6921720704,,,0.2688629117959617,0.006451202014659573,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_international_law,0.23140495867768596,0.038498560987940904,"acc,none",6921720704,5.0,answer,0.23140495867768596,0.038498560987940904,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_jurisprudence,0.32407407407407407,0.045245960070300476,"acc,none",6921720704,5.0,answer,0.32407407407407407,0.045245960070300476,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_logical_fallacies,0.25153374233128833,0.03408997886857529,"acc,none",6921720704,5.0,answer,0.25153374233128833,0.03408997886857529,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_machine_learning,0.35714285714285715,0.04547960999764376,"acc,none",6921720704,5.0,answer,0.35714285714285715,0.04547960999764376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_management,0.2524271844660194,0.04301250399690878,"acc,none",6921720704,5.0,answer,0.2524271844660194,0.04301250399690878,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_marketing,0.3162393162393162,0.030463656747340254,"acc,none",6921720704,5.0,answer,0.3162393162393162,0.030463656747340254,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_medical_genetics,0.29,0.045604802157206845,"acc,none",6921720704,5.0,answer,0.29,0.045604802157206845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_miscellaneous,0.30779054916985954,0.01650604504515563,"acc,none",6921720704,5.0,answer,0.30779054916985954,0.01650604504515563,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_moral_disputes,0.2832369942196532,0.02425790170532337,"acc,none",6921720704,5.0,answer,0.2832369942196532,0.02425790170532337,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_moral_scenarios,0.24134078212290502,0.014310999547961452,"acc,none",6921720704,5.0,answer,0.24134078212290502,0.014310999547961452,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_nutrition,0.27450980392156865,0.025553169991826528,"acc,none",6921720704,5.0,answer,0.27450980392156865,0.025553169991826528,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_other,0.30447376890891537,0.008215009170523717,"acc,none",6921720704,,,0.30447376890891537,0.008215009170523717,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_philosophy,0.3054662379421222,0.026160584450140488,"acc,none",6921720704,5.0,answer,0.3054662379421222,0.026160584450140488,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_prehistory,0.32098765432098764,0.025976566010862737,"acc,none",6921720704,5.0,answer,0.32098765432098764,0.025976566010862737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_professional_accounting,0.29432624113475175,0.0271871270115038,"acc,none",6921720704,5.0,answer,0.29432624113475175,0.0271871270115038,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_professional_law,0.24771838331160365,0.011025499291443744,"acc,none",6921720704,5.0,answer,0.24771838331160365,0.011025499291443744,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_professional_medicine,0.2536764705882353,0.026431329870789538,"acc,none",6921720704,5.0,answer,0.2536764705882353,0.026431329870789538,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_professional_psychology,0.2647058823529412,0.01784808957491322,"acc,none",6921720704,5.0,answer,0.2647058823529412,0.01784808957491322,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_public_relations,0.34545454545454546,0.04554619617541054,"acc,none",6921720704,5.0,answer,0.34545454545454546,0.04554619617541054,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_security_studies,0.27755102040816326,0.02866685779027465,"acc,none",6921720704,5.0,answer,0.27755102040816326,0.02866685779027465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_social_sciences,0.26519337016574585,0.007928747651234531,"acc,none",6921720704,,,0.26519337016574585,0.007928747651234531,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_sociology,0.3582089552238806,0.03390393042268814,"acc,none",6921720704,5.0,answer,0.3582089552238806,0.03390393042268814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_stem,0.2527751347922613,0.007723026418937003,"acc,none",6921720704,,,0.2527751347922613,0.007723026418937003,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_us_foreign_policy,0.4,0.049236596391733084,"acc,none",6921720704,5.0,answer,0.4,0.049236596391733084,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_virology,0.37349397590361444,0.037658451171688624,"acc,none",6921720704,5.0,answer,0.37349397590361444,0.037658451171688624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_world_religions,0.38011695906432746,0.037229657413855394,"acc,none",6921720704,5.0,answer,0.38011695906432746,0.037229657413855394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,other,0.46829739298358547,0.008651722737544136,"acc,none",6921720704,,,0.46829739298358547,0.008651722737544136,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,social sciences,0.42151446213844657,0.00876142163006902,"acc,none",6921720704,,,0.42151446213844657,0.00876142163006902,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,stem,0.32952743418966063,0.008227069450364508,"acc,none",6921720704,,,0.32952743418966063,0.008227069450364508,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,truthfulqa_gen,0.26193390452876375,0.015392118805015032,"rouge1_acc,none",6921720704,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,22.94988386811439,0.7615448861452995,0.31456548347613217,0.016255241993179157,-9.394852578921569,0.8212595840283824,46.38152710528668,0.904128478635869,0.26193390452876375,0.015392118805015032,-12.884574235910073,0.8966997947833258,29.27481347413483,1.0080179575484558,0.19706242350061198,0.013925080734473742,-15.135500681470209,1.0640462391363208,43.58154340232294,0.8984783551873023,0.25458996328029376,0.015250117079156494,-13.22334928041737,0.9087456114696895
tiiuae/falcon-7b,3000000000000,openllm,truthfulqa_mc1,0.22399020807833536,0.014594964329474203,"acc,none",6921720704,0.0,0,0.22399020807833536,0.014594964329474203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,truthfulqa_mc2,0.34277162913779985,0.013272681916634024,"acc,none",6921720704,0.0,0,0.34277162913779985,0.013272681916634024,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,winogrande,0.7308602999210734,0.01246491195126874,"acc,none",6921720704,5.0,,0.7308602999210734,0.01246491195126874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
