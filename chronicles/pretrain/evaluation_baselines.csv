model_name,training_tokens,benchmark,dataset,score,stderr,score_name,model_num_parameters,num_fewshot,doc_to_target,"acc,none","acc_stderr,none","acc_norm,none","acc_norm_stderr,none","rouge1,none","rouge1_stderr,none","f1,none","f1_stderr,none","exact,none","exact_stderr,none","is_included,none","is_included_stderr,none","prompt_level_strict_acc,none","prompt_level_strict_acc_stderr,none","inst_level_strict_acc,none","inst_level_strict_acc_stderr,none","prompt_level_loose_acc,none","prompt_level_loose_acc_stderr,none","inst_level_loose_acc,none","inst_level_loose_acc_stderr,none","exact_match,none","exact_match_stderr,none","exact_match,strict-match","exact_match_stderr,strict-match","exact_match,flexible-extract","exact_match_stderr,flexible-extract","bleu_max,none","bleu_max_stderr,none","bleu_acc,none","bleu_acc_stderr,none","bleu_diff,none","bleu_diff_stderr,none","rouge1_max,none","rouge1_max_stderr,none","rouge1_acc,none","rouge1_acc_stderr,none","rouge1_diff,none","rouge1_diff_stderr,none","rouge2_max,none","rouge2_max_stderr,none","rouge2_acc,none","rouge2_acc_stderr,none","rouge2_diff,none","rouge2_diff_stderr,none","rougeL_max,none","rougeL_max_stderr,none","rougeL_acc,none","rougeL_acc_stderr,none","rougeL_diff,none","rougeL_diff_stderr,none"
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_compréhension_de_la_date,0.144,0.022249407735450213,"acc_norm,none",6857302016,3.0,{{target}},,,0.144,0.022249407735450213,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_compréhension_des_sports,0.552,0.03151438761115351,"acc_norm,none",6857302016,3.0,{{target}},,,0.552,0.03151438761115351,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_comptage_d_objets,0.224,0.02642136168734791,"acc_norm,none",6857302016,3.0,{{target}},,,0.224,0.02642136168734791,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_déduction_logique_cinq_objets,0.22,0.02625179282460584,"acc_norm,none",6857302016,3.0,{{target}},,,0.22,0.02625179282460584,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_déduction_logique_sept_objets,0.148,0.02250354724380614,"acc_norm,none",6857302016,3.0,{{target}},,,0.148,0.02250354724380614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_déduction_logique_trois_objets,0.348,0.030186568464511686,"acc_norm,none",6857302016,3.0,{{target}},,,0.348,0.030186568464511686,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_désambiguïsation_qa,0.312,0.029361067575219817,"acc_norm,none",6857302016,3.0,{{target}},,,0.312,0.029361067575219817,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_expressions_booléennes,0.46,0.031584653891499004,"acc_norm,none",6857302016,3.0,{{target}},,,0.46,0.031584653891499004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_formes_géométriques,0.112,0.019985536939171437,"acc_norm,none",6857302016,3.0,{{target}},,,0.112,0.019985536939171437,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_hyperbate,0.484,0.03166998503010742,"acc_norm,none",6857302016,3.0,{{target}},,,0.484,0.03166998503010742,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_jugement_causal,0.4919786096256685,0.03665706061581778,"acc_norm,none",6857302016,3.0,{{target}},,,0.4919786096256685,0.03665706061581778,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_naviguer,0.576,0.03131803437491614,"acc_norm,none",6857302016,3.0,{{target}},,,0.576,0.03131803437491614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_pingouins_sur_une_table,0.2191780821917808,0.03435504786264928,"acc_norm,none",6857302016,3.0,{{target}},,,0.2191780821917808,0.03435504786264928,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_raisonnement_sur_les_objets_colorés,0.156,0.022995023034068748,"acc_norm,none",6857302016,3.0,{{target}},,,0.156,0.022995023034068748,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_recommandation_de_film,0.296,0.02892893938837963,"acc_norm,none",6857302016,3.0,{{target}},,,0.296,0.02892893938837963,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_sarcasmes,0.5393258426966292,0.03746587736387869,"acc_norm,none",6857302016,3.0,{{target}},,,0.5393258426966292,0.03746587736387869,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_sophismes_formels,0.532,0.031621252575725504,"acc_norm,none",6857302016,3.0,{{target}},,,0.532,0.031621252575725504,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_cinq_objets,0.2,0.025348970020979078,"acc_norm,none",6857302016,3.0,{{target}},,,0.2,0.025348970020979078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_sept_objets,0.108,0.019669559381568752,"acc_norm,none",6857302016,3.0,{{target}},,,0.108,0.019669559381568752,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_trois_objets,0.332,0.02984403904746589,"acc_norm,none",6857302016,3.0,{{target}},,,0.332,0.02984403904746589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_séquences_temporelles,0.184,0.02455581299422256,"acc_norm,none",6857302016,3.0,{{target}},,,0.184,0.02455581299422256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_bbh_toile_de_mensonges,0.512,0.03167708558254708,"acc_norm,none",6857302016,3.0,{{target}},,,0.512,0.03167708558254708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_gpqa_diamond_fr,0.2639593908629442,0.03148410927642122,"acc_norm,none",6857302016,0.0,answer,,,0.2639593908629442,0.03148410927642122,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_gpqa_extended_fr,0.24861878453038674,0.018565108552111804,"acc_norm,none",6857302016,0.0,answer,,,0.24861878453038674,0.018565108552111804,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_gpqa_main_fr,0.2261072261072261,0.020219747704114503,"acc_norm,none",6857302016,0.0,answer,,,0.2261072261072261,0.020219747704114503,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_ifeval_fr,0.02330097087378641,0.006654046431364153,"prompt_level_loose_acc,none",6857302016,0.0,0,,,,,,,,,,,,,0.02524271844660194,0.006918863237981381,0.2086824067022087,N/A,0.02330097087378641,0.006654046431364153,0.21858339680121858,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_algebra_hard_fr,0.014285714285714285,0.006352048301969054,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.014285714285714285,0.006352048301969054,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_counting_and_prob_hard_fr,0.00510204081632653,0.005102040816326531,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.00510204081632653,0.005102040816326531,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_geometry_hard_fr,0.009708737864077669,0.006848349729346166,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.009708737864077669,0.006848349729346166,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_num_theory_hard_fr,0.004608294930875576,0.004608294930875575,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.004608294930875576,0.004608294930875575,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_prealgebra_hard_fr,0.0,0.0,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_math_precalculus_hard_fr,0.0,0.0,"exact_match,none",6857302016,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_mmlu_fr,0.2519584104828372,0.0036637706770020913,"acc,none",6857302016,5.0,Answer,0.2519584104828372,0.0036637706770020913,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_musr_murder_mysteries_fr,0.504,0.031685198551199154,"acc_norm,none",6857302016,0.0,{{answer_choice}},,,0.504,0.031685198551199154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_musr_object_placements_fr,0.265625,0.027658162598649488,"acc_norm,none",6857302016,0.0,{{answer_choice}},,,0.265625,0.027658162598649488,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,fr_leaderboard,leaderboard_musr_team_allocation_fr,0.248,0.027367497504863555,"acc_norm,none",6857302016,0.0,{{answer_choice}},,,0.248,0.027367497504863555,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,french_bench,french_bench_arc_challenge,0.290846877673225,0.013288647804165358,"acc_norm,none",6857302016,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.2446535500427716,0.012578458921815737,0.290846877673225,0.013288647804165358,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,french_bench,french_bench_grammar,0.7815126050420168,0.03803997152889484,"acc,none",6857302016,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7815126050420168,0.03803997152889484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,french_bench,french_bench_hellaswag,0.4197901049475262,0.005107460126998205,"acc_norm,none",6857302016,5.0,{{label}},0.3366887984579139,0.004890680817363448,0.4197901049475262,0.005107460126998205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,french_bench,french_bench_vocab,0.6974789915966386,0.04228655753449826,"acc,none",6857302016,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.6974789915966386,0.04228655753449826,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,arc_de,0.2728828058169376,0.013033734552785703,"acc_norm,none",6857302016,25.0,gold,0.21899059024807527,0.012100949230812877,0.2728828058169376,0.013033734552785703,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,arc_es,0.2923076923076923,0.013302556252265542,"acc_norm,none",6857302016,25.0,gold,0.2452991452991453,0.01258427449627725,0.2923076923076923,0.013302556252265542,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,arc_fr,0.29597946963216426,0.013356788048643695,"acc_norm,none",6857302016,25.0,gold,0.2506415739948674,0.012680895706050929,0.29597946963216426,0.013356788048643695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,arc_it,0.2694610778443114,0.012982199528535547,"acc_norm,none",6857302016,25.0,gold,0.24037639007698888,0.01250327289928353,0.2694610778443114,0.012982199528535547,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,m_mmlu_de,0.2582591642781717,0.003801292923533988,"acc,none",6857302016,25.0,answer,0.2582591642781717,0.003801292923533988,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,m_mmlu_es,0.2583620818959052,0.0037909356391134376,"acc,none",6857302016,25.0,answer,0.2583620818959052,0.0037909356391134376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,m_mmlu_fr,0.2542204568023833,0.003805753516423808,"acc,none",6857302016,25.0,answer,0.2542204568023833,0.003805753516423808,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,leaderboard,m_mmlu_it,0.26493918561607616,0.0038358035342660733,"acc,none",6857302016,25.0,answer,0.26493918561607616,0.0038358035342660733,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,arc_de,0.2728828058169376,0.013033734552785703,"acc_norm,none",6857302016,25.0,gold,0.21899059024807527,0.012100949230812877,0.2728828058169376,0.013033734552785703,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,arc_es,0.2923076923076923,0.013302556252265542,"acc_norm,none",6857302016,25.0,gold,0.2452991452991453,0.01258427449627725,0.2923076923076923,0.013302556252265542,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,arc_fr,0.29597946963216426,0.013356788048643695,"acc_norm,none",6857302016,25.0,gold,0.2506415739948674,0.012680895706050929,0.29597946963216426,0.013356788048643695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,arc_it,0.2694610778443114,0.012982199528535547,"acc_norm,none",6857302016,25.0,gold,0.24037639007698888,0.01250327289928353,0.2694610778443114,0.012982199528535547,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,m_mmlu_de,0.2582591642781717,0.003801292923533988,"acc,none",6857302016,25.0,answer,0.2582591642781717,0.003801292923533988,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,m_mmlu_es,0.2583620818959052,0.0037909356391134376,"acc,none",6857302016,25.0,answer,0.2583620818959052,0.0037909356391134376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,m_mmlu_fr,0.2542204568023833,0.003805753516423808,"acc,none",6857302016,25.0,answer,0.2542204568023833,0.003805753516423808,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,okapi,m_mmlu_it,0.26493918561607616,0.0038358035342660733,"acc,none",6857302016,25.0,answer,0.26493918561607616,0.0038358035342660733,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,arc_challenge,0.3856655290102389,0.01422425097325718,"acc_norm,none",6857302016,25.0,{{choices.label.index(answerKey)}},0.34897610921501704,0.013928933461382501,0.3856655290102389,0.01422425097325718,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,gsm8k,0.026535253980288095,0.004427045987265163,"exact_match,flexible-extract",6857302016,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.02350265352539803,0.004172883669643973,0.026535253980288095,0.004427045987265163,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,hellaswag,0.6539533957379008,0.004747360500742485,"acc_norm,none",6857302016,10.0,{{label}},0.4809798844851623,0.004986169849946311,0.6539533957379008,0.004747360500742485,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,humanities,0.3013815090329437,0.006632629194659845,"acc,none",6857302016,,,0.3013815090329437,0.006632629194659845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu,0.2641361629397522,0.0037153261259414965,"acc,none",6857302016,,,0.2641361629397522,0.0037153261259414965,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_abstract_algebra,0.25,0.04351941398892446,"acc,none",6857302016,5.0,answer,0.25,0.04351941398892446,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_anatomy,0.34074074074074073,0.04094376269996794,"acc,none",6857302016,5.0,answer,0.34074074074074073,0.04094376269996794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_astronomy,0.27631578947368424,0.03639057569952925,"acc,none",6857302016,5.0,answer,0.27631578947368424,0.03639057569952925,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_business_ethics,0.25,0.04351941398892446,"acc,none",6857302016,5.0,answer,0.25,0.04351941398892446,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_clinical_knowledge,0.2490566037735849,0.02661648298050171,"acc,none",6857302016,5.0,answer,0.2490566037735849,0.02661648298050171,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_biology,0.2777777777777778,0.03745554791462457,"acc,none",6857302016,5.0,answer,0.2777777777777778,0.03745554791462457,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_chemistry,0.21,0.040936018074033256,"acc,none",6857302016,5.0,answer,0.21,0.040936018074033256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_computer_science,0.3,0.046056618647183814,"acc,none",6857302016,5.0,answer,0.3,0.046056618647183814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_mathematics,0.21,0.040936018074033256,"acc,none",6857302016,5.0,answer,0.21,0.040936018074033256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_medicine,0.1907514450867052,0.029957851329869337,"acc,none",6857302016,5.0,answer,0.1907514450867052,0.029957851329869337,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_college_physics,0.2549019607843137,0.043364327079931785,"acc,none",6857302016,5.0,answer,0.2549019607843137,0.043364327079931785,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_computer_security,0.29,0.04560480215720684,"acc,none",6857302016,5.0,answer,0.29,0.04560480215720684,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_conceptual_physics,0.32340425531914896,0.030579442773610348,"acc,none",6857302016,5.0,answer,0.32340425531914896,0.030579442773610348,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation,0.32965389545648766,0.003908571442781024,"acc,none",6857302016,,,0.32965389545648766,0.003908571442781024,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_abstract_algebra,0.25,0.04351941398892446,"acc_norm,none",6857302016,0.0,{{answer}},0.22,0.04163331998932268,0.25,0.04351941398892446,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_anatomy,0.37777777777777777,0.04188307537595853,"acc_norm,none",6857302016,0.0,{{answer}},0.4,0.04232073695151589,0.37777777777777777,0.04188307537595853,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_astronomy,0.39473684210526316,0.039777499346220734,"acc_norm,none",6857302016,0.0,{{answer}},0.3026315789473684,0.03738520676119668,0.39473684210526316,0.039777499346220734,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_business_ethics,0.45,0.05000000000000001,"acc_norm,none",6857302016,0.0,{{answer}},0.47,0.050161355804659205,0.45,0.05000000000000001,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_clinical_knowledge,0.4188679245283019,0.030365050829115205,"acc_norm,none",6857302016,0.0,{{answer}},0.3433962264150943,0.02922452646912479,0.4188679245283019,0.030365050829115205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_biology,0.3611111111111111,0.040166600304512336,"acc_norm,none",6857302016,0.0,{{answer}},0.3680555555555556,0.040329990539607175,0.3611111111111111,0.040166600304512336,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_chemistry,0.24,0.042923469599092816,"acc_norm,none",6857302016,0.0,{{answer}},0.26,0.04408440022768078,0.24,0.042923469599092816,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_computer_science,0.23,0.04229525846816505,"acc_norm,none",6857302016,0.0,{{answer}},0.26,0.0440844002276808,0.23,0.04229525846816505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_mathematics,0.21,0.040936018074033256,"acc_norm,none",6857302016,0.0,{{answer}},0.22,0.041633319989322695,0.21,0.040936018074033256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_medicine,0.3468208092485549,0.03629146670159663,"acc_norm,none",6857302016,0.0,{{answer}},0.3352601156069364,0.03599586301247078,0.3468208092485549,0.03629146670159663,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_college_physics,0.23529411764705882,0.04220773659171453,"acc_norm,none",6857302016,0.0,{{answer}},0.19607843137254902,0.03950581861179964,0.23529411764705882,0.04220773659171453,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_computer_security,0.4,0.04923659639173309,"acc_norm,none",6857302016,0.0,{{answer}},0.4,0.049236596391733084,0.4,0.04923659639173309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_conceptual_physics,0.33191489361702126,0.030783736757745657,"acc_norm,none",6857302016,0.0,{{answer}},0.34893617021276596,0.03115852213135778,0.33191489361702126,0.030783736757745657,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_econometrics,0.22807017543859648,0.03947152782669415,"acc_norm,none",6857302016,0.0,{{answer}},0.21052631578947367,0.038351539543994194,0.22807017543859648,0.03947152782669415,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_electrical_engineering,0.32413793103448274,0.03900432069185553,"acc_norm,none",6857302016,0.0,{{answer}},0.2896551724137931,0.037800192304380156,0.32413793103448274,0.03900432069185553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_elementary_mathematics,0.30952380952380953,0.023809523809523864,"acc_norm,none",6857302016,0.0,{{answer}},0.29894179894179895,0.02357760479165581,0.30952380952380953,0.023809523809523864,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_formal_logic,0.38095238095238093,0.043435254289490986,"acc_norm,none",6857302016,0.0,{{answer}},0.3412698412698413,0.04240799327574923,0.38095238095238093,0.043435254289490986,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_global_facts,0.27,0.044619604333847394,"acc_norm,none",6857302016,0.0,{{answer}},0.29,0.045604802157206845,0.27,0.044619604333847394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_biology,0.432258064516129,0.02818173972001941,"acc_norm,none",6857302016,0.0,{{answer}},0.36451612903225805,0.027379871229943245,0.432258064516129,0.02818173972001941,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_chemistry,0.2512315270935961,0.030516530732694436,"acc_norm,none",6857302016,0.0,{{answer}},0.1921182266009852,0.027719315709614768,0.2512315270935961,0.030516530732694436,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_computer_science,0.37,0.04852365870939099,"acc_norm,none",6857302016,0.0,{{answer}},0.32,0.046882617226215034,0.37,0.04852365870939099,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_european_history,0.44242424242424244,0.03878372113711274,"acc_norm,none",6857302016,0.0,{{answer}},0.28484848484848485,0.03524390844511784,0.44242424242424244,0.03878372113711274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_geography,0.4292929292929293,0.03526552724601198,"acc_norm,none",6857302016,0.0,{{answer}},0.36363636363636365,0.03427308652999936,0.4292929292929293,0.03526552724601198,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_government_and_politics,0.47150259067357514,0.03602573571288442,"acc_norm,none",6857302016,0.0,{{answer}},0.41968911917098445,0.03561587327685884,0.47150259067357514,0.03602573571288442,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_macroeconomics,0.31794871794871793,0.02361088430892786,"acc_norm,none",6857302016,0.0,{{answer}},0.26666666666666666,0.02242127361292371,0.31794871794871793,0.02361088430892786,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_mathematics,0.2074074074074074,0.024720713193952155,"acc_norm,none",6857302016,0.0,{{answer}},0.15925925925925927,0.02231039463004063,0.2074074074074074,0.024720713193952155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_microeconomics,0.37815126050420167,0.031499305777849054,"acc_norm,none",6857302016,0.0,{{answer}},0.3067226890756303,0.029953823891887044,0.37815126050420167,0.031499305777849054,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_physics,0.26490066225165565,0.03603038545360384,"acc_norm,none",6857302016,0.0,{{answer}},0.31125827814569534,0.03780445850526732,0.26490066225165565,0.03603038545360384,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_psychology,0.45137614678899085,0.02133571471126879,"acc_norm,none",6857302016,0.0,{{answer}},0.4935779816513762,0.021435554820013077,0.45137614678899085,0.02133571471126879,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_statistics,0.28703703703703703,0.030851992993257013,"acc_norm,none",6857302016,0.0,{{answer}},0.2962962962962963,0.031141447823536048,0.28703703703703703,0.030851992993257013,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_us_history,0.39215686274509803,0.03426712349247271,"acc_norm,none",6857302016,0.0,{{answer}},0.3431372549019608,0.033321399446680854,0.39215686274509803,0.03426712349247271,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_high_school_world_history,0.3670886075949367,0.031376240725616185,"acc_norm,none",6857302016,0.0,{{answer}},0.33755274261603374,0.03078154910202621,0.3670886075949367,0.031376240725616185,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_human_aging,0.36771300448430494,0.03236198350928275,"acc_norm,none",6857302016,0.0,{{answer}},0.3901345291479821,0.03273766725459156,0.36771300448430494,0.03236198350928275,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_human_sexuality,0.37404580152671757,0.042438692422305246,"acc_norm,none",6857302016,0.0,{{answer}},0.4351145038167939,0.04348208051644858,0.37404580152671757,0.042438692422305246,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_international_law,0.33884297520661155,0.043207678075366705,"acc_norm,none",6857302016,0.0,{{answer}},0.2396694214876033,0.03896878985070416,0.33884297520661155,0.043207678075366705,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_jurisprudence,0.39814814814814814,0.04732332615978813,"acc_norm,none",6857302016,0.0,{{answer}},0.25925925925925924,0.04236511258094631,0.39814814814814814,0.04732332615978813,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_logical_fallacies,0.38650306748466257,0.038258255488486076,"acc_norm,none",6857302016,0.0,{{answer}},0.3558282208588957,0.03761521380046734,0.38650306748466257,0.038258255488486076,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_machine_learning,0.25,0.04109974682633932,"acc_norm,none",6857302016,0.0,{{answer}},0.30357142857142855,0.04364226155841044,0.25,0.04109974682633932,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_management,0.4563106796116505,0.04931801994220414,"acc_norm,none",6857302016,0.0,{{answer}},0.3106796116504854,0.0458212416016155,0.4563106796116505,0.04931801994220414,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_marketing,0.5042735042735043,0.032754892643821316,"acc_norm,none",6857302016,0.0,{{answer}},0.5,0.03275608910402091,0.5042735042735043,0.032754892643821316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_medical_genetics,0.48,0.050211673156867795,"acc_norm,none",6857302016,0.0,{{answer}},0.38,0.048783173121456316,0.48,0.050211673156867795,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_miscellaneous,0.46871008939974457,0.017844918090468544,"acc_norm,none",6857302016,0.0,{{answer}},0.5057471264367817,0.01787878232612923,0.46871008939974457,0.017844918090468544,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_moral_disputes,0.28034682080924855,0.02418242749657762,"acc_norm,none",6857302016,0.0,{{answer}},0.3063583815028902,0.024818350129436593,0.28034682080924855,0.02418242749657762,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_moral_scenarios,0.27262569832402234,0.014893391735249603,"acc_norm,none",6857302016,0.0,{{answer}},0.23798882681564246,0.014242630070574885,0.27262569832402234,0.014893391735249603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_nutrition,0.3954248366013072,0.02799672318063146,"acc_norm,none",6857302016,0.0,{{answer}},0.3006535947712418,0.026256053835718964,0.3954248366013072,0.02799672318063146,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_philosophy,0.3858520900321543,0.02764814959975147,"acc_norm,none",6857302016,0.0,{{answer}},0.3408360128617363,0.026920841260776162,0.3858520900321543,0.02764814959975147,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_prehistory,0.37962962962962965,0.027002521034516468,"acc_norm,none",6857302016,0.0,{{answer}},0.42901234567901236,0.027538925613470863,0.37962962962962965,0.027002521034516468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_professional_accounting,0.22695035460992907,0.024987106365642976,"acc_norm,none",6857302016,0.0,{{answer}},0.2624113475177305,0.026244920349843007,0.22695035460992907,0.024987106365642976,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_professional_law,0.2966101694915254,0.011665946586082849,"acc_norm,none",6857302016,0.0,{{answer}},0.26988265971316816,0.01133738108425041,0.2966101694915254,0.011665946586082849,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_professional_medicine,0.33088235294117646,0.02858270975389842,"acc_norm,none",6857302016,0.0,{{answer}},0.3088235294117647,0.028064998167040094,0.33088235294117646,0.02858270975389842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_professional_psychology,0.31862745098039214,0.018850084696468712,"acc_norm,none",6857302016,0.0,{{answer}},0.32189542483660133,0.018901015322093092,0.31862745098039214,0.018850084696468712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_public_relations,0.3090909090909091,0.044262946482000985,"acc_norm,none",6857302016,0.0,{{answer}},0.45454545454545453,0.04769300568972744,0.3090909090909091,0.044262946482000985,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_security_studies,0.2979591836734694,0.029279567411065667,"acc_norm,none",6857302016,0.0,{{answer}},0.3224489795918367,0.029923100563683913,0.2979591836734694,0.029279567411065667,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_sociology,0.3582089552238806,0.03390393042268815,"acc_norm,none",6857302016,0.0,{{answer}},0.3034825870646766,0.03251006816458619,0.3582089552238806,0.03390393042268815,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_us_foreign_policy,0.34,0.04760952285695236,"acc_norm,none",6857302016,0.0,{{answer}},0.37,0.04852365870939099,0.34,0.04760952285695236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_virology,0.3674698795180723,0.03753267402120574,"acc_norm,none",6857302016,0.0,{{answer}},0.26506024096385544,0.03436024037944967,0.3674698795180723,0.03753267402120574,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_continuation_world_religions,0.5730994152046783,0.03793620616529916,"acc_norm,none",6857302016,0.0,{{answer}},0.49707602339181284,0.03834759370936839,0.5730994152046783,0.03793620616529916,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_econometrics,0.2719298245614035,0.04185774424022057,"acc,none",6857302016,5.0,answer,0.2719298245614035,0.04185774424022057,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_electrical_engineering,0.2620689655172414,0.036646663372252565,"acc,none",6857302016,5.0,answer,0.2620689655172414,0.036646663372252565,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_elementary_mathematics,0.291005291005291,0.023393826500484865,"acc,none",6857302016,5.0,answer,0.291005291005291,0.023393826500484865,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_formal_logic,0.15873015873015872,0.03268454013011744,"acc,none",6857302016,5.0,answer,0.15873015873015872,0.03268454013011744,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_global_facts,0.32,0.04688261722621505,"acc,none",6857302016,5.0,answer,0.32,0.04688261722621505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_biology,0.25483870967741934,0.0247901184593322,"acc,none",6857302016,5.0,answer,0.25483870967741934,0.0247901184593322,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_chemistry,0.2512315270935961,0.030516530732694436,"acc,none",6857302016,5.0,answer,0.2512315270935961,0.030516530732694436,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_computer_science,0.32,0.046882617226215034,"acc,none",6857302016,5.0,answer,0.32,0.046882617226215034,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_european_history,0.24242424242424243,0.03346409881055953,"acc,none",6857302016,5.0,answer,0.24242424242424243,0.03346409881055953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_geography,0.24242424242424243,0.030532892233932036,"acc,none",6857302016,5.0,answer,0.24242424242424243,0.030532892233932036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_government_and_politics,0.23316062176165803,0.03051611137147601,"acc,none",6857302016,5.0,answer,0.23316062176165803,0.03051611137147601,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_macroeconomics,0.32051282051282054,0.023661296393964283,"acc,none",6857302016,5.0,answer,0.32051282051282054,0.023661296393964283,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_mathematics,0.24074074074074073,0.026067159222275798,"acc,none",6857302016,5.0,answer,0.24074074074074073,0.026067159222275798,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_microeconomics,0.23109243697478993,0.027381406927868952,"acc,none",6857302016,5.0,answer,0.23109243697478993,0.027381406927868952,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_physics,0.2582781456953642,0.035737053147634576,"acc,none",6857302016,5.0,answer,0.2582781456953642,0.035737053147634576,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_psychology,0.23853211009174313,0.01827257581023186,"acc,none",6857302016,5.0,answer,0.23853211009174313,0.01827257581023186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_statistics,0.35185185185185186,0.03256850570293647,"acc,none",6857302016,5.0,answer,0.35185185185185186,0.03256850570293647,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_us_history,0.25980392156862747,0.030778554678693254,"acc,none",6857302016,5.0,answer,0.25980392156862747,0.030778554678693254,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_high_school_world_history,0.2616033755274262,0.028609516716994927,"acc,none",6857302016,5.0,answer,0.2616033755274262,0.028609516716994927,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_human_aging,0.3004484304932735,0.03076935200822914,"acc,none",6857302016,5.0,answer,0.3004484304932735,0.03076935200822914,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_human_sexuality,0.2366412213740458,0.03727673575596919,"acc,none",6857302016,5.0,answer,0.2366412213740458,0.03727673575596919,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_humanities,0.25313496280552605,0.006332817760876259,"acc,none",6857302016,,,0.25313496280552605,0.006332817760876259,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_international_law,0.38016528925619836,0.04431324501968432,"acc,none",6857302016,5.0,answer,0.38016528925619836,0.04431324501968432,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_jurisprudence,0.28703703703703703,0.043733130409147614,"acc,none",6857302016,5.0,answer,0.28703703703703703,0.043733130409147614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_logical_fallacies,0.26993865030674846,0.03487825168497892,"acc,none",6857302016,5.0,answer,0.26993865030674846,0.03487825168497892,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_machine_learning,0.2767857142857143,0.04246624336697625,"acc,none",6857302016,5.0,answer,0.2767857142857143,0.04246624336697625,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_management,0.2912621359223301,0.04498676320572922,"acc,none",6857302016,5.0,answer,0.2912621359223301,0.04498676320572922,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_marketing,0.23076923076923078,0.02760192138141759,"acc,none",6857302016,5.0,answer,0.23076923076923078,0.02760192138141759,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_medical_genetics,0.22,0.04163331998932268,"acc,none",6857302016,5.0,answer,0.22,0.04163331998932268,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_miscellaneous,0.28607918263090676,0.016160871405127532,"acc,none",6857302016,5.0,answer,0.28607918263090676,0.016160871405127532,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_moral_disputes,0.26011560693641617,0.023618678310069374,"acc,none",6857302016,5.0,answer,0.26011560693641617,0.023618678310069374,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_moral_scenarios,0.24692737430167597,0.014422292204808838,"acc,none",6857302016,5.0,answer,0.24692737430167597,0.014422292204808838,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_nutrition,0.2581699346405229,0.025058503316958154,"acc,none",6857302016,5.0,answer,0.2581699346405229,0.025058503316958154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_other,0.27325394271000963,0.007988634722887952,"acc,none",6857302016,,,0.27325394271000963,0.007988634722887952,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_philosophy,0.2765273311897106,0.02540383297817961,"acc,none",6857302016,5.0,answer,0.2765273311897106,0.02540383297817961,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_prehistory,0.25925925925925924,0.02438366553103545,"acc,none",6857302016,5.0,answer,0.25925925925925924,0.02438366553103545,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_professional_accounting,0.30851063829787234,0.02755336616510137,"acc,none",6857302016,5.0,answer,0.30851063829787234,0.02755336616510137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_professional_law,0.23728813559322035,0.010865436690780257,"acc,none",6857302016,5.0,answer,0.23728813559322035,0.010865436690780257,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_professional_medicine,0.27941176470588236,0.02725720260611495,"acc,none",6857302016,5.0,answer,0.27941176470588236,0.02725720260611495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_professional_psychology,0.272875816993464,0.01802047414839358,"acc,none",6857302016,5.0,answer,0.272875816993464,0.01802047414839358,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_public_relations,0.35454545454545455,0.045820048415054174,"acc,none",6857302016,5.0,answer,0.35454545454545455,0.045820048415054174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_security_studies,0.17551020408163265,0.024352800722970015,"acc,none",6857302016,5.0,answer,0.17551020408163265,0.024352800722970015,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_social_sciences,0.257393565160871,0.007863516247594533,"acc,none",6857302016,,,0.257393565160871,0.007863516247594533,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_sociology,0.25870646766169153,0.03096590312357303,"acc,none",6857302016,5.0,answer,0.25870646766169153,0.03096590312357303,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_stem,0.2781477957500793,0.00797663241870028,"acc,none",6857302016,,,0.2781477957500793,0.00797663241870028,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_us_foreign_policy,0.26,0.04408440022768077,"acc,none",6857302016,5.0,answer,0.26,0.04408440022768077,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_virology,0.3253012048192771,0.03647168523683227,"acc,none",6857302016,5.0,answer,0.3253012048192771,0.03647168523683227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,mmlu_world_religions,0.29239766081871343,0.034886477134579215,"acc,none",6857302016,5.0,answer,0.29239766081871343,0.034886477134579215,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,other,0.3826842613453492,0.008569963961337869,"acc,none",6857302016,,,0.3826842613453492,0.008569963961337869,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,social sciences,0.3587910302242444,0.00853650897578099,"acc,none",6857302016,,,0.3587910302242444,0.00853650897578099,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,stem,0.291151284490961,0.008020157440186268,"acc,none",6857302016,,,0.291151284490961,0.008020157440186268,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,truthfulqa_gen,0.2692778457772338,0.015528566637087267,"rouge1_acc,none",6857302016,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,24.63010173546932,0.7421573982715609,0.2802937576499388,0.015723139524608774,-8.847440755540575,0.7895947800599356,50.582545922869016,0.8275122047019977,0.2692778457772338,0.015528566637087267,-10.822346464064593,0.8350764254792622,33.52358105770809,0.958380497583704,0.22276621787025705,0.014566506961396742,-13.284869933945387,1.0121558045387418,47.64422658278255,0.8388993412931399,0.25458996328029376,0.015250117079156508,-11.12778948183462,0.8519785160659245
EleutherAI/pythia-6.9b,299892736000,openllm,truthfulqa_mc1,0.21909424724602203,0.01448003857875746,"acc,none",6857302016,0.0,0,0.21909424724602203,0.01448003857875746,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,truthfulqa_mc2,0.35137357991296697,0.013616200377976177,"acc,none",6857302016,0.0,0,0.35137357991296697,0.013616200377976177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EleutherAI/pythia-6.9b,299892736000,openllm,winogrande,0.648776637726914,0.013415981370545128,"acc,none",6857302016,5.0,,0.648776637726914,0.013415981370545128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_compréhension_de_la_date,0.208,0.025721398901416396,"acc_norm,none",7069016064,3.0,{{target}},,,0.208,0.025721398901416396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_compréhension_des_sports,0.54,0.031584653891499,"acc_norm,none",7069016064,3.0,{{target}},,,0.54,0.031584653891499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_comptage_d_objets,0.096,0.018668961419477187,"acc_norm,none",7069016064,3.0,{{target}},,,0.096,0.018668961419477187,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_cinq_objets,0.184,0.02455581299422256,"acc_norm,none",7069016064,3.0,{{target}},,,0.184,0.02455581299422256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_sept_objets,0.152,0.022752024491765468,"acc_norm,none",7069016064,3.0,{{target}},,,0.152,0.022752024491765468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_trois_objets,0.336,0.029933259094191516,"acc_norm,none",7069016064,3.0,{{target}},,,0.336,0.029933259094191516,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_désambiguïsation_qa,0.3,0.029040893477575862,"acc_norm,none",7069016064,3.0,{{target}},,,0.3,0.029040893477575862,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_expressions_booléennes,0.46,0.031584653891499004,"acc_norm,none",7069016064,3.0,{{target}},,,0.46,0.031584653891499004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_formes_géométriques,0.112,0.019985536939171444,"acc_norm,none",7069016064,3.0,{{target}},,,0.112,0.019985536939171444,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_hyperbate,0.516,0.03166998503010742,"acc_norm,none",7069016064,3.0,{{target}},,,0.516,0.03166998503010742,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_jugement_causal,0.5187165775401069,0.03663608375537842,"acc_norm,none",7069016064,3.0,{{target}},,,0.5187165775401069,0.03663608375537842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_naviguer,0.472,0.03163648953154439,"acc_norm,none",7069016064,3.0,{{target}},,,0.472,0.03163648953154439,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_pingouins_sur_une_table,0.2328767123287671,0.03510036341139227,"acc_norm,none",7069016064,3.0,{{target}},,,0.2328767123287671,0.03510036341139227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_raisonnement_sur_les_objets_colorés,0.148,0.022503547243806144,"acc_norm,none",7069016064,3.0,{{target}},,,0.148,0.022503547243806144,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_recommandation_de_film,0.248,0.027367497504863548,"acc_norm,none",7069016064,3.0,{{target}},,,0.248,0.027367497504863548,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_sarcasmes,0.550561797752809,0.03738964966056965,"acc_norm,none",7069016064,3.0,{{target}},,,0.550561797752809,0.03738964966056965,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_sophismes_formels,0.468,0.031621252575725504,"acc_norm,none",7069016064,3.0,{{target}},,,0.468,0.031621252575725504,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_cinq_objets,0.2,0.025348970020979078,"acc_norm,none",7069016064,3.0,{{target}},,,0.2,0.025348970020979078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_sept_objets,0.152,0.022752024491765468,"acc_norm,none",7069016064,3.0,{{target}},,,0.152,0.022752024491765468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_trois_objets,0.308,0.029256928606501864,"acc_norm,none",7069016064,3.0,{{target}},,,0.308,0.029256928606501864,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_séquences_temporelles,0.256,0.02765710871820491,"acc_norm,none",7069016064,3.0,{{target}},,,0.256,0.02765710871820491,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_bbh_toile_de_mensonges,0.456,0.03156328506121339,"acc_norm,none",7069016064,3.0,{{target}},,,0.456,0.03156328506121339,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_gpqa_diamond_fr,0.28426395939086296,0.032218796071824624,"acc_norm,none",7069016064,0.0,answer,,,0.28426395939086296,0.032218796071824624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_gpqa_extended_fr,0.2578268876611418,0.018789579807329218,"acc_norm,none",7069016064,0.0,answer,,,0.2578268876611418,0.018789579807329218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_gpqa_main_fr,0.2331002331002331,0.020437076579903176,"acc_norm,none",7069016064,0.0,answer,,,0.2331002331002331,0.020437076579903176,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_ifeval_fr,0.009708737864077669,0.004324952097790176,"prompt_level_loose_acc,none",7069016064,0.0,0,,,,,,,,,,,,,0.013592233009708738,0.005107308452923588,0.08453922315308454,N/A,0.009708737864077669,0.004324952097790176,0.08606245239908607,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_algebra_hard_fr,0.0,0.0,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_counting_and_prob_hard_fr,0.0,0.0,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_geometry_hard_fr,0.0,0.0,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_num_theory_hard_fr,0.004608294930875576,0.004608294930875572,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.004608294930875576,0.004608294930875572,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_prealgebra_hard_fr,0.0,0.0,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_math_precalculus_hard_fr,0.0,0.0,"exact_match,none",7069016064,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_mmlu_fr,0.26299672411337416,0.0037154451893209877,"acc,none",7069016064,5.0,Answer,0.26299672411337416,0.0037154451893209877,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_musr_murder_mysteries_fr,0.5,0.031686212526223896,"acc_norm,none",7069016064,0.0,{{answer_choice}},,,0.5,0.031686212526223896,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_musr_object_placements_fr,0.25,0.02711630722733202,"acc_norm,none",7069016064,0.0,{{answer_choice}},,,0.25,0.02711630722733202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,fr_leaderboard,leaderboard_musr_team_allocation_fr,0.252,0.02751385193303136,"acc_norm,none",7069016064,0.0,{{answer_choice}},,,0.252,0.02751385193303136,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench,french_bench_arc_challenge,0.3712574850299401,0.014136848432206825,"acc_norm,none",7069016064,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.31479897348160824,0.01358952368253817,0.3712574850299401,0.014136848432206825,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench,french_bench_grammar,0.8235294117647058,0.03509414936549958,"acc,none",7069016064,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.8235294117647058,0.03509414936549958,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench,french_bench_hellaswag,0.5739987149282502,0.005117492740296177,"acc_norm,none",7069016064,5.0,{{label}},0.4281430713214821,0.00512076062053857,0.5739987149282502,0.005117492740296177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench,french_bench_vocab,0.7899159663865546,0.03750126918012132,"acc,none",7069016064,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7899159663865546,0.03750126918012132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench_gen,french_bench_fquadv2_genq,0.225548205370323,0.0,"rouge1,none",7069016064,5.0,{{question}},,,,,0.225548205370323,N/A,0.19100956522380838,0.008865356603873258,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench_gen,french_bench_fquadv2_hasAns,0.6320749460209172,0.0,"rouge1,none",7069016064,5.0,{% if answers.text| length > 0 %}{{answers.text[0]}}{% else %}{{['Impossible']}}{% endif %},,,,,0.6320749460209172,N/A,0.615987946884996,0.019214447966658863,0.3675,0.024136399679191754,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench_gen,french_bench_multifquad,0.5288150672066294,0.0,"rouge1,none",7069016064,5.0,"{{', '.join(answers.text)}}",,,,,0.5288150672066294,N/A,0.5144646807578526,0.01596414206549759,0.0775,0.01338590044887033,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench_gen,french_bench_orangesum_abstract,0.2253368277538288,0.0,"rouge1,none",7069016064,5.0,{{summary}},,,,,0.2253368277538288,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bigscience/bloom-7b1,350000000000,french_bench_gen,french_bench_trivia,0.5488751292567084,0.0,"rouge1,none",7069016064,5.0,{{Answer}},,,,,0.5488751292567084,N/A,0.5376839826839829,0.023571421106675133,0.4394736842105263,0.025494402360263287,0.41842105263157897,0.025339118200992206,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_compréhension_de_la_date,0.188,0.02476037772775051,"acc_norm,none",1345423360,3.0,{{target}},,,0.188,0.02476037772775051,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_compréhension_des_sports,0.46,0.031584653891499,"acc_norm,none",1345423360,3.0,{{target}},,,0.46,0.031584653891499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_comptage_d_objets,0.068,0.01595374841074702,"acc_norm,none",1345423360,3.0,{{target}},,,0.068,0.01595374841074702,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_cinq_objets,0.22,0.026251792824605845,"acc_norm,none",1345423360,3.0,{{target}},,,0.22,0.026251792824605845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_sept_objets,0.136,0.021723342617052065,"acc_norm,none",1345423360,3.0,{{target}},,,0.136,0.021723342617052065,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_trois_objets,0.36,0.030418764025174978,"acc_norm,none",1345423360,3.0,{{target}},,,0.36,0.030418764025174978,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_désambiguïsation_qa,0.372,0.030630325944558313,"acc_norm,none",1345423360,3.0,{{target}},,,0.372,0.030630325944558313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_expressions_booléennes,0.46,0.031584653891499004,"acc_norm,none",1345423360,3.0,{{target}},,,0.46,0.031584653891499004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_formes_géométriques,0.084,0.017578738526776324,"acc_norm,none",1345423360,3.0,{{target}},,,0.084,0.017578738526776324,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_hyperbate,0.436,0.03142556706028128,"acc_norm,none",1345423360,3.0,{{target}},,,0.436,0.03142556706028128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_jugement_causal,0.5187165775401069,0.03663608375537842,"acc_norm,none",1345423360,3.0,{{target}},,,0.5187165775401069,0.03663608375537842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_naviguer,0.42,0.03127799950463662,"acc_norm,none",1345423360,3.0,{{target}},,,0.42,0.03127799950463662,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_pingouins_sur_une_table,0.2534246575342466,0.03612245461624573,"acc_norm,none",1345423360,3.0,{{target}},,,0.2534246575342466,0.03612245461624573,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_raisonnement_sur_les_objets_colorés,0.168,0.023692813205492585,"acc_norm,none",1345423360,3.0,{{target}},,,0.168,0.023692813205492585,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_recommandation_de_film,0.228,0.026587432487268494,"acc_norm,none",1345423360,3.0,{{target}},,,0.228,0.026587432487268494,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_sarcasmes,0.46629213483146065,0.03749680060368987,"acc_norm,none",1345423360,3.0,{{target}},,,0.46629213483146065,0.03749680060368987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_sophismes_formels,0.46,0.031584653891499004,"acc_norm,none",1345423360,3.0,{{target}},,,0.46,0.031584653891499004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_cinq_objets,0.192,0.024960691989172005,"acc_norm,none",1345423360,3.0,{{target}},,,0.192,0.024960691989172005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_sept_objets,0.148,0.022503547243806148,"acc_norm,none",1345423360,3.0,{{target}},,,0.148,0.022503547243806148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_trois_objets,0.328,0.029752391824475373,"acc_norm,none",1345423360,3.0,{{target}},,,0.328,0.029752391824475373,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_séquences_temporelles,0.204,0.025537121574548148,"acc_norm,none",1345423360,3.0,{{target}},,,0.204,0.025537121574548148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_bbh_toile_de_mensonges,0.512,0.03167708558254708,"acc_norm,none",1345423360,3.0,{{target}},,,0.512,0.03167708558254708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_gpqa_diamond_fr,0.27411167512690354,0.03186182418247317,"acc_norm,none",1345423360,0.0,answer,,,0.27411167512690354,0.03186182418247317,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_gpqa_extended_fr,0.2504604051565378,0.01861089187330517,"acc_norm,none",1345423360,0.0,answer,,,0.2504604051565378,0.01861089187330517,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_gpqa_main_fr,0.2540792540792541,0.021043068137727102,"acc_norm,none",1345423360,0.0,answer,,,0.2540792540792541,0.021043068137727102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_ifeval_fr,0.015533980582524271,0.0054545657935066464,"prompt_level_loose_acc,none",1345423360,0.0,0,,,,,,,,,,,,,0.019417475728155338,0.006086349682701512,0.12871287128712872,N/A,0.015533980582524271,0.0054545657935066464,0.1210967250571211,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_algebra_hard_fr,0.011428571428571429,0.005689672739661832,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.011428571428571429,0.005689672739661832,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_counting_and_prob_hard_fr,0.01020408163265306,0.007196850575679085,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.01020408163265306,0.007196850575679085,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_geometry_hard_fr,0.0,0.0,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_num_theory_hard_fr,0.0,0.0,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_prealgebra_hard_fr,0.0,0.0,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_math_precalculus_hard_fr,0.0,0.0,"exact_match,none",1345423360,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.0,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_mmlu_fr,0.25210084033613445,0.0036644571663738473,"acc,none",1345423360,5.0,Answer,0.25210084033613445,0.0036644571663738473,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_musr_murder_mysteries_fr,0.508,0.031682156431413803,"acc_norm,none",1345423360,0.0,{{answer_choice}},,,0.508,0.031682156431413803,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_musr_object_placements_fr,0.23828125,0.026679160987075002,"acc_norm,none",1345423360,0.0,{{answer_choice}},,,0.23828125,0.026679160987075002,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,fr_leaderboard,leaderboard_musr_team_allocation_fr,0.236,0.02690933759495384,"acc_norm,none",1345423360,0.0,{{answer_choice}},,,0.236,0.02690933759495384,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,french_bench,french_bench_arc_challenge,0.3105218135158255,0.013538947396873264,"acc_norm,none",1345423360,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.2557741659538067,0.012766130743681107,0.3105218135158255,0.013538947396873264,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,french_bench,french_bench_grammar,0.7983193277310925,0.03693851725228281,"acc,none",1345423360,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7983193277310925,0.03693851725228281,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,french_bench,french_bench_hellaswag,0.5071749839366031,0.005173942584570164,"acc_norm,none",1345423360,5.0,{{label}},0.39408866995073893,0.005057056803697162,0.5071749839366031,0.005173942584570164,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,french_bench,french_bench_vocab,0.7815126050420168,0.03803997152889484,"acc,none",1345423360,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7815126050420168,0.03803997152889484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,arc_de,0.2557741659538067,0.012766130743681098,"acc_norm,none",1345423360,25.0,gold,0.1924721984602224,0.01153563015540481,0.2557741659538067,0.012766130743681098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,arc_es,0.2564102564102564,0.012771065618749026,"acc_norm,none",1345423360,25.0,gold,0.2111111111111111,0.011935928534109866,0.2564102564102564,0.012771065618749026,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,arc_fr,0.31137724550898205,0.013549170237200151,"acc_norm,none",1345423360,25.0,gold,0.2660393498716852,0.012929683850700155,0.31137724550898205,0.013549170237200151,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,arc_it,0.262617621899059,0.012876175520452837,"acc_norm,none",1345423360,25.0,gold,0.20102651839178784,0.011726581781869408,0.262617621899059,0.012876175520452837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,m_mmlu_de,0.2481520591341077,0.0037514686438599775,"acc,none",1345423360,25.0,answer,0.2481520591341077,0.0037514686438599775,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,m_mmlu_es,0.23953802309884506,0.003696256004644125,"acc,none",1345423360,25.0,answer,0.23953802309884506,0.003696256004644125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,m_mmlu_fr,0.25796348636467803,0.0038240356620502457,"acc,none",1345423360,25.0,answer,0.25796348636467803,0.0038240356620502457,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,leaderboard,m_mmlu_it,0.2433330815139382,0.0037297057684196814,"acc,none",1345423360,25.0,answer,0.2433330815139382,0.0037297057684196814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,arc_de,0.2557741659538067,0.012766130743681098,"acc_norm,none",1345423360,25.0,gold,0.1924721984602224,0.01153563015540481,0.2557741659538067,0.012766130743681098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,arc_es,0.2564102564102564,0.012771065618749026,"acc_norm,none",1345423360,25.0,gold,0.2111111111111111,0.011935928534109866,0.2564102564102564,0.012771065618749026,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,arc_fr,0.31137724550898205,0.013549170237200151,"acc_norm,none",1345423360,25.0,gold,0.2660393498716852,0.012929683850700155,0.31137724550898205,0.013549170237200151,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,arc_it,0.262617621899059,0.012876175520452837,"acc_norm,none",1345423360,25.0,gold,0.20102651839178784,0.011726581781869408,0.262617621899059,0.012876175520452837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,m_mmlu_de,0.2481520591341077,0.0037514686438599775,"acc,none",1345423360,25.0,answer,0.2481520591341077,0.0037514686438599775,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,m_mmlu_es,0.23953802309884506,0.003696256004644125,"acc,none",1345423360,25.0,answer,0.23953802309884506,0.003696256004644125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,m_mmlu_fr,0.25796348636467803,0.0038240356620502457,"acc,none",1345423360,25.0,answer,0.25796348636467803,0.0038240356620502457,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,okapi,m_mmlu_it,0.2433330815139382,0.0037297057684196814,"acc,none",1345423360,25.0,answer,0.2433330815139382,0.0037297057684196814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,arc_challenge,0.3174061433447099,0.01360223908803817,"acc_norm,none",1345423360,25.0,{{choices.label.index(answerKey)}},0.2713310580204778,0.0129938077275458,0.3174061433447099,0.01360223908803817,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,gsm8k,0.021986353297952996,0.0040391627581100615,"exact_match,flexible-extract",1345423360,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.006065200909780136,0.0021386703014604656,0.021986353297952996,0.0040391627581100615,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,hellaswag,0.5404301931886079,0.004973442060741631,"acc_norm,none",1345423360,10.0,{{label}},0.41047600079665403,0.004909148239488267,0.5404301931886079,0.004973442060741631,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,humanities,0.2680127523910733,0.006434959535282622,"acc,none",1345423360,,,0.2680127523910733,0.006434959535282622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu,0.25160233584959407,0.0036576787872109997,"acc,none",1345423360,,,0.25160233584959407,0.0036576787872109997,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_abstract_algebra,0.34,0.04760952285695235,"acc,none",1345423360,5.0,answer,0.34,0.04760952285695235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_anatomy,0.28888888888888886,0.0391545063041425,"acc,none",1345423360,5.0,answer,0.28888888888888886,0.0391545063041425,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_astronomy,0.19736842105263158,0.03238981601699397,"acc,none",1345423360,5.0,answer,0.19736842105263158,0.03238981601699397,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_business_ethics,0.23,0.04229525846816506,"acc,none",1345423360,5.0,answer,0.23,0.04229525846816506,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_clinical_knowledge,0.25660377358490566,0.026880647889051982,"acc,none",1345423360,5.0,answer,0.25660377358490566,0.026880647889051982,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_biology,0.25,0.03621034121889507,"acc,none",1345423360,5.0,answer,0.25,0.03621034121889507,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_chemistry,0.22,0.04163331998932269,"acc,none",1345423360,5.0,answer,0.22,0.04163331998932269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_computer_science,0.41,0.049431107042371025,"acc,none",1345423360,5.0,answer,0.41,0.049431107042371025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_mathematics,0.31,0.04648231987117316,"acc,none",1345423360,5.0,answer,0.31,0.04648231987117316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_medicine,0.24277456647398843,0.0326926380614177,"acc,none",1345423360,5.0,answer,0.24277456647398843,0.0326926380614177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_college_physics,0.2549019607843137,0.043364327079931785,"acc,none",1345423360,5.0,answer,0.2549019607843137,0.043364327079931785,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_computer_security,0.24,0.04292346959909284,"acc,none",1345423360,5.0,answer,0.24,0.04292346959909284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_conceptual_physics,0.2851063829787234,0.029513196625539355,"acc,none",1345423360,5.0,answer,0.2851063829787234,0.029513196625539355,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation,0.2940464321321749,0.0038097993776527953,"acc,none",1345423360,,,0.2940464321321749,0.0038097993776527953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_abstract_algebra,0.18,0.038612291966536955,"acc_norm,none",1345423360,0.0,{{answer}},0.22,0.041633319989322674,0.18,0.038612291966536955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_anatomy,0.2962962962962963,0.03944624162501116,"acc_norm,none",1345423360,0.0,{{answer}},0.3111111111111111,0.03999262876617723,0.2962962962962963,0.03944624162501116,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_astronomy,0.3881578947368421,0.03965842097512744,"acc_norm,none",1345423360,0.0,{{answer}},0.2894736842105263,0.036906779861372814,0.3881578947368421,0.03965842097512744,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_business_ethics,0.44,0.04988876515698589,"acc_norm,none",1345423360,0.0,{{answer}},0.52,0.050211673156867795,0.44,0.04988876515698589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_clinical_knowledge,0.3320754716981132,0.028985455652334395,"acc_norm,none",1345423360,0.0,{{answer}},0.2679245283018868,0.027257260322494845,0.3320754716981132,0.028985455652334395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_biology,0.3194444444444444,0.03899073687357335,"acc_norm,none",1345423360,0.0,{{answer}},0.3194444444444444,0.038990736873573344,0.3194444444444444,0.03899073687357335,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_chemistry,0.25,0.04351941398892446,"acc_norm,none",1345423360,0.0,{{answer}},0.25,0.04351941398892446,0.25,0.04351941398892446,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_computer_science,0.26,0.04408440022768079,"acc_norm,none",1345423360,0.0,{{answer}},0.28,0.04512608598542127,0.26,0.04408440022768079,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_mathematics,0.22,0.04163331998932269,"acc_norm,none",1345423360,0.0,{{answer}},0.16,0.03684529491774711,0.22,0.04163331998932269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_medicine,0.27167630057803466,0.03391750322321659,"acc_norm,none",1345423360,0.0,{{answer}},0.30057803468208094,0.03496101481191181,0.27167630057803466,0.03391750322321659,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_college_physics,0.2549019607843137,0.04336432707993177,"acc_norm,none",1345423360,0.0,{{answer}},0.2549019607843137,0.04336432707993177,0.2549019607843137,0.04336432707993177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_computer_security,0.37,0.048523658709391,"acc_norm,none",1345423360,0.0,{{answer}},0.34,0.047609522856952344,0.37,0.048523658709391,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_conceptual_physics,0.2680851063829787,0.028957342788342343,"acc_norm,none",1345423360,0.0,{{answer}},0.3404255319148936,0.030976692998534432,0.2680851063829787,0.028957342788342343,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_econometrics,0.2543859649122807,0.040969851398436716,"acc_norm,none",1345423360,0.0,{{answer}},0.21052631578947367,0.038351539543994194,0.2543859649122807,0.040969851398436716,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_electrical_engineering,0.27586206896551724,0.037245636197746325,"acc_norm,none",1345423360,0.0,{{answer}},0.2689655172413793,0.036951833116502325,0.27586206896551724,0.037245636197746325,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_elementary_mathematics,0.25132275132275134,0.022340482339643895,"acc_norm,none",1345423360,0.0,{{answer}},0.23015873015873015,0.021679219663693138,0.25132275132275134,0.022340482339643895,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_formal_logic,0.23809523809523808,0.038095238095238106,"acc_norm,none",1345423360,0.0,{{answer}},0.2857142857142857,0.0404061017820884,0.23809523809523808,0.038095238095238106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_global_facts,0.28,0.04512608598542127,"acc_norm,none",1345423360,0.0,{{answer}},0.34,0.04760952285695235,0.28,0.04512608598542127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_biology,0.31290322580645163,0.026377567028645858,"acc_norm,none",1345423360,0.0,{{answer}},0.29354838709677417,0.025906087021319295,0.31290322580645163,0.026377567028645858,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_chemistry,0.22660098522167488,0.02945486383529298,"acc_norm,none",1345423360,0.0,{{answer}},0.18719211822660098,0.027444924966882615,0.22660098522167488,0.02945486383529298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_computer_science,0.34,0.04760952285695235,"acc_norm,none",1345423360,0.0,{{answer}},0.28,0.04512608598542127,0.34,0.04760952285695235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_european_history,0.4484848484848485,0.038835659779569286,"acc_norm,none",1345423360,0.0,{{answer}},0.2727272727272727,0.0347769116216366,0.4484848484848485,0.038835659779569286,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_geography,0.3484848484848485,0.033948539651564025,"acc_norm,none",1345423360,0.0,{{answer}},0.3181818181818182,0.03318477333845331,0.3484848484848485,0.033948539651564025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_government_and_politics,0.40932642487046633,0.03548608168860806,"acc_norm,none",1345423360,0.0,{{answer}},0.32124352331606215,0.033699508685490674,0.40932642487046633,0.03548608168860806,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_macroeconomics,0.2717948717948718,0.02255655101013235,"acc_norm,none",1345423360,0.0,{{answer}},0.28205128205128205,0.02281581309889661,0.2717948717948718,0.02255655101013235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_mathematics,0.17037037037037037,0.02292255486307496,"acc_norm,none",1345423360,0.0,{{answer}},0.14814814814814814,0.02165977842211803,0.17037037037037037,0.02292255486307496,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_microeconomics,0.3319327731092437,0.030588697013783667,"acc_norm,none",1345423360,0.0,{{answer}},0.2605042016806723,0.028510251512341923,0.3319327731092437,0.030588697013783667,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_physics,0.271523178807947,0.03631329803969653,"acc_norm,none",1345423360,0.0,{{answer}},0.2582781456953642,0.035737053147634576,0.271523178807947,0.03631329803969653,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_psychology,0.3504587155963303,0.020456077599824457,"acc_norm,none",1345423360,0.0,{{answer}},0.3724770642201835,0.020728368457638494,0.3504587155963303,0.020456077599824457,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_statistics,0.2916666666666667,0.03099866630456053,"acc_norm,none",1345423360,0.0,{{answer}},0.26851851851851855,0.030225226160012407,0.2916666666666667,0.03099866630456053,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_us_history,0.36764705882352944,0.03384132045674118,"acc_norm,none",1345423360,0.0,{{answer}},0.3088235294117647,0.03242661719827218,0.36764705882352944,0.03384132045674118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_high_school_world_history,0.32489451476793246,0.030486039389105296,"acc_norm,none",1345423360,0.0,{{answer}},0.29535864978902954,0.029696338713422893,0.32489451476793246,0.030486039389105296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_human_aging,0.32286995515695066,0.03138147637575498,"acc_norm,none",1345423360,0.0,{{answer}},0.4170403587443946,0.03309266936071721,0.32286995515695066,0.03138147637575498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_human_sexuality,0.3816793893129771,0.042607351576445594,"acc_norm,none",1345423360,0.0,{{answer}},0.3893129770992366,0.04276486542814591,0.3816793893129771,0.042607351576445594,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_international_law,0.38016528925619836,0.04431324501968432,"acc_norm,none",1345423360,0.0,{{answer}},0.19008264462809918,0.035817969517092825,0.38016528925619836,0.04431324501968432,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_jurisprudence,0.3333333333333333,0.04557239513497751,"acc_norm,none",1345423360,0.0,{{answer}},0.24074074074074073,0.04133119440243839,0.3333333333333333,0.04557239513497751,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_logical_fallacies,0.3619631901840491,0.037757007291414416,"acc_norm,none",1345423360,0.0,{{answer}},0.294478527607362,0.03581165790474082,0.3619631901840491,0.037757007291414416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_machine_learning,0.30357142857142855,0.04364226155841044,"acc_norm,none",1345423360,0.0,{{answer}},0.22321428571428573,0.03952301967702511,0.30357142857142855,0.04364226155841044,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_management,0.36893203883495146,0.04777615181156739,"acc_norm,none",1345423360,0.0,{{answer}},0.3106796116504854,0.045821241601615506,0.36893203883495146,0.04777615181156739,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_marketing,0.42735042735042733,0.032408473935163266,"acc_norm,none",1345423360,0.0,{{answer}},0.44871794871794873,0.032583346493868806,0.42735042735042733,0.032408473935163266,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_medical_genetics,0.38,0.04878317312145632,"acc_norm,none",1345423360,0.0,{{answer}},0.27,0.0446196043338474,0.38,0.04878317312145632,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_miscellaneous,0.4112388250319285,0.017595971908056573,"acc_norm,none",1345423360,0.0,{{answer}},0.4342273307790549,0.01772458938967779,0.4112388250319285,0.017595971908056573,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_moral_disputes,0.2398843930635838,0.022989592543123567,"acc_norm,none",1345423360,0.0,{{answer}},0.2543352601156069,0.023445826276545546,0.2398843930635838,0.022989592543123567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_moral_scenarios,0.27262569832402234,0.014893391735249603,"acc_norm,none",1345423360,0.0,{{answer}},0.23798882681564246,0.014242630070574885,0.27262569832402234,0.014893391735249603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_nutrition,0.3464052287581699,0.02724561304721536,"acc_norm,none",1345423360,0.0,{{answer}},0.2777777777777778,0.025646863097137908,0.3464052287581699,0.02724561304721536,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_philosophy,0.2861736334405145,0.02567025924218894,"acc_norm,none",1345423360,0.0,{{answer}},0.2540192926045016,0.024723861504771696,0.2861736334405145,0.02567025924218894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_prehistory,0.2623456790123457,0.024477222856135114,"acc_norm,none",1345423360,0.0,{{answer}},0.3765432098765432,0.026959344518747787,0.2623456790123457,0.024477222856135114,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_professional_accounting,0.24113475177304963,0.025518731049537766,"acc_norm,none",1345423360,0.0,{{answer}},0.2872340425531915,0.026992199173064356,0.24113475177304963,0.025518731049537766,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_professional_law,0.26988265971316816,0.011337381084250408,"acc_norm,none",1345423360,0.0,{{answer}},0.24967405475880053,0.011054538377832329,0.26988265971316816,0.011337381084250408,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_professional_medicine,0.30514705882352944,0.0279715413701706,"acc_norm,none",1345423360,0.0,{{answer}},0.29044117647058826,0.027576468622740512,0.30514705882352944,0.0279715413701706,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_professional_psychology,0.29248366013071897,0.018403415710109797,"acc_norm,none",1345423360,0.0,{{answer}},0.2957516339869281,0.01846315413263281,0.29248366013071897,0.018403415710109797,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_public_relations,0.3090909090909091,0.044262946482000985,"acc_norm,none",1345423360,0.0,{{answer}},0.42727272727272725,0.04738198703545483,0.3090909090909091,0.044262946482000985,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_security_studies,0.2571428571428571,0.02797982353874455,"acc_norm,none",1345423360,0.0,{{answer}},0.3142857142857143,0.02971932942241748,0.2571428571428571,0.02797982353874455,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_sociology,0.2537313432835821,0.03076944496729601,"acc_norm,none",1345423360,0.0,{{answer}},0.27860696517412936,0.031700561834973086,0.2537313432835821,0.03076944496729601,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_us_foreign_policy,0.35,0.0479372485441102,"acc_norm,none",1345423360,0.0,{{answer}},0.3,0.046056618647183814,0.35,0.0479372485441102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_virology,0.3253012048192771,0.03647168523683227,"acc_norm,none",1345423360,0.0,{{answer}},0.25903614457831325,0.03410646614071856,0.3253012048192771,0.03647168523683227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_continuation_world_religions,0.4269005847953216,0.03793620616529918,"acc_norm,none",1345423360,0.0,{{answer}},0.38011695906432746,0.037229657413855394,0.4269005847953216,0.03793620616529918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_econometrics,0.21929824561403508,0.03892431106518753,"acc,none",1345423360,5.0,answer,0.21929824561403508,0.03892431106518753,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_electrical_engineering,0.25517241379310346,0.03632984052707842,"acc,none",1345423360,5.0,answer,0.25517241379310346,0.03632984052707842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_elementary_mathematics,0.24338624338624337,0.022101128787415422,"acc,none",1345423360,5.0,answer,0.24338624338624337,0.022101128787415422,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_formal_logic,0.1746031746031746,0.03395490020856112,"acc,none",1345423360,5.0,answer,0.1746031746031746,0.03395490020856112,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_global_facts,0.29,0.045604802157206845,"acc,none",1345423360,5.0,answer,0.29,0.045604802157206845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_biology,0.23870967741935484,0.024251071262208834,"acc,none",1345423360,5.0,answer,0.23870967741935484,0.024251071262208834,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_chemistry,0.2413793103448276,0.03010833071801162,"acc,none",1345423360,5.0,answer,0.2413793103448276,0.03010833071801162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_computer_science,0.34,0.04760952285695236,"acc,none",1345423360,5.0,answer,0.34,0.04760952285695236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_european_history,0.24242424242424243,0.03346409881055953,"acc,none",1345423360,5.0,answer,0.24242424242424243,0.03346409881055953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_geography,0.24242424242424243,0.030532892233932026,"acc,none",1345423360,5.0,answer,0.24242424242424243,0.030532892233932026,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_government_and_politics,0.18652849740932642,0.02811209121011746,"acc,none",1345423360,5.0,answer,0.18652849740932642,0.02811209121011746,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_macroeconomics,0.2153846153846154,0.020843034557462874,"acc,none",1345423360,5.0,answer,0.2153846153846154,0.020843034557462874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_mathematics,0.25555555555555554,0.026593939101844072,"acc,none",1345423360,5.0,answer,0.25555555555555554,0.026593939101844072,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_microeconomics,0.24789915966386555,0.028047967224176896,"acc,none",1345423360,5.0,answer,0.24789915966386555,0.028047967224176896,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_physics,0.26490066225165565,0.036030385453603826,"acc,none",1345423360,5.0,answer,0.26490066225165565,0.036030385453603826,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_psychology,0.25688073394495414,0.01873249292834245,"acc,none",1345423360,5.0,answer,0.25688073394495414,0.01873249292834245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_statistics,0.25,0.029531221160930918,"acc,none",1345423360,5.0,answer,0.25,0.029531221160930918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_us_history,0.24509803921568626,0.030190282453501954,"acc,none",1345423360,5.0,answer,0.24509803921568626,0.030190282453501954,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_high_school_world_history,0.2489451476793249,0.028146970599422644,"acc,none",1345423360,5.0,answer,0.2489451476793249,0.028146970599422644,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_human_aging,0.2914798206278027,0.030500283176545913,"acc,none",1345423360,5.0,answer,0.2914798206278027,0.030500283176545913,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_human_sexuality,0.2366412213740458,0.03727673575596918,"acc,none",1345423360,5.0,answer,0.2366412213740458,0.03727673575596918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_humanities,0.2512221041445271,0.006317885948846767,"acc,none",1345423360,,,0.2512221041445271,0.006317885948846767,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_international_law,0.3884297520661157,0.044492703500683836,"acc,none",1345423360,5.0,answer,0.3884297520661157,0.044492703500683836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_jurisprudence,0.3055555555555556,0.04453197507374984,"acc,none",1345423360,5.0,answer,0.3055555555555556,0.04453197507374984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_logical_fallacies,0.294478527607362,0.03581165790474082,"acc,none",1345423360,5.0,answer,0.294478527607362,0.03581165790474082,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_machine_learning,0.3392857142857143,0.04493949068613539,"acc,none",1345423360,5.0,answer,0.3392857142857143,0.04493949068613539,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_management,0.20388349514563106,0.03989139859531772,"acc,none",1345423360,5.0,answer,0.20388349514563106,0.03989139859531772,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_marketing,0.28205128205128205,0.02948036054954119,"acc,none",1345423360,5.0,answer,0.28205128205128205,0.02948036054954119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_medical_genetics,0.3,0.046056618647183814,"acc,none",1345423360,5.0,answer,0.3,0.046056618647183814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_miscellaneous,0.2413793103448276,0.015302380123542075,"acc,none",1345423360,5.0,answer,0.2413793103448276,0.015302380123542075,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_moral_disputes,0.23410404624277456,0.02279711027807113,"acc,none",1345423360,5.0,answer,0.23410404624277456,0.02279711027807113,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_moral_scenarios,0.2446927374301676,0.014378169884098433,"acc,none",1345423360,5.0,answer,0.2446927374301676,0.014378169884098433,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_nutrition,0.23529411764705882,0.02428861946604612,"acc,none",1345423360,5.0,answer,0.23529411764705882,0.02428861946604612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_other,0.2500804634695848,0.0077662237578896036,"acc,none",1345423360,,,0.2500804634695848,0.0077662237578896036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_philosophy,0.2572347266881029,0.024826171289250888,"acc,none",1345423360,5.0,answer,0.2572347266881029,0.024826171289250888,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_prehistory,0.24382716049382716,0.023891879541959607,"acc,none",1345423360,5.0,answer,0.24382716049382716,0.023891879541959607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_professional_accounting,0.26595744680851063,0.026358065698880592,"acc,none",1345423360,5.0,answer,0.26595744680851063,0.026358065698880592,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_professional_law,0.2470664928292047,0.011015752255279345,"acc,none",1345423360,5.0,answer,0.2470664928292047,0.011015752255279345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_professional_medicine,0.18382352941176472,0.02352924218519311,"acc,none",1345423360,5.0,answer,0.18382352941176472,0.02352924218519311,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_professional_psychology,0.25980392156862747,0.01774089950917779,"acc,none",1345423360,5.0,answer,0.25980392156862747,0.01774089950917779,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_public_relations,0.2909090909090909,0.04350271442923243,"acc,none",1345423360,5.0,answer,0.2909090909090909,0.04350271442923243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_security_studies,0.1673469387755102,0.02389714476891452,"acc,none",1345423360,5.0,answer,0.1673469387755102,0.02389714476891452,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_social_sciences,0.23951901202469938,0.0076870452380985044,"acc,none",1345423360,,,0.23951901202469938,0.0076870452380985044,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_sociology,0.263681592039801,0.03115715086935557,"acc,none",1345423360,5.0,answer,0.263681592039801,0.03115715086935557,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_stem,0.2654614652711703,0.007850468029483575,"acc,none",1345423360,,,0.2654614652711703,0.007850468029483575,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_us_foreign_policy,0.29,0.045604802157206845,"acc,none",1345423360,5.0,answer,0.29,0.045604802157206845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_virology,0.28313253012048195,0.03507295431370518,"acc,none",1345423360,5.0,answer,0.28313253012048195,0.03507295431370518,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,mmlu_world_religions,0.2631578947368421,0.033773102522091945,"acc,none",1345423360,5.0,answer,0.2631578947368421,0.033773102522091945,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,other,0.352108142903122,0.008466119440621258,"acc,none",1345423360,,,0.352108142903122,0.008466119440621258,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,social sciences,0.31394215144621385,0.008339656991123474,"acc,none",1345423360,,,0.31394215144621385,0.008339656991123474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,stem,0.2562638756739613,0.0077352956242838136,"acc,none",1345423360,,,0.2562638756739613,0.0077352956242838136,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,truthfulqa_gen,0.20195838433292534,0.014053957441512353,"rouge1_acc,none",1345423360,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,3.5746537481769134,0.3204682059351352,0.1799265605875153,0.013447109235537557,-0.41536581002333867,0.15088790330809135,11.08653249926408,0.6426562325628733,0.20195838433292534,0.014053957441512353,-0.7437016904278017,0.27128164233606417,5.284406697114497,0.4846428197466845,0.09424724602203183,0.010228079300416427,-0.9424459257333404,0.24451124560783855,10.187513725393815,0.599412300063159,0.19583843329253367,0.01389234436774208,-0.655557180045168,0.2435752222722309
croissantllm/CroissantLLMBase,3000000000000,openllm,truthfulqa_mc1,0.23623011015911874,0.014869755015871093,"acc,none",1345423360,0.0,0,0.23623011015911874,0.014869755015871093,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,truthfulqa_mc2,0.37237821800712717,0.01366221421305425,"acc,none",1345423360,0.0,0,0.37237821800712717,0.01366221421305425,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
croissantllm/CroissantLLMBase,3000000000000,openllm,winogrande,0.55327545382794,0.01397248837161669,"acc,none",1345423360,5.0,,0.55327545382794,0.01397248837161669,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_compréhension_de_la_date,0.476,0.03164968895968781,"acc_norm,none",8030261248,3.0,{{target}},,,0.476,0.03164968895968781,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_compréhension_des_sports,0.652,0.030186568464511697,"acc_norm,none",8030261248,3.0,{{target}},,,0.652,0.030186568464511697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_comptage_d_objets,0.392,0.030938207620401195,"acc_norm,none",8030261248,3.0,{{target}},,,0.392,0.030938207620401195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_cinq_objets,0.328,0.029752391824475383,"acc_norm,none",8030261248,3.0,{{target}},,,0.328,0.029752391824475383,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_sept_objets,0.236,0.026909337594953838,"acc_norm,none",8030261248,3.0,{{target}},,,0.236,0.026909337594953838,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_déduction_logique_trois_objets,0.48,0.03166085340849519,"acc_norm,none",8030261248,3.0,{{target}},,,0.48,0.03166085340849519,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_désambiguïsation_qa,0.4,0.031046021028253233,"acc_norm,none",8030261248,3.0,{{target}},,,0.4,0.031046021028253233,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_expressions_booléennes,0.46,0.031584653891499004,"acc_norm,none",8030261248,3.0,{{target}},,,0.46,0.031584653891499004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_formes_géométriques,0.348,0.030186568464511707,"acc_norm,none",8030261248,3.0,{{target}},,,0.348,0.030186568464511707,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_hyperbate,0.528,0.0316364895315444,"acc_norm,none",8030261248,3.0,{{target}},,,0.528,0.0316364895315444,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_jugement_causal,0.48128342245989303,0.03663608375537842,"acc_norm,none",8030261248,3.0,{{target}},,,0.48128342245989303,0.03663608375537842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_naviguer,0.576,0.03131803437491614,"acc_norm,none",8030261248,3.0,{{target}},,,0.576,0.03131803437491614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_pingouins_sur_une_table,0.3835616438356164,0.04038112474853564,"acc_norm,none",8030261248,3.0,{{target}},,,0.3835616438356164,0.04038112474853564,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_raisonnement_sur_les_objets_colorés,0.372,0.030630325944558313,"acc_norm,none",8030261248,3.0,{{target}},,,0.372,0.030630325944558313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_recommandation_de_film,0.556,0.03148684942554574,"acc_norm,none",8030261248,3.0,{{target}},,,0.556,0.03148684942554574,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_sarcasmes,0.5056179775280899,0.03757992900475981,"acc_norm,none",8030261248,3.0,{{target}},,,0.5056179775280899,0.03757992900475981,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_sophismes_formels,0.504,0.031685198551199154,"acc_norm,none",8030261248,3.0,{{target}},,,0.504,0.031685198551199154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_cinq_objets,0.144,0.02224940773545021,"acc_norm,none",8030261248,3.0,{{target}},,,0.144,0.02224940773545021,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_sept_objets,0.124,0.02088638225867326,"acc_norm,none",8030261248,3.0,{{target}},,,0.124,0.02088638225867326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_suivi_objets_mélangés_trois_objets,0.328,0.029752391824475376,"acc_norm,none",8030261248,3.0,{{target}},,,0.328,0.029752391824475376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_séquences_temporelles,0.196,0.025156857313255936,"acc_norm,none",8030261248,3.0,{{target}},,,0.196,0.025156857313255936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_bbh_toile_de_mensonges,0.508,0.031682156431413803,"acc_norm,none",8030261248,3.0,{{target}},,,0.508,0.031682156431413803,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_gpqa_diamond_fr,0.36548223350253806,0.03439750899385474,"acc_norm,none",8030261248,0.0,answer,,,0.36548223350253806,0.03439750899385474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_gpqa_extended_fr,0.289134438305709,0.019473499586525275,"acc_norm,none",8030261248,0.0,answer,,,0.289134438305709,0.019473499586525275,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_gpqa_main_fr,0.29603729603729606,0.022066129127119836,"acc_norm,none",8030261248,0.0,answer,,,0.29603729603729606,0.022066129127119836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_ifeval_fr,0.019417475728155338,0.006086349682701494,"prompt_level_loose_acc,none",8030261248,0.0,0,,,,,,,,,,,,,0.02330097087378641,0.006654046431364158,0.15765422696115766,N/A,0.019417475728155338,0.006086349682701494,0.1667936024371668,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_algebra_hard_fr,0.07714285714285714,0.014282439248008285,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.07714285714285714,0.014282439248008285,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_counting_and_prob_hard_fr,0.015306122448979591,0.008791559199116599,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.015306122448979591,0.008791559199116599,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_geometry_hard_fr,0.019417475728155338,0.00963743649866822,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.019417475728155338,0.00963743649866822,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_num_theory_hard_fr,0.013824884792626729,0.007944762237164794,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.013824884792626729,0.007944762237164794,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_prealgebra_hard_fr,0.09734513274336283,0.028009733301017123,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.09734513274336283,0.028009733301017123,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_math_precalculus_hard_fr,0.011904761904761904,0.011904761904761902,"exact_match,none",8030261248,4.0,{{answer if few_shot is undefined else solution}},,,,,,,,,,,,,,,,,,,,,0.011904761904761904,0.011904761904761902,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_mmlu_fr,0.5263495228599915,0.004213733711006387,"acc,none",8030261248,5.0,Answer,0.5263495228599915,0.004213733711006387,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_musr_murder_mysteries_fr,0.5,0.031686212526223896,"acc_norm,none",8030261248,0.0,{{answer_choice}},,,0.5,0.031686212526223896,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_musr_object_placements_fr,0.2890625,0.02838843806999465,"acc_norm,none",8030261248,0.0,{{answer_choice}},,,0.2890625,0.02838843806999465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,fr_leaderboard,leaderboard_musr_team_allocation_fr,0.264,0.027934518957690908,"acc_norm,none",8030261248,0.0,{{answer_choice}},,,0.264,0.027934518957690908,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench,french_bench_arc_challenge,0.5089820359281437,0.01462778257777394,"acc_norm,none",8030261248,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.4533789563729683,0.01456640669542,0.5089820359281437,0.01462778257777394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench,french_bench_grammar,0.7647058823529411,0.03904916456144797,"acc,none",8030261248,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7647058823529411,0.03904916456144797,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench,french_bench_hellaswag,0.6718783465410152,0.0048591363823387666,"acc_norm,none",8030261248,5.0,{{label}},0.4937888198757764,0.005174076114991718,0.6718783465410152,0.0048591363823387666,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench,french_bench_vocab,0.8487394957983193,0.032984429309406524,"acc,none",8030261248,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.8487394957983193,0.032984429309406524,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench_gen,french_bench_fquadv2_genq,0.3490946664074175,0.0,"rouge1,none",8030261248,5.0,{{question}},,,,,0.3490946664074175,N/A,0.3290418688272903,0.011775499775611025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench_gen,french_bench_fquadv2_hasAns,0.8090964538890904,0.0,"rouge1,none",8030261248,5.0,{% if answers.text| length > 0 %}{{answers.text[0]}}{% else %}{{['Impossible']}}{% endif %},,,,,0.8090964538890904,N/A,0.800287864883104,0.014893714494017513,0.545,0.024929725792117705,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench_gen,french_bench_multifquad,0.6628420045258809,0.0,"rouge1,none",8030261248,5.0,"{{', '.join(answers.text)}}",,,,,0.6628420045258809,N/A,0.6564214069666591,0.014634477373571372,0.19,0.019639610121239274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench_gen,french_bench_orangesum_abstract,0.2872868740021828,0.0,"rouge1,none",8030261248,5.0,{{summary}},,,,,0.2872868740021828,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,french_bench_gen,french_bench_trivia,0.7430472232445917,0.0,"rouge1,none",8030261248,5.0,{{Answer}},,,,,0.7430472232445917,N/A,0.728627025709069,0.020261345600661663,0.6078947368421053,0.025078174648458913,0.5842105263157895,0.02531639301514226,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,arc_de,0.4636441402908469,0.01459141740443866,"acc_norm,none",8030261248,25.0,gold,0.41659538066723695,0.01442516320371223,0.4636441402908469,0.01459141740443866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,arc_es,0.5153846153846153,0.014616960326221319,"acc_norm,none",8030261248,25.0,gold,0.46324786324786327,0.014584325475224508,0.5153846153846153,0.014616960326221319,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,arc_fr,0.5004277159965783,0.014630138046609936,"acc_norm,none",8030261248,25.0,gold,0.4593669803250642,0.014581753402377686,0.5004277159965783,0.014630138046609936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,arc_it,0.5072711719418306,0.014628596328069858,"acc_norm,none",8030261248,25.0,gold,0.4636441402908469,0.014591417404438664,0.5072711719418306,0.014628596328069858,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,m_mmlu_de,0.561547744757882,0.0043095494629396104,"acc,none",8030261248,25.0,answer,0.561547744757882,0.0043095494629396104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,m_mmlu_es,0.5777711114444278,0.0042774795718552505,"acc,none",8030261248,25.0,answer,0.5777711114444278,0.0042774795718552505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,m_mmlu_fr,0.5744404552746162,0.004321483442731279,"acc,none",8030261248,25.0,answer,0.5744404552746162,0.004321483442731279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,leaderboard,m_mmlu_it,0.5712019339729546,0.004301727187947193,"acc,none",8030261248,25.0,answer,0.5712019339729546,0.004301727187947193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,arc_de,0.4636441402908469,0.01459141740443866,"acc_norm,none",8030261248,25.0,gold,0.41659538066723695,0.01442516320371223,0.4636441402908469,0.01459141740443866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,arc_es,0.5153846153846153,0.014616960326221319,"acc_norm,none",8030261248,25.0,gold,0.46324786324786327,0.014584325475224508,0.5153846153846153,0.014616960326221319,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,arc_fr,0.5004277159965783,0.014630138046609936,"acc_norm,none",8030261248,25.0,gold,0.4593669803250642,0.014581753402377686,0.5004277159965783,0.014630138046609936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,arc_it,0.5072711719418306,0.014628596328069858,"acc_norm,none",8030261248,25.0,gold,0.4636441402908469,0.014591417404438664,0.5072711719418306,0.014628596328069858,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,m_mmlu_de,0.561547744757882,0.0043095494629396104,"acc,none",8030261248,25.0,answer,0.561547744757882,0.0043095494629396104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,m_mmlu_es,0.5777711114444278,0.0042774795718552505,"acc,none",8030261248,25.0,answer,0.5777711114444278,0.0042774795718552505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,m_mmlu_fr,0.5744404552746162,0.004321483442731279,"acc,none",8030261248,25.0,answer,0.5744404552746162,0.004321483442731279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,okapi,m_mmlu_it,0.5712019339729546,0.004301727187947193,"acc,none",8030261248,25.0,answer,0.5712019339729546,0.004301727187947193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,arc_challenge,0.5819112627986348,0.014413988396996074,"acc_norm,none",8030261248,25.0,{{choices.label.index(answerKey)}},0.5477815699658704,0.014544519880633827,0.5819112627986348,0.014413988396996074,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,gsm8k,0.514783927217589,0.013766463050787601,"exact_match,flexible-extract",8030261248,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.511751326762699,0.013768680408142796,0.514783927217589,0.013766463050787601,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,hellaswag,0.8170683130850428,0.0038582038518200175,"acc_norm,none",8030261248,10.0,{{label}},0.6170085640310695,0.004851227527070909,0.8170683130850428,0.0038582038518200175,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,humanities,0.35515409139213605,0.006773666936701515,"acc,none",8030261248,,,0.35515409139213605,0.006773666936701515,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu,0.6519726534681669,0.00379623139201471,"acc,none",8030261248,,,0.6519726534681669,0.00379623139201471,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_abstract_algebra,0.3,0.046056618647183814,"acc,none",8030261248,5.0,answer,0.3,0.046056618647183814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_anatomy,0.6222222222222222,0.04188307537595853,"acc,none",8030261248,5.0,answer,0.6222222222222222,0.04188307537595853,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_astronomy,0.7039473684210527,0.03715062154998904,"acc,none",8030261248,5.0,answer,0.7039473684210527,0.03715062154998904,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_business_ethics,0.65,0.0479372485441102,"acc,none",8030261248,5.0,answer,0.65,0.0479372485441102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_clinical_knowledge,0.7584905660377359,0.026341480371118362,"acc,none",8030261248,5.0,answer,0.7584905660377359,0.026341480371118362,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_biology,0.7569444444444444,0.03586879280080342,"acc,none",8030261248,5.0,answer,0.7569444444444444,0.03586879280080342,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_chemistry,0.43,0.04975698519562428,"acc,none",8030261248,5.0,answer,0.43,0.04975698519562428,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_computer_science,0.46,0.05009082659620332,"acc,none",8030261248,5.0,answer,0.46,0.05009082659620332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_mathematics,0.3,0.046056618647183814,"acc,none",8030261248,5.0,answer,0.3,0.046056618647183814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_medicine,0.653179190751445,0.03629146670159663,"acc,none",8030261248,5.0,answer,0.653179190751445,0.03629146670159663,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_college_physics,0.49019607843137253,0.04974229460422817,"acc,none",8030261248,5.0,answer,0.49019607843137253,0.04974229460422817,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_computer_security,0.83,0.0377525168068637,"acc,none",8030261248,5.0,answer,0.83,0.0377525168068637,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_conceptual_physics,0.6,0.03202563076101735,"acc,none",8030261248,5.0,answer,0.6,0.03202563076101735,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation,0.44609030052699045,0.004028669044988495,"acc,none",8030261248,,,0.44609030052699045,0.004028669044988495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_abstract_algebra,0.28,0.045126085985421276,"acc_norm,none",8030261248,0.0,{{answer}},0.24,0.04292346959909283,0.28,0.045126085985421276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_anatomy,0.5777777777777777,0.042667634040995814,"acc_norm,none",8030261248,0.0,{{answer}},0.5555555555555556,0.04292596718256981,0.5777777777777777,0.042667634040995814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_astronomy,0.5460526315789473,0.04051646342874143,"acc_norm,none",8030261248,0.0,{{answer}},0.47368421052631576,0.04063302731486671,0.5460526315789473,0.04051646342874143,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_business_ethics,0.63,0.04852365870939098,"acc_norm,none",8030261248,0.0,{{answer}},0.72,0.045126085985421276,0.63,0.04852365870939098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_clinical_knowledge,0.5358490566037736,0.030693675018458006,"acc_norm,none",8030261248,0.0,{{answer}},0.43018867924528303,0.030471445867183235,0.5358490566037736,0.030693675018458006,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_biology,0.5902777777777778,0.04112490974670787,"acc_norm,none",8030261248,0.0,{{answer}},0.5625,0.04148415739394154,0.5902777777777778,0.04112490974670787,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_chemistry,0.42,0.049604496374885836,"acc_norm,none",8030261248,0.0,{{answer}},0.37,0.04852365870939098,0.42,0.049604496374885836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_computer_science,0.44,0.04988876515698589,"acc_norm,none",8030261248,0.0,{{answer}},0.36,0.048241815132442176,0.44,0.04988876515698589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_mathematics,0.29,0.045604802157206845,"acc_norm,none",8030261248,0.0,{{answer}},0.22,0.041633319989322695,0.29,0.045604802157206845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_medicine,0.48554913294797686,0.03810871630454764,"acc_norm,none",8030261248,0.0,{{answer}},0.43352601156069365,0.03778621079092055,0.48554913294797686,0.03810871630454764,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_college_physics,0.3431372549019608,0.04724007352383889,"acc_norm,none",8030261248,0.0,{{answer}},0.3333333333333333,0.04690650298201943,0.3431372549019608,0.04724007352383889,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_computer_security,0.56,0.049888765156985884,"acc_norm,none",8030261248,0.0,{{answer}},0.53,0.050161355804659205,0.56,0.049888765156985884,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_conceptual_physics,0.5234042553191489,0.03265019475033582,"acc_norm,none",8030261248,0.0,{{answer}},0.5574468085106383,0.03246956919789958,0.5234042553191489,0.03265019475033582,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_econometrics,0.3508771929824561,0.04489539350270699,"acc_norm,none",8030261248,0.0,{{answer}},0.37719298245614036,0.04559522141958216,0.3508771929824561,0.04489539350270699,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_electrical_engineering,0.4413793103448276,0.04137931034482758,"acc_norm,none",8030261248,0.0,{{answer}},0.3931034482758621,0.040703290137070705,0.4413793103448276,0.04137931034482758,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_elementary_mathematics,0.6957671957671958,0.02369541500946309,"acc_norm,none",8030261248,0.0,{{answer}},0.6878306878306878,0.023865206836972578,0.6957671957671958,0.02369541500946309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_formal_logic,0.42857142857142855,0.04426266681379909,"acc_norm,none",8030261248,0.0,{{answer}},0.4523809523809524,0.044518079590553275,0.42857142857142855,0.04426266681379909,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_global_facts,0.54,0.05009082659620332,"acc_norm,none",8030261248,0.0,{{answer}},0.55,0.049999999999999996,0.54,0.05009082659620332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_biology,0.5709677419354838,0.028156036538233193,"acc_norm,none",8030261248,0.0,{{answer}},0.4870967741935484,0.028434533152681848,0.5709677419354838,0.028156036538233193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_chemistry,0.3645320197044335,0.0338640574606209,"acc_norm,none",8030261248,0.0,{{answer}},0.3251231527093596,0.032957975663112704,0.3645320197044335,0.0338640574606209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_computer_science,0.48,0.050211673156867795,"acc_norm,none",8030261248,0.0,{{answer}},0.47,0.050161355804659205,0.48,0.050211673156867795,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_european_history,0.503030303030303,0.03904272341431857,"acc_norm,none",8030261248,0.0,{{answer}},0.38181818181818183,0.03793713171165633,0.503030303030303,0.03904272341431857,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_geography,0.6060606060606061,0.03481285338232963,"acc_norm,none",8030261248,0.0,{{answer}},0.5303030303030303,0.03555804051763929,0.6060606060606061,0.03481285338232963,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_government_and_politics,0.5958549222797928,0.0354150857888402,"acc_norm,none",8030261248,0.0,{{answer}},0.5492227979274611,0.035909109522355244,0.5958549222797928,0.0354150857888402,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_macroeconomics,0.5051282051282051,0.02534967290683866,"acc_norm,none",8030261248,0.0,{{answer}},0.44358974358974357,0.025189149894764205,0.5051282051282051,0.02534967290683866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_mathematics,0.35555555555555557,0.02918571494985741,"acc_norm,none",8030261248,0.0,{{answer}},0.3037037037037037,0.02803792996911499,0.35555555555555557,0.02918571494985741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_microeconomics,0.5672268907563025,0.03218358107742613,"acc_norm,none",8030261248,0.0,{{answer}},0.46638655462184875,0.03240501447690071,0.5672268907563025,0.03218358107742613,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_physics,0.33112582781456956,0.038425817186598696,"acc_norm,none",8030261248,0.0,{{answer}},0.3509933774834437,0.03896981964257374,0.33112582781456956,0.038425817186598696,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_psychology,0.6385321100917432,0.02059808200993737,"acc_norm,none",8030261248,0.0,{{answer}},0.6752293577981652,0.02007772910931033,0.6385321100917432,0.02059808200993737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_statistics,0.4166666666666667,0.03362277436608043,"acc_norm,none",8030261248,0.0,{{answer}},0.32407407407407407,0.03191923445686186,0.4166666666666667,0.03362277436608043,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_us_history,0.5343137254901961,0.03501038327635897,"acc_norm,none",8030261248,0.0,{{answer}},0.44607843137254904,0.03488845451304974,0.5343137254901961,0.03501038327635897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_high_school_world_history,0.510548523206751,0.032539983791662855,"acc_norm,none",8030261248,0.0,{{answer}},0.4345991561181435,0.03226759995510145,0.510548523206751,0.032539983791662855,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_human_aging,0.5291479820627802,0.03350073248773404,"acc_norm,none",8030261248,0.0,{{answer}},0.5336322869955157,0.033481800170603065,0.5291479820627802,0.03350073248773404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_human_sexuality,0.5114503816793893,0.04384140024078016,"acc_norm,none",8030261248,0.0,{{answer}},0.5190839694656488,0.04382094705550988,0.5114503816793893,0.04384140024078016,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_international_law,0.512396694214876,0.045629515481807666,"acc_norm,none",8030261248,0.0,{{answer}},0.2809917355371901,0.04103203830514512,0.512396694214876,0.045629515481807666,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_jurisprudence,0.5092592592592593,0.04832853553437055,"acc_norm,none",8030261248,0.0,{{answer}},0.3888888888888889,0.0471282125742677,0.5092592592592593,0.04832853553437055,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_logical_fallacies,0.49079754601226994,0.03927705600787443,"acc_norm,none",8030261248,0.0,{{answer}},0.4785276073619632,0.03924746876751129,0.49079754601226994,0.03927705600787443,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_machine_learning,0.375,0.04595091388086298,"acc_norm,none",8030261248,0.0,{{answer}},0.4375,0.04708567521880525,0.375,0.04595091388086298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_management,0.6213592233009708,0.04802694698258974,"acc_norm,none",8030261248,0.0,{{answer}},0.5242718446601942,0.049449010929737795,0.6213592233009708,0.04802694698258974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_marketing,0.6581196581196581,0.031075028526507762,"acc_norm,none",8030261248,0.0,{{answer}},0.717948717948718,0.029480360549541187,0.6581196581196581,0.031075028526507762,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_medical_genetics,0.59,0.04943110704237101,"acc_norm,none",8030261248,0.0,{{answer}},0.53,0.050161355804659205,0.59,0.04943110704237101,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_miscellaneous,0.698595146871009,0.016409091097268787,"acc_norm,none",8030261248,0.0,{{answer}},0.70242656449553,0.01634911191290942,0.698595146871009,0.016409091097268787,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_moral_disputes,0.41329479768786126,0.026511261369409244,"acc_norm,none",8030261248,0.0,{{answer}},0.3583815028901734,0.025816756791584204,0.41329479768786126,0.026511261369409244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_moral_scenarios,0.27262569832402234,0.014893391735249603,"acc_norm,none",8030261248,0.0,{{answer}},0.23798882681564246,0.014242630070574885,0.27262569832402234,0.014893391735249603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_nutrition,0.4738562091503268,0.028590752958852394,"acc_norm,none",8030261248,0.0,{{answer}},0.38235294117647056,0.027826109307283697,0.4738562091503268,0.028590752958852394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_philosophy,0.4983922829581994,0.028397944907806612,"acc_norm,none",8030261248,0.0,{{answer}},0.4662379421221865,0.02833327710956279,0.4983922829581994,0.028397944907806612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_prehistory,0.5154320987654321,0.02780749004427619,"acc_norm,none",8030261248,0.0,{{answer}},0.5123456790123457,0.027812262269327242,0.5154320987654321,0.02780749004427619,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_professional_accounting,0.35815602836879434,0.02860208586275943,"acc_norm,none",8030261248,0.0,{{answer}},0.3723404255319149,0.028838921471251448,0.35815602836879434,0.02860208586275943,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_professional_law,0.30834419817470665,0.011794833789715322,"acc_norm,none",8030261248,0.0,{{answer}},0.2803129074315515,0.011471555944958616,0.30834419817470665,0.011794833789715322,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_professional_medicine,0.5073529411764706,0.030369552523902173,"acc_norm,none",8030261248,0.0,{{answer}},0.5330882352941176,0.03030625772246831,0.5073529411764706,0.030369552523902173,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_professional_psychology,0.49836601307189543,0.020227726838150117,"acc_norm,none",8030261248,0.0,{{answer}},0.4395424836601307,0.020079420408087918,0.49836601307189543,0.020227726838150117,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_public_relations,0.43636363636363634,0.04750185058907297,"acc_norm,none",8030261248,0.0,{{answer}},0.4727272727272727,0.04782001791380063,0.43636363636363634,0.04750185058907297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_security_studies,0.3020408163265306,0.029393609319879818,"acc_norm,none",8030261248,0.0,{{answer}},0.33877551020408164,0.030299506562154185,0.3020408163265306,0.029393609319879818,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_sociology,0.5174129353233831,0.03533389234739245,"acc_norm,none",8030261248,0.0,{{answer}},0.4228855721393035,0.034932317774212816,0.5174129353233831,0.03533389234739245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_us_foreign_policy,0.51,0.05024183937956912,"acc_norm,none",8030261248,0.0,{{answer}},0.46,0.05009082659620332,0.51,0.05024183937956912,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_virology,0.42168674698795183,0.03844453181770917,"acc_norm,none",8030261248,0.0,{{answer}},0.3433734939759036,0.03696584317010601,0.42168674698795183,0.03844453181770917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_continuation_world_religions,0.7719298245614035,0.03218093795602357,"acc_norm,none",8030261248,0.0,{{answer}},0.7309941520467836,0.034010526201040885,0.7719298245614035,0.03218093795602357,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_econometrics,0.5,0.047036043419179864,"acc,none",8030261248,5.0,answer,0.5,0.047036043419179864,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_electrical_engineering,0.6344827586206897,0.04013124195424385,"acc,none",8030261248,5.0,answer,0.6344827586206897,0.04013124195424385,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_elementary_mathematics,0.43386243386243384,0.025525034382474894,"acc,none",8030261248,5.0,answer,0.43386243386243384,0.025525034382474894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_formal_logic,0.47619047619047616,0.04467062628403273,"acc,none",8030261248,5.0,answer,0.47619047619047616,0.04467062628403273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_global_facts,0.32,0.04688261722621505,"acc,none",8030261248,5.0,answer,0.32,0.04688261722621505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_biology,0.7741935483870968,0.02378557788418101,"acc,none",8030261248,5.0,answer,0.7741935483870968,0.02378557788418101,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_chemistry,0.541871921182266,0.03505630140785741,"acc,none",8030261248,5.0,answer,0.541871921182266,0.03505630140785741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_computer_science,0.67,0.04725815626252607,"acc,none",8030261248,5.0,answer,0.67,0.04725815626252607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_european_history,0.7818181818181819,0.03225078108306289,"acc,none",8030261248,5.0,answer,0.7818181818181819,0.03225078108306289,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_geography,0.8131313131313131,0.02777253333421899,"acc,none",8030261248,5.0,answer,0.8131313131313131,0.02777253333421899,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_government_and_politics,0.8911917098445595,0.022473253332768756,"acc,none",8030261248,5.0,answer,0.8911917098445595,0.022473253332768756,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_macroeconomics,0.6435897435897436,0.0242831405294673,"acc,none",8030261248,5.0,answer,0.6435897435897436,0.0242831405294673,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_mathematics,0.4074074074074074,0.029958249250082118,"acc,none",8030261248,5.0,answer,0.4074074074074074,0.029958249250082118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_microeconomics,0.7394957983193278,0.028510251512341933,"acc,none",8030261248,5.0,answer,0.7394957983193278,0.028510251512341933,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_physics,0.4503311258278146,0.04062290018683775,"acc,none",8030261248,5.0,answer,0.4503311258278146,0.04062290018683775,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_psychology,0.8532110091743119,0.015173141845126267,"acc,none",8030261248,5.0,answer,0.8532110091743119,0.015173141845126267,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_statistics,0.5416666666666666,0.03398110890294636,"acc,none",8030261248,5.0,answer,0.5416666666666666,0.03398110890294636,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_us_history,0.8284313725490197,0.02646056956124065,"acc,none",8030261248,5.0,answer,0.8284313725490197,0.02646056956124065,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_high_school_world_history,0.8227848101265823,0.024856364184503234,"acc,none",8030261248,5.0,answer,0.8227848101265823,0.024856364184503234,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_human_aging,0.6905829596412556,0.03102441174057221,"acc,none",8030261248,5.0,answer,0.6905829596412556,0.03102441174057221,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_human_sexuality,0.7786259541984732,0.036412970813137296,"acc,none",8030261248,5.0,answer,0.7786259541984732,0.036412970813137296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_humanities,0.5997874601487779,0.006787659513650209,"acc,none",8030261248,,,0.5997874601487779,0.006787659513650209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_international_law,0.8181818181818182,0.03520893951097653,"acc,none",8030261248,5.0,answer,0.8181818181818182,0.03520893951097653,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_jurisprudence,0.7314814814814815,0.042844679680521934,"acc,none",8030261248,5.0,answer,0.7314814814814815,0.042844679680521934,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_logical_fallacies,0.7423312883435583,0.03436150827846917,"acc,none",8030261248,5.0,answer,0.7423312883435583,0.03436150827846917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_machine_learning,0.44642857142857145,0.04718471485219588,"acc,none",8030261248,5.0,answer,0.44642857142857145,0.04718471485219588,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_management,0.8543689320388349,0.03492606476623789,"acc,none",8030261248,5.0,answer,0.8543689320388349,0.03492606476623789,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_marketing,0.8632478632478633,0.0225090339370778,"acc,none",8030261248,5.0,answer,0.8632478632478633,0.0225090339370778,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_medical_genetics,0.84,0.0368452949177471,"acc,none",8030261248,5.0,answer,0.84,0.0368452949177471,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_miscellaneous,0.8109833971902938,0.014000791294406999,"acc,none",8030261248,5.0,answer,0.8109833971902938,0.014000791294406999,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_moral_disputes,0.7283236994219653,0.023948512905468348,"acc,none",8030261248,5.0,answer,0.7283236994219653,0.023948512905468348,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_moral_scenarios,0.4223463687150838,0.016519594275297114,"acc,none",8030261248,5.0,answer,0.4223463687150838,0.016519594275297114,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_nutrition,0.8071895424836601,0.022589318888176686,"acc,none",8030261248,5.0,answer,0.8071895424836601,0.022589318888176686,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_other,0.7215963952365626,0.007705274282282836,"acc,none",8030261248,,,0.7215963952365626,0.007705274282282836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_philosophy,0.729903536977492,0.025218040373410622,"acc,none",8030261248,5.0,answer,0.729903536977492,0.025218040373410622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_prehistory,0.7129629629629629,0.02517104191530968,"acc,none",8030261248,5.0,answer,0.7129629629629629,0.02517104191530968,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_professional_accounting,0.4929078014184397,0.02982449855912901,"acc,none",8030261248,5.0,answer,0.4929078014184397,0.02982449855912901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_professional_law,0.48435462842242505,0.012763982838120958,"acc,none",8030261248,5.0,answer,0.48435462842242505,0.012763982838120958,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_professional_medicine,0.6911764705882353,0.028064998167040094,"acc,none",8030261248,5.0,answer,0.6911764705882353,0.028064998167040094,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_professional_psychology,0.7222222222222222,0.018120224251484587,"acc,none",8030261248,5.0,answer,0.7222222222222222,0.018120224251484587,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_public_relations,0.7363636363636363,0.04220224692971987,"acc,none",8030261248,5.0,answer,0.7363636363636363,0.04220224692971987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_security_studies,0.726530612244898,0.028535560337128445,"acc,none",8030261248,5.0,answer,0.726530612244898,0.028535560337128445,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_social_sciences,0.7637309067273318,0.007481993756042584,"acc,none",8030261248,,,0.7637309067273318,0.007481993756042584,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_sociology,0.8805970149253731,0.02292879327721974,"acc,none",8030261248,5.0,answer,0.8805970149253731,0.02292879327721974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_stem,0.5521725340945132,0.00850210540398227,"acc,none",8030261248,,,0.5521725340945132,0.00850210540398227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_us_foreign_policy,0.88,0.03265986323710905,"acc,none",8030261248,5.0,answer,0.88,0.03265986323710905,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_virology,0.5662650602409639,0.03858158940685517,"acc,none",8030261248,5.0,answer,0.5662650602409639,0.03858158940685517,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,mmlu_world_religions,0.8128654970760234,0.029913127232368036,"acc,none",8030261248,5.0,answer,0.8128654970760234,0.029913127232368036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,other,0.54200193112327,0.008610989594701456,"acc,none",8030261248,,,0.54200193112327,0.008610989594701456,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,social sciences,0.49041273968150795,0.008849322702152353,"acc,none",8030261248,,,0.49041273968150795,0.008849322702152353,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,stem,0.44402156676181415,0.008553742667352173,"acc,none",8030261248,,,0.44402156676181415,0.008553742667352173,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,truthfulqa_gen,0.34149326805385555,0.016600688619950833,"rouge1_acc,none",8030261248,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,21.008720522711336,0.7336018872891364,0.3561811505507956,0.01676379072844632,-3.124722099823706,0.7213074992957418,44.93669655271626,0.8843149537731662,0.34149326805385555,0.016600688619950833,-5.462640969860286,0.8755065045791989,29.898162940041654,0.9679408518061355,0.28151774785801714,0.01574402724825605,-5.757788559988837,0.9913047201873912,42.14288698905598,0.883642650692877,0.3427172582619339,0.016614949385347015,-5.687997367703529,0.8829608433181637
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,truthfulqa_mc1,0.2839657282741738,0.01578537085839674,"acc,none",8030261248,0.0,0,0.2839657282741738,0.01578537085839674,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,truthfulqa_mc2,0.45216712850729296,0.014324862042820553,"acc,none",8030261248,0.0,0,0.45216712850729296,0.014324862042820553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
meta-llama/Meta-Llama-3.1-8B,15000000000000,openllm,winogrande,0.7774269928966061,0.011690933809712669,"acc,none",8030261248,5.0,,0.7774269928966061,0.011690933809712669,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,french_bench,french_bench_arc_challenge,0.5115483319076134,0.01462624061094917,"acc_norm,none",7241732096,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.4627887082976903,0.014589571001051863,0.5115483319076134,0.01462624061094917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,french_bench,french_bench_grammar,0.7815126050420168,0.038039971528894836,"acc,none",7241732096,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7815126050420168,0.038039971528894836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,french_bench,french_bench_hellaswag,0.6608481473548939,0.004899415320445792,"acc_norm,none",7241732096,5.0,{{label}},0.48704219318911973,0.005172737450145475,0.6608481473548939,0.004899415320445792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,french_bench,french_bench_vocab,0.7815126050420168,0.038039971528894836,"acc,none",7241732096,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.7815126050420168,0.038039971528894836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,arc_de,0.4798973481608212,0.014618314049208825,"acc_norm,none",7241732096,25.0,gold,0.4508126603934987,0.014559179118156086,0.4798973481608212,0.014618314049208825,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,arc_es,0.5213675213675214,0.014610524729617743,"acc_norm,none",7241732096,25.0,gold,0.4752136752136752,0.014605904746627946,0.5213675213675214,0.014610524729617743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,arc_fr,0.5106928999144568,0.01462679745105401,"acc_norm,none",7241732096,25.0,gold,0.4739093242087254,0.014610211661428528,0.5106928999144568,0.01462679745105401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,arc_it,0.48845166809238666,0.014626240610949168,"acc_norm,none",7241732096,25.0,gold,0.4525235243798118,0.014564040919200123,0.48845166809238666,0.014626240610949168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,m_mmlu_de,0.5220244380751244,0.0043383604809106545,"acc,none",7241732096,25.0,answer,0.5220244380751244,0.0043383604809106545,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,m_mmlu_es,0.24126293685315733,0.0037053310443897194,"acc,none",7241732096,25.0,answer,0.24126293685315733,0.0037053310443897194,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,m_mmlu_fr,0.5424337330990757,0.004354422058827953,"acc,none",7241732096,25.0,answer,0.5424337330990757,0.004354422058827953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,leaderboard,m_mmlu_it,0.48779935030596056,0.004344724991105597,"acc,none",7241732096,25.0,answer,0.48779935030596056,0.004344724991105597,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,arc_de,0.4798973481608212,0.014618314049208825,"acc_norm,none",7241732096,25.0,gold,0.4508126603934987,0.014559179118156086,0.4798973481608212,0.014618314049208825,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,arc_es,0.5213675213675214,0.014610524729617743,"acc_norm,none",7241732096,25.0,gold,0.4752136752136752,0.014605904746627946,0.5213675213675214,0.014610524729617743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,arc_fr,0.5106928999144568,0.01462679745105401,"acc_norm,none",7241732096,25.0,gold,0.4739093242087254,0.014610211661428528,0.5106928999144568,0.01462679745105401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,arc_it,0.48845166809238666,0.014626240610949168,"acc_norm,none",7241732096,25.0,gold,0.4525235243798118,0.014564040919200123,0.48845166809238666,0.014626240610949168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,m_mmlu_de,0.5220244380751244,0.0043383604809106545,"acc,none",7241732096,25.0,answer,0.5220244380751244,0.0043383604809106545,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,m_mmlu_es,0.24126293685315733,0.0037053310443897194,"acc,none",7241732096,25.0,answer,0.24126293685315733,0.0037053310443897194,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,m_mmlu_fr,0.5424337330990757,0.004354422058827953,"acc,none",7241732096,25.0,answer,0.5424337330990757,0.004354422058827953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,okapi,m_mmlu_it,0.48779935030596056,0.004344724991105597,"acc,none",7241732096,25.0,answer,0.48779935030596056,0.004344724991105597,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,arc_challenge,0.6015358361774744,0.014306946052735567,"acc_norm,none",7241732096,25.0,{{choices.label.index(answerKey)}},0.5622866894197952,0.014497573881108282,0.6015358361774744,0.014306946052735567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,gsm8k,0.38286580742987114,0.013389223491820465,"exact_match,flexible-extract",7241732096,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.379833206974981,0.013368818096960498,0.38286580742987114,0.013389223491820465,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,hellaswag,0.832105158334993,0.0037300899105376854,"acc_norm,none",7241732096,10.0,{{label}},0.6294562836088429,0.004819633668832527,0.832105158334993,0.0037300899105376854,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,humanities,0.357066950053135,0.006768857003008058,"acc,none",7241732096,,,0.357066950053135,0.006768857003008058,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu,0.6239851872952571,0.003840358304043326,"acc,none",7241732096,,,0.6239851872952571,0.003840358304043326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_abstract_algebra,0.26,0.044084400227680794,"acc,none",7241732096,5.0,answer,0.26,0.044084400227680794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_anatomy,0.6,0.04232073695151589,"acc,none",7241732096,5.0,answer,0.6,0.04232073695151589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_astronomy,0.6578947368421053,0.03860731599316091,"acc,none",7241732096,5.0,answer,0.6578947368421053,0.03860731599316091,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_business_ethics,0.57,0.04975698519562428,"acc,none",7241732096,5.0,answer,0.57,0.04975698519562428,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_clinical_knowledge,0.690566037735849,0.028450154794118634,"acc,none",7241732096,5.0,answer,0.690566037735849,0.028450154794118634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_biology,0.7222222222222222,0.037455547914624576,"acc,none",7241732096,5.0,answer,0.7222222222222222,0.037455547914624576,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_chemistry,0.49,0.05024183937956912,"acc,none",7241732096,5.0,answer,0.49,0.05024183937956912,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_computer_science,0.54,0.05009082659620332,"acc,none",7241732096,5.0,answer,0.54,0.05009082659620332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_mathematics,0.38,0.04878317312145633,"acc,none",7241732096,5.0,answer,0.38,0.04878317312145633,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_medicine,0.6242774566473989,0.03692820767264867,"acc,none",7241732096,5.0,answer,0.6242774566473989,0.03692820767264867,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_college_physics,0.3235294117647059,0.046550104113196177,"acc,none",7241732096,5.0,answer,0.3235294117647059,0.046550104113196177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_computer_security,0.78,0.041633319989322626,"acc,none",7241732096,5.0,answer,0.78,0.041633319989322626,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_conceptual_physics,0.5872340425531914,0.03218471141400351,"acc,none",7241732096,5.0,answer,0.5872340425531914,0.03218471141400351,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation,0.4346246973365617,0.004019493865828048,"acc,none",7241732096,,,0.4346246973365617,0.004019493865828048,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_abstract_algebra,0.24,0.04292346959909284,"acc_norm,none",7241732096,0.0,{{answer}},0.23,0.04229525846816506,0.24,0.04292346959909284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_anatomy,0.562962962962963,0.04284958639753401,"acc_norm,none",7241732096,0.0,{{answer}},0.5703703703703704,0.042763494943765995,0.562962962962963,0.04284958639753401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_astronomy,0.5328947368421053,0.040601270352363966,"acc_norm,none",7241732096,0.0,{{answer}},0.46710526315789475,0.040601270352363966,0.5328947368421053,0.040601270352363966,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_business_ethics,0.63,0.048523658709390974,"acc_norm,none",7241732096,0.0,{{answer}},0.67,0.04725815626252609,0.63,0.048523658709390974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_clinical_knowledge,0.539622641509434,0.030676096599389184,"acc_norm,none",7241732096,0.0,{{answer}},0.42641509433962266,0.03043779434298305,0.539622641509434,0.030676096599389184,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_biology,0.5138888888888888,0.04179596617581002,"acc_norm,none",7241732096,0.0,{{answer}},0.4930555555555556,0.04180806750294938,0.5138888888888888,0.04179596617581002,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_chemistry,0.36,0.04824181513244218,"acc_norm,none",7241732096,0.0,{{answer}},0.36,0.04824181513244218,0.36,0.04824181513244218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_computer_science,0.37,0.048523658709391,"acc_norm,none",7241732096,0.0,{{answer}},0.36,0.04824181513244218,0.37,0.048523658709391,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_mathematics,0.26,0.04408440022768077,"acc_norm,none",7241732096,0.0,{{answer}},0.2,0.040201512610368445,0.26,0.04408440022768077,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_medicine,0.41040462427745666,0.037507570448955356,"acc_norm,none",7241732096,0.0,{{answer}},0.3988439306358382,0.03733626655383509,0.41040462427745666,0.037507570448955356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_college_physics,0.4019607843137255,0.048786087144669976,"acc_norm,none",7241732096,0.0,{{answer}},0.35294117647058826,0.04755129616062949,0.4019607843137255,0.048786087144669976,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_computer_security,0.56,0.049888765156985884,"acc_norm,none",7241732096,0.0,{{answer}},0.49,0.05024183937956913,0.56,0.049888765156985884,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_conceptual_physics,0.5106382978723404,0.03267862331014063,"acc_norm,none",7241732096,0.0,{{answer}},0.574468085106383,0.03232146916224468,0.5106382978723404,0.03267862331014063,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_econometrics,0.3333333333333333,0.044346007015849245,"acc_norm,none",7241732096,0.0,{{answer}},0.3333333333333333,0.04434600701584925,0.3333333333333333,0.044346007015849245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_electrical_engineering,0.4068965517241379,0.04093793981266237,"acc_norm,none",7241732096,0.0,{{answer}},0.35172413793103446,0.03979236637497409,0.4068965517241379,0.04093793981266237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_elementary_mathematics,0.6111111111111112,0.025107425481137296,"acc_norm,none",7241732096,0.0,{{answer}},0.6111111111111112,0.025107425481137296,0.6111111111111112,0.025107425481137296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_formal_logic,0.38095238095238093,0.04343525428949098,"acc_norm,none",7241732096,0.0,{{answer}},0.40476190476190477,0.04390259265377563,0.38095238095238093,0.04343525428949098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_global_facts,0.58,0.049604496374885836,"acc_norm,none",7241732096,0.0,{{answer}},0.57,0.04975698519562428,0.58,0.049604496374885836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_biology,0.567741935483871,0.028181739720019416,"acc_norm,none",7241732096,0.0,{{answer}},0.4645161290322581,0.028372287797962956,0.567741935483871,0.028181739720019416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_chemistry,0.33497536945812806,0.033208527423483104,"acc_norm,none",7241732096,0.0,{{answer}},0.30049261083743845,0.03225799476233484,0.33497536945812806,0.033208527423483104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_computer_science,0.49,0.05024183937956911,"acc_norm,none",7241732096,0.0,{{answer}},0.42,0.049604496374885836,0.49,0.05024183937956911,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_european_history,0.5333333333333333,0.03895658065271846,"acc_norm,none",7241732096,0.0,{{answer}},0.36363636363636365,0.03756335775187896,0.5333333333333333,0.03895658065271846,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_geography,0.5555555555555556,0.035402943770953675,"acc_norm,none",7241732096,0.0,{{answer}},0.51010101010101,0.035616254886737454,0.5555555555555556,0.035402943770953675,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_government_and_politics,0.5751295336787565,0.035674713352125395,"acc_norm,none",7241732096,0.0,{{answer}},0.5544041450777202,0.03587014986075659,0.5751295336787565,0.035674713352125395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_macroeconomics,0.44871794871794873,0.025217315184846486,"acc_norm,none",7241732096,0.0,{{answer}},0.41794871794871796,0.025007329882461217,0.44871794871794873,0.025217315184846486,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_mathematics,0.32222222222222224,0.028493465091028597,"acc_norm,none",7241732096,0.0,{{answer}},0.24074074074074073,0.026067159222275794,0.32222222222222224,0.028493465091028597,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_microeconomics,0.5336134453781513,0.03240501447690071,"acc_norm,none",7241732096,0.0,{{answer}},0.453781512605042,0.032339434681820885,0.5336134453781513,0.03240501447690071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_physics,0.3576158940397351,0.03913453431177258,"acc_norm,none",7241732096,0.0,{{answer}},0.3509933774834437,0.03896981964257375,0.3576158940397351,0.03913453431177258,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_psychology,0.653211009174312,0.020406097104093027,"acc_norm,none",7241732096,0.0,{{answer}},0.6623853211009174,0.02027526598663891,0.653211009174312,0.020406097104093027,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_statistics,0.4074074074074074,0.033509916046960436,"acc_norm,none",7241732096,0.0,{{answer}},0.3611111111111111,0.03275773486100999,0.4074074074074074,0.033509916046960436,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_us_history,0.5196078431372549,0.03506612560524866,"acc_norm,none",7241732096,0.0,{{answer}},0.46078431372549017,0.03498501649369527,0.5196078431372549,0.03506612560524866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_high_school_world_history,0.4767932489451477,0.032512152011410174,"acc_norm,none",7241732096,0.0,{{answer}},0.43037974683544306,0.03223017195937599,0.4767932489451477,0.032512152011410174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_human_aging,0.5067264573991032,0.03355476596234354,"acc_norm,none",7241732096,0.0,{{answer}},0.5336322869955157,0.033481800170603065,0.5067264573991032,0.03355476596234354,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_human_sexuality,0.4580152671755725,0.04369802690578756,"acc_norm,none",7241732096,0.0,{{answer}},0.5190839694656488,0.04382094705550988,0.4580152671755725,0.04369802690578756,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_international_law,0.5289256198347108,0.04556710331269498,"acc_norm,none",7241732096,0.0,{{answer}},0.3140495867768595,0.04236964753041018,0.5289256198347108,0.04556710331269498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_jurisprudence,0.5277777777777778,0.04826217294139894,"acc_norm,none",7241732096,0.0,{{answer}},0.4351851851851852,0.04792898170907061,0.5277777777777778,0.04826217294139894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_logical_fallacies,0.4785276073619632,0.0392474687675113,"acc_norm,none",7241732096,0.0,{{answer}},0.4049079754601227,0.03856672163548913,0.4785276073619632,0.0392474687675113,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_machine_learning,0.35714285714285715,0.04547960999764376,"acc_norm,none",7241732096,0.0,{{answer}},0.33035714285714285,0.04464285714285713,0.35714285714285715,0.04547960999764376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_management,0.6116504854368932,0.04825729337356389,"acc_norm,none",7241732096,0.0,{{answer}},0.4854368932038835,0.049486373240266376,0.6116504854368932,0.04825729337356389,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_marketing,0.6623931623931624,0.03098029699261856,"acc_norm,none",7241732096,0.0,{{answer}},0.7136752136752137,0.029614323690456648,0.6623931623931624,0.03098029699261856,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_medical_genetics,0.57,0.04975698519562428,"acc_norm,none",7241732096,0.0,{{answer}},0.49,0.05024183937956912,0.57,0.04975698519562428,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_miscellaneous,0.7279693486590039,0.015913367447500503,"acc_norm,none",7241732096,0.0,{{answer}},0.7062579821200511,0.016287759388491658,0.7279693486590039,0.015913367447500503,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_moral_disputes,0.4190751445086705,0.026564178111422625,"acc_norm,none",7241732096,0.0,{{answer}},0.3815028901734104,0.0261521986197268,0.4190751445086705,0.026564178111422625,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_moral_scenarios,0.2737430167597765,0.014912413096372432,"acc_norm,none",7241732096,0.0,{{answer}},0.23798882681564246,0.014242630070574885,0.2737430167597765,0.014912413096372432,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_nutrition,0.4542483660130719,0.028509807802626567,"acc_norm,none",7241732096,0.0,{{answer}},0.37254901960784315,0.02768418188330289,0.4542483660130719,0.028509807802626567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_philosophy,0.5144694533762058,0.028386198084177687,"acc_norm,none",7241732096,0.0,{{answer}},0.48231511254019294,0.02838032284907713,0.5144694533762058,0.028386198084177687,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_prehistory,0.49074074074074076,0.027815973433878014,"acc_norm,none",7241732096,0.0,{{answer}},0.5493827160493827,0.0276847214156562,0.49074074074074076,0.027815973433878014,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_professional_accounting,0.32269503546099293,0.027889139300534792,"acc_norm,none",7241732096,0.0,{{answer}},0.3262411347517731,0.02796845304356316,0.32269503546099293,0.027889139300534792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_professional_law,0.3076923076923077,0.01178791025166459,"acc_norm,none",7241732096,0.0,{{answer}},0.27640156453715775,0.011422153194553567,0.3076923076923077,0.01178791025166459,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_professional_medicine,0.4889705882352941,0.030365446477275675,"acc_norm,none",7241732096,0.0,{{answer}},0.4852941176470588,0.03035969707904612,0.4889705882352941,0.030365446477275675,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_professional_psychology,0.46078431372549017,0.020165523313907904,"acc_norm,none",7241732096,0.0,{{answer}},0.4215686274509804,0.01997742260022747,0.46078431372549017,0.020165523313907904,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_public_relations,0.4,0.0469237132203465,"acc_norm,none",7241732096,0.0,{{answer}},0.509090909090909,0.04788339768702861,0.4,0.0469237132203465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_security_studies,0.3306122448979592,0.030116426296540596,"acc_norm,none",7241732096,0.0,{{answer}},0.3306122448979592,0.030116426296540596,0.3306122448979592,0.030116426296540596,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_sociology,0.46766169154228854,0.03528131472933607,"acc_norm,none",7241732096,0.0,{{answer}},0.4079601990049751,0.034751163651940926,0.46766169154228854,0.03528131472933607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_us_foreign_policy,0.51,0.05024183937956911,"acc_norm,none",7241732096,0.0,{{answer}},0.45,0.05,0.51,0.05024183937956911,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_virology,0.42771084337349397,0.03851597683718533,"acc_norm,none",7241732096,0.0,{{answer}},0.3433734939759036,0.03696584317010601,0.42771084337349397,0.03851597683718533,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_continuation_world_religions,0.7543859649122807,0.0330140594698725,"acc_norm,none",7241732096,0.0,{{answer}},0.7309941520467836,0.03401052620104089,0.7543859649122807,0.0330140594698725,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_econometrics,0.49122807017543857,0.04702880432049615,"acc,none",7241732096,5.0,answer,0.49122807017543857,0.04702880432049615,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_electrical_engineering,0.5655172413793104,0.04130740879555498,"acc,none",7241732096,5.0,answer,0.5655172413793104,0.04130740879555498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_elementary_mathematics,0.3862433862433862,0.025075981767601688,"acc,none",7241732096,5.0,answer,0.3862433862433862,0.025075981767601688,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_formal_logic,0.3968253968253968,0.04375888492727062,"acc,none",7241732096,5.0,answer,0.3968253968253968,0.04375888492727062,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_global_facts,0.33,0.047258156262526045,"acc,none",7241732096,5.0,answer,0.33,0.047258156262526045,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_biology,0.7709677419354839,0.023904914311782648,"acc,none",7241732096,5.0,answer,0.7709677419354839,0.023904914311782648,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_chemistry,0.49261083743842365,0.035176035403610084,"acc,none",7241732096,5.0,answer,0.49261083743842365,0.035176035403610084,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_computer_science,0.68,0.04688261722621505,"acc,none",7241732096,5.0,answer,0.68,0.04688261722621505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_european_history,0.7696969696969697,0.0328766675860349,"acc,none",7241732096,5.0,answer,0.7696969696969697,0.0328766675860349,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_geography,0.7727272727272727,0.0298575156733864,"acc,none",7241732096,5.0,answer,0.7727272727272727,0.0298575156733864,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_government_and_politics,0.8601036269430051,0.025033870583015174,"acc,none",7241732096,5.0,answer,0.8601036269430051,0.025033870583015174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_macroeconomics,0.6512820512820513,0.02416278028401772,"acc,none",7241732096,5.0,answer,0.6512820512820513,0.02416278028401772,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_mathematics,0.37407407407407406,0.02950286112895529,"acc,none",7241732096,5.0,answer,0.37407407407407406,0.02950286112895529,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_microeconomics,0.6596638655462185,0.030778057422931673,"acc,none",7241732096,5.0,answer,0.6596638655462185,0.030778057422931673,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_physics,0.2980132450331126,0.03734535676787198,"acc,none",7241732096,5.0,answer,0.2980132450331126,0.03734535676787198,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_psychology,0.8238532110091743,0.016332882393431412,"acc,none",7241732096,5.0,answer,0.8238532110091743,0.016332882393431412,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_statistics,0.5416666666666666,0.033981108902946366,"acc,none",7241732096,5.0,answer,0.5416666666666666,0.033981108902946366,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_us_history,0.803921568627451,0.02786594228663933,"acc,none",7241732096,5.0,answer,0.803921568627451,0.02786594228663933,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_high_school_world_history,0.7637130801687764,0.027652153144159274,"acc,none",7241732096,5.0,answer,0.7637130801687764,0.027652153144159274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_human_aging,0.6905829596412556,0.03102441174057222,"acc,none",7241732096,5.0,answer,0.6905829596412556,0.03102441174057222,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_human_sexuality,0.8015267175572519,0.03498149385462472,"acc,none",7241732096,5.0,answer,0.8015267175572519,0.03498149385462472,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_humanities,0.5674814027630181,0.006756112143898837,"acc,none",7241732096,,,0.5674814027630181,0.006756112143898837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_international_law,0.7851239669421488,0.03749492448709699,"acc,none",7241732096,5.0,answer,0.7851239669421488,0.03749492448709699,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_jurisprudence,0.7407407407407407,0.04236511258094632,"acc,none",7241732096,5.0,answer,0.7407407407407407,0.04236511258094632,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_logical_fallacies,0.7668711656441718,0.0332201579577674,"acc,none",7241732096,5.0,answer,0.7668711656441718,0.0332201579577674,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_machine_learning,0.5,0.04745789978762494,"acc,none",7241732096,5.0,answer,0.5,0.04745789978762494,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_management,0.8155339805825242,0.03840423627288276,"acc,none",7241732096,5.0,answer,0.8155339805825242,0.03840423627288276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_marketing,0.8760683760683761,0.021586494001281382,"acc,none",7241732096,5.0,answer,0.8760683760683761,0.021586494001281382,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_medical_genetics,0.73,0.0446196043338474,"acc,none",7241732096,5.0,answer,0.73,0.0446196043338474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_miscellaneous,0.8160919540229885,0.013853724170922531,"acc,none",7241732096,5.0,answer,0.8160919540229885,0.013853724170922531,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_moral_disputes,0.7109826589595376,0.02440517393578323,"acc,none",7241732096,5.0,answer,0.7109826589595376,0.02440517393578323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_moral_scenarios,0.33631284916201115,0.015801003729145897,"acc,none",7241732096,5.0,answer,0.33631284916201115,0.015801003729145897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_nutrition,0.7581699346405228,0.024518195641879334,"acc,none",7241732096,5.0,answer,0.7581699346405228,0.024518195641879334,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_other,0.7019633086578694,0.007890986670550425,"acc,none",7241732096,,,0.7019633086578694,0.007890986670550425,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_philosophy,0.7138263665594855,0.025670259242188957,"acc,none",7241732096,5.0,answer,0.7138263665594855,0.025670259242188957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_prehistory,0.7253086419753086,0.024836057868294677,"acc,none",7241732096,5.0,answer,0.7253086419753086,0.024836057868294677,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_professional_accounting,0.5106382978723404,0.02982074719142244,"acc,none",7241732096,5.0,answer,0.5106382978723404,0.02982074719142244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_professional_law,0.45697522816166886,0.012722869501611419,"acc,none",7241732096,5.0,answer,0.45697522816166886,0.012722869501611419,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_professional_medicine,0.6580882352941176,0.028814722422254177,"acc,none",7241732096,5.0,answer,0.6580882352941176,0.028814722422254177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_professional_psychology,0.6781045751633987,0.018901015322093095,"acc,none",7241732096,5.0,answer,0.6781045751633987,0.018901015322093095,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_public_relations,0.6454545454545455,0.04582004841505417,"acc,none",7241732096,5.0,answer,0.6454545454545455,0.04582004841505417,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_security_studies,0.7224489795918367,0.028666857790274648,"acc,none",7241732096,5.0,answer,0.7224489795918367,0.028666857790274648,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_social_sciences,0.7331816704582386,0.007818618394580918,"acc,none",7241732096,,,0.7331816704582386,0.007818618394580918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_sociology,0.8258706467661692,0.026814951200421603,"acc,none",7241732096,5.0,answer,0.8258706467661692,0.026814951200421603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_stem,0.5248969235648588,0.008497277469390123,"acc,none",7241732096,,,0.5248969235648588,0.008497277469390123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_us_foreign_policy,0.87,0.033799766898963086,"acc,none",7241732096,5.0,answer,0.87,0.033799766898963086,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_virology,0.5421686746987951,0.038786267710023595,"acc,none",7241732096,5.0,answer,0.5421686746987951,0.038786267710023595,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,mmlu_world_religions,0.8362573099415205,0.028380919596145866,"acc,none",7241732096,5.0,answer,0.8362573099415205,0.028380919596145866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,other,0.5275185065980045,0.008583586441129502,"acc,none",7241732096,,,0.5275185065980045,0.008583586441129502,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,social sciences,0.47708807279818005,0.00882914630032474,"acc,none",7241732096,,,0.47708807279818005,0.00882914630032474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,stem,0.4173802727561053,0.008523106433407465,"acc,none",7241732096,,,0.4173802727561053,0.008523106433407465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,truthfulqa_gen,0.397796817625459,0.017133934248559652,"rouge1_acc,none",7241732096,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,30.6279834548563,0.8547319050508423,0.412484700122399,0.017233299399571203,-1.5509407922441087,1.0402223690206764,55.94312022053465,0.9274292142352155,0.397796817625459,0.017133934248559652,-1.8253353029993267,1.257419367486379,41.155472362892276,1.0988337199287672,0.3488372093023256,0.0166844198599869,-2.680860077292509,1.4170725702460802,53.26292829659245,0.94472775094152,0.397796817625459,0.017133934248559655,-2.2005251181239904,1.2743301643103866
mistralai/Mistral-7B-v0.1,7000000000000,openllm,truthfulqa_mc1,0.28151774785801714,0.01574402724825605,"acc,none",7241732096,0.0,0,0.28151774785801714,0.01574402724825605,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,truthfulqa_mc2,0.4261059629785501,0.014205676389190985,"acc,none",7241732096,0.0,0,0.4261059629785501,0.014205676389190985,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mistralai/Mistral-7B-v0.1,7000000000000,openllm,winogrande,0.7790055248618785,0.011661223637643417,"acc,none",7241732096,5.0,,0.7790055248618785,0.011661223637643417,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,french_bench,french_bench_arc_challenge,0.40290846877673225,0.014351663146567203,"acc_norm,none",6921720704,5.0,"{{['A', 'B', 'C', 'D'].index(answerKey)}}",0.3473053892215569,0.013931226499492359,0.40290846877673225,0.014351663146567203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,french_bench,french_bench_grammar,0.8235294117647058,0.0350941493654996,"acc,none",6921720704,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.8235294117647058,0.0350941493654996,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,french_bench,french_bench_hellaswag,0.6530306275433712,0.00492616173150174,"acc_norm,none",6921720704,5.0,{{label}},0.48650674662668664,0.005172590825294664,0.6530306275433712,0.00492616173150174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,french_bench,french_bench_vocab,0.8319327731092437,0.03442267607655234,"acc,none",6921720704,5.0,"{{[""answerA"", ""answerB"", ""answerC"", ""answerD""].index(""answer"" + answer)}}",0.8319327731092437,0.03442267607655234,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,arc_de,0.31394354148845166,0.013579515768185788,"acc_norm,none",6921720704,25.0,gold,0.2805816937553465,0.013146162224654298,0.31394354148845166,0.013579515768185788,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,arc_es,0.3803418803418803,0.014198938935522067,"acc_norm,none",6921720704,25.0,gold,0.335042735042735,0.013805105015816038,0.3803418803418803,0.014198938935522067,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,arc_fr,0.40889649272882805,0.01438523759817326,"acc_norm,none",6921720704,25.0,gold,0.3627031650983747,0.014067765882432828,0.40889649272882805,0.01438523759817326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,arc_it,0.38922155688622756,0.014266547006095372,"acc_norm,none",6921720704,25.0,gold,0.3481608212147134,0.013939229153926038,0.38922155688622756,0.014266547006095372,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,m_mmlu_de,0.24732237139840096,0.003747257839320172,"acc,none",6921720704,25.0,answer,0.24732237139840096,0.003747257839320172,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,m_mmlu_es,0.2521373931303435,0.0037606732343109465,"acc,none",6921720704,25.0,answer,0.2521373931303435,0.0037606732343109465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,m_mmlu_fr,0.2501718738064319,0.0037855612802412102,"acc,none",6921720704,25.0,answer,0.2501718738064319,0.0037855612802412102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,leaderboard,m_mmlu_it,0.24922565535997582,0.0037598689325997773,"acc,none",6921720704,25.0,answer,0.24922565535997582,0.0037598689325997773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,arc_de,0.31394354148845166,0.013579515768185788,"acc_norm,none",6921720704,25.0,gold,0.2805816937553465,0.013146162224654298,0.31394354148845166,0.013579515768185788,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,arc_es,0.3803418803418803,0.014198938935522067,"acc_norm,none",6921720704,25.0,gold,0.335042735042735,0.013805105015816038,0.3803418803418803,0.014198938935522067,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,arc_fr,0.40889649272882805,0.01438523759817326,"acc_norm,none",6921720704,25.0,gold,0.3627031650983747,0.014067765882432828,0.40889649272882805,0.01438523759817326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,arc_it,0.38922155688622756,0.014266547006095372,"acc_norm,none",6921720704,25.0,gold,0.3481608212147134,0.013939229153926038,0.38922155688622756,0.014266547006095372,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,m_mmlu_de,0.24732237139840096,0.003747257839320172,"acc,none",6921720704,25.0,answer,0.24732237139840096,0.003747257839320172,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,m_mmlu_es,0.2521373931303435,0.0037606732343109465,"acc,none",6921720704,25.0,answer,0.2521373931303435,0.0037606732343109465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,m_mmlu_fr,0.2501718738064319,0.0037855612802412102,"acc,none",6921720704,25.0,answer,0.2501718738064319,0.0037855612802412102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,okapi,m_mmlu_it,0.24922565535997582,0.0037598689325997773,"acc,none",6921720704,25.0,answer,0.24922565535997582,0.0037598689325997773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,arc_challenge,0.4658703071672355,0.014577311315231097,"acc_norm,none",6921720704,25.0,{{choices.label.index(answerKey)}},0.431740614334471,0.014474591427196202,0.4658703071672355,0.014577311315231097,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,gsm8k,0.043214556482183475,0.005600987515237861,"exact_match,flexible-extract",6921720704,5.0,{{answer}},,,,,,,,,,,,,,,,,,,,,,,0.037149355572403335,0.005209516283073793,0.043214556482183475,0.005600987515237861,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,hellaswag,0.7811192989444333,0.004126424809818356,"acc_norm,none",6921720704,10.0,{{label}},0.5843457478589922,0.004918272352137557,0.7811192989444333,0.004126424809818356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,humanities,0.32157279489904356,0.006690785751445344,"acc,none",6921720704,,,0.32157279489904356,0.006690785751445344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu,0.2723258795043441,0.0037419871393619217,"acc,none",6921720704,,,0.2723258795043441,0.0037419871393619217,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_abstract_algebra,0.25,0.04351941398892446,"acc,none",6921720704,5.0,answer,0.25,0.04351941398892446,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_anatomy,0.21481481481481482,0.035478541985608236,"acc,none",6921720704,5.0,answer,0.21481481481481482,0.035478541985608236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_astronomy,0.25,0.03523807393012047,"acc,none",6921720704,5.0,answer,0.25,0.03523807393012047,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_business_ethics,0.21,0.04093601807403326,"acc,none",6921720704,5.0,answer,0.21,0.04093601807403326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_clinical_knowledge,0.30566037735849055,0.028353298073322666,"acc,none",6921720704,5.0,answer,0.30566037735849055,0.028353298073322666,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_biology,0.22916666666666666,0.035146974678623884,"acc,none",6921720704,5.0,answer,0.22916666666666666,0.035146974678623884,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_chemistry,0.22,0.041633319989322695,"acc,none",6921720704,5.0,answer,0.22,0.041633319989322695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_computer_science,0.28,0.04512608598542127,"acc,none",6921720704,5.0,answer,0.28,0.04512608598542127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_mathematics,0.28,0.04512608598542126,"acc,none",6921720704,5.0,answer,0.28,0.04512608598542126,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_medicine,0.24277456647398843,0.0326926380614177,"acc,none",6921720704,5.0,answer,0.24277456647398843,0.0326926380614177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_college_physics,0.22549019607843138,0.04158307533083286,"acc,none",6921720704,5.0,answer,0.22549019607843138,0.04158307533083286,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_computer_security,0.26,0.044084400227680794,"acc,none",6921720704,5.0,answer,0.26,0.044084400227680794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_conceptual_physics,0.28936170212765955,0.029644006577009618,"acc,none",6921720704,5.0,answer,0.28936170212765955,0.029644006577009618,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation,0.37772397094430993,0.00397353272197644,"acc,none",6921720704,,,0.37772397094430993,0.00397353272197644,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_abstract_algebra,0.17,0.03775251680686371,"acc_norm,none",6921720704,0.0,{{answer}},0.16,0.03684529491774708,0.17,0.03775251680686371,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_anatomy,0.45925925925925926,0.04304979692464243,"acc_norm,none",6921720704,0.0,{{answer}},0.4666666666666667,0.043097329010363554,0.45925925925925926,0.04304979692464243,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_astronomy,0.4934210526315789,0.04068590050224971,"acc_norm,none",6921720704,0.0,{{answer}},0.4144736842105263,0.04008973785779206,0.4934210526315789,0.04068590050224971,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_business_ethics,0.6,0.049236596391733084,"acc_norm,none",6921720704,0.0,{{answer}},0.59,0.04943110704237102,0.6,0.049236596391733084,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_clinical_knowledge,0.5056603773584906,0.030770900763851302,"acc_norm,none",6921720704,0.0,{{answer}},0.39245283018867927,0.030052580579557845,0.5056603773584906,0.030770900763851302,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_biology,0.4236111111111111,0.04132125019723368,"acc_norm,none",6921720704,0.0,{{answer}},0.4166666666666667,0.04122728707651282,0.4236111111111111,0.04132125019723368,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_chemistry,0.29,0.045604802157206845,"acc_norm,none",6921720704,0.0,{{answer}},0.26,0.04408440022768077,0.29,0.045604802157206845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_computer_science,0.3,0.046056618647183814,"acc_norm,none",6921720704,0.0,{{answer}},0.27,0.044619604333847415,0.3,0.046056618647183814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_mathematics,0.17,0.0377525168068637,"acc_norm,none",6921720704,0.0,{{answer}},0.14,0.03487350880197772,0.17,0.0377525168068637,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_medicine,0.3872832369942196,0.03714325906302065,"acc_norm,none",6921720704,0.0,{{answer}},0.3468208092485549,0.036291466701596636,0.3872832369942196,0.03714325906302065,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_college_physics,0.2647058823529412,0.0438986995680878,"acc_norm,none",6921720704,0.0,{{answer}},0.20588235294117646,0.04023382273617747,0.2647058823529412,0.0438986995680878,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_computer_security,0.51,0.05024183937956912,"acc_norm,none",6921720704,0.0,{{answer}},0.47,0.050161355804659205,0.51,0.05024183937956912,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_conceptual_physics,0.42127659574468085,0.03227834510146267,"acc_norm,none",6921720704,0.0,{{answer}},0.4425531914893617,0.032469569197899575,0.42127659574468085,0.03227834510146267,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_econometrics,0.24561403508771928,0.04049339297748141,"acc_norm,none",6921720704,0.0,{{answer}},0.2543859649122807,0.040969851398436695,0.24561403508771928,0.04049339297748141,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_electrical_engineering,0.3586206896551724,0.039966295748767186,"acc_norm,none",6921720704,0.0,{{answer}},0.32413793103448274,0.03900432069185555,0.3586206896551724,0.039966295748767186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_elementary_mathematics,0.4126984126984127,0.025355741263055284,"acc_norm,none",6921720704,0.0,{{answer}},0.3915343915343915,0.025138091388851095,0.4126984126984127,0.025355741263055284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_formal_logic,0.3253968253968254,0.041905964388711366,"acc_norm,none",6921720704,0.0,{{answer}},0.30952380952380953,0.04134913018303316,0.3253968253968254,0.041905964388711366,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_global_facts,0.53,0.050161355804659205,"acc_norm,none",6921720704,0.0,{{answer}},0.51,0.05024183937956912,0.53,0.050161355804659205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_biology,0.45483870967741935,0.028327743091561074,"acc_norm,none",6921720704,0.0,{{answer}},0.3870967741935484,0.02770935967503249,0.45483870967741935,0.028327743091561074,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_chemistry,0.270935960591133,0.031270907132976984,"acc_norm,none",6921720704,0.0,{{answer}},0.28078817733990147,0.0316185633535861,0.270935960591133,0.031270907132976984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_computer_science,0.44,0.04988876515698589,"acc_norm,none",6921720704,0.0,{{answer}},0.38,0.04878317312145633,0.44,0.04988876515698589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_european_history,0.49696969696969695,0.03904272341431855,"acc_norm,none",6921720704,0.0,{{answer}},0.3212121212121212,0.03646204963253811,0.49696969696969695,0.03904272341431855,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_geography,0.4797979797979798,0.03559443565563918,"acc_norm,none",6921720704,0.0,{{answer}},0.4797979797979798,0.035594435655639196,0.4797979797979798,0.03559443565563918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_government_and_politics,0.5544041450777202,0.03587014986075659,"acc_norm,none",6921720704,0.0,{{answer}},0.48704663212435234,0.03607228061047749,0.5544041450777202,0.03587014986075659,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_macroeconomics,0.38974358974358975,0.024726967886647074,"acc_norm,none",6921720704,0.0,{{answer}},0.3487179487179487,0.024162780284017724,0.38974358974358975,0.024726967886647074,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_mathematics,0.21481481481481482,0.025040443877000683,"acc_norm,none",6921720704,0.0,{{answer}},0.1962962962962963,0.02421742132741715,0.21481481481481482,0.025040443877000683,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_microeconomics,0.47058823529411764,0.03242225027115007,"acc_norm,none",6921720704,0.0,{{answer}},0.4117647058823529,0.031968769891957786,0.47058823529411764,0.03242225027115007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_physics,0.2582781456953642,0.035737053147634576,"acc_norm,none",6921720704,0.0,{{answer}},0.2582781456953642,0.035737053147634576,0.2582781456953642,0.035737053147634576,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_psychology,0.5486238532110091,0.021335714711268782,"acc_norm,none",6921720704,0.0,{{answer}},0.5798165137614679,0.021162420048273504,0.5486238532110091,0.021335714711268782,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_statistics,0.3101851851851852,0.03154696285656628,"acc_norm,none",6921720704,0.0,{{answer}},0.2916666666666667,0.030998666304560534,0.3101851851851852,0.03154696285656628,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_us_history,0.44607843137254904,0.03488845451304974,"acc_norm,none",6921720704,0.0,{{answer}},0.43137254901960786,0.03476099060501636,0.44607843137254904,0.03488845451304974,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_high_school_world_history,0.39662447257383965,0.03184399873811224,"acc_norm,none",6921720704,0.0,{{answer}},0.38396624472573837,0.031658678064106674,0.39662447257383965,0.03184399873811224,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_human_aging,0.47085201793721976,0.03350073248773404,"acc_norm,none",6921720704,0.0,{{answer}},0.47533632286995514,0.03351695167652628,0.47085201793721976,0.03350073248773404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_human_sexuality,0.44274809160305345,0.04356447202665069,"acc_norm,none",6921720704,0.0,{{answer}},0.4732824427480916,0.04379024936553894,0.44274809160305345,0.04356447202665069,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_international_law,0.4132231404958678,0.04495087843548408,"acc_norm,none",6921720704,0.0,{{answer}},0.2644628099173554,0.04026187527591206,0.4132231404958678,0.04495087843548408,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_jurisprudence,0.4074074074074074,0.047500773411999854,"acc_norm,none",6921720704,0.0,{{answer}},0.3148148148148148,0.04489931073591312,0.4074074074074074,0.047500773411999854,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_logical_fallacies,0.43558282208588955,0.038956324641389366,"acc_norm,none",6921720704,0.0,{{answer}},0.3558282208588957,0.03761521380046734,0.43558282208588955,0.038956324641389366,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_machine_learning,0.25892857142857145,0.04157751539865629,"acc_norm,none",6921720704,0.0,{{answer}},0.29464285714285715,0.04327040932578728,0.25892857142857145,0.04157751539865629,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_management,0.5922330097087378,0.048657775704107675,"acc_norm,none",6921720704,0.0,{{answer}},0.49514563106796117,0.04950504382128919,0.5922330097087378,0.048657775704107675,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_marketing,0.6367521367521367,0.03150712523091264,"acc_norm,none",6921720704,0.0,{{answer}},0.6410256410256411,0.03142616993791925,0.6367521367521367,0.03150712523091264,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_medical_genetics,0.52,0.050211673156867795,"acc_norm,none",6921720704,0.0,{{answer}},0.43,0.049756985195624284,0.52,0.050211673156867795,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_miscellaneous,0.6181353767560664,0.01737373273667759,"acc_norm,none",6921720704,0.0,{{answer}},0.6296296296296297,0.017268607560005776,0.6181353767560664,0.01737373273667759,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_moral_disputes,0.37572254335260113,0.02607431485165708,"acc_norm,none",6921720704,0.0,{{answer}},0.3352601156069364,0.025416003773165555,0.37572254335260113,0.02607431485165708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_moral_scenarios,0.2759776536312849,0.014950103002475353,"acc_norm,none",6921720704,0.0,{{answer}},0.23798882681564246,0.014242630070574885,0.2759776536312849,0.014950103002475353,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_nutrition,0.4411764705882353,0.02843109544417664,"acc_norm,none",6921720704,0.0,{{answer}},0.3431372549019608,0.027184498909941613,0.4411764705882353,0.02843109544417664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_philosophy,0.40836012861736337,0.027917050748484634,"acc_norm,none",6921720704,0.0,{{answer}},0.3954983922829582,0.027770918531427838,0.40836012861736337,0.027917050748484634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_prehistory,0.43209876543209874,0.02756301097160668,"acc_norm,none",6921720704,0.0,{{answer}},0.4783950617283951,0.027794760105008736,0.43209876543209874,0.02756301097160668,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_professional_accounting,0.2765957446808511,0.026684564340460997,"acc_norm,none",6921720704,0.0,{{answer}},0.2907801418439716,0.027090664368353178,0.2765957446808511,0.026684564340460997,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_professional_law,0.2966101694915254,0.011665946586082832,"acc_norm,none",6921720704,0.0,{{answer}},0.26597131681877445,0.011285033165551277,0.2966101694915254,0.011665946586082832,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_professional_medicine,0.40441176470588236,0.029812630701569743,"acc_norm,none",6921720704,0.0,{{answer}},0.36764705882352944,0.029289413409403196,0.40441176470588236,0.029812630701569743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_professional_psychology,0.4117647058823529,0.019910377463105935,"acc_norm,none",6921720704,0.0,{{answer}},0.36764705882352944,0.019506291693954843,0.4117647058823529,0.019910377463105935,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_public_relations,0.37272727272727274,0.04631381319425463,"acc_norm,none",6921720704,0.0,{{answer}},0.45454545454545453,0.04769300568972744,0.37272727272727274,0.04631381319425463,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_security_studies,0.2938775510204082,0.029162738410249765,"acc_norm,none",6921720704,0.0,{{answer}},0.3224489795918367,0.02992310056368391,0.2938775510204082,0.029162738410249765,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_sociology,0.43283582089552236,0.03503490923673281,"acc_norm,none",6921720704,0.0,{{answer}},0.34328358208955223,0.03357379665433432,0.43283582089552236,0.03503490923673281,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_us_foreign_policy,0.46,0.05009082659620332,"acc_norm,none",6921720704,0.0,{{answer}},0.44,0.049888765156985884,0.46,0.05009082659620332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_virology,0.4036144578313253,0.03819486140758398,"acc_norm,none",6921720704,0.0,{{answer}},0.3072289156626506,0.03591566797824662,0.4036144578313253,0.03819486140758398,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_continuation_world_religions,0.6374269005847953,0.03687130615562059,"acc_norm,none",6921720704,0.0,{{answer}},0.6023391812865497,0.03753638955761691,0.6374269005847953,0.03687130615562059,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_econometrics,0.23684210526315788,0.03999423879281338,"acc,none",6921720704,5.0,answer,0.23684210526315788,0.03999423879281338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_electrical_engineering,0.2896551724137931,0.03780019230438014,"acc,none",6921720704,5.0,answer,0.2896551724137931,0.03780019230438014,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_elementary_mathematics,0.24338624338624337,0.02210112878741543,"acc,none",6921720704,5.0,answer,0.24338624338624337,0.02210112878741543,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_formal_logic,0.23809523809523808,0.0380952380952381,"acc,none",6921720704,5.0,answer,0.23809523809523808,0.0380952380952381,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_global_facts,0.31,0.04648231987117316,"acc,none",6921720704,5.0,answer,0.31,0.04648231987117316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_biology,0.25161290322580643,0.024685979286239963,"acc,none",6921720704,5.0,answer,0.25161290322580643,0.024685979286239963,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_chemistry,0.2561576354679803,0.030712730070982592,"acc,none",6921720704,5.0,answer,0.2561576354679803,0.030712730070982592,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_computer_science,0.31,0.04648231987117316,"acc,none",6921720704,5.0,answer,0.31,0.04648231987117316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_european_history,0.24848484848484848,0.03374402644139404,"acc,none",6921720704,5.0,answer,0.24848484848484848,0.03374402644139404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_geography,0.17676767676767677,0.027178752639044915,"acc,none",6921720704,5.0,answer,0.17676767676767677,0.027178752639044915,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_government_and_politics,0.2538860103626943,0.03141024780565318,"acc,none",6921720704,5.0,answer,0.2538860103626943,0.03141024780565318,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_macroeconomics,0.24102564102564103,0.021685546665333174,"acc,none",6921720704,5.0,answer,0.24102564102564103,0.021685546665333174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_mathematics,0.24814814814814815,0.0263357394040558,"acc,none",6921720704,5.0,answer,0.24814814814814815,0.0263357394040558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_microeconomics,0.2689075630252101,0.02880139219363127,"acc,none",6921720704,5.0,answer,0.2689075630252101,0.02880139219363127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_physics,0.304635761589404,0.03757949922943343,"acc,none",6921720704,5.0,answer,0.304635761589404,0.03757949922943343,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_psychology,0.23669724770642203,0.018224078117299067,"acc,none",6921720704,5.0,answer,0.23669724770642203,0.018224078117299067,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_statistics,0.13425925925925927,0.023251277590545894,"acc,none",6921720704,5.0,answer,0.13425925925925927,0.023251277590545894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_us_history,0.31862745098039214,0.0327028718148208,"acc,none",6921720704,5.0,answer,0.31862745098039214,0.0327028718148208,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_high_school_world_history,0.28270042194092826,0.029312814153955914,"acc,none",6921720704,5.0,answer,0.28270042194092826,0.029312814153955914,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_human_aging,0.4618834080717489,0.033460150119732274,"acc,none",6921720704,5.0,answer,0.4618834080717489,0.033460150119732274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_human_sexuality,0.2900763358778626,0.039800662464677665,"acc,none",6921720704,5.0,answer,0.2900763358778626,0.039800662464677665,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_humanities,0.2688629117959617,0.006451202014659573,"acc,none",6921720704,,,0.2688629117959617,0.006451202014659573,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_international_law,0.23140495867768596,0.038498560987940904,"acc,none",6921720704,5.0,answer,0.23140495867768596,0.038498560987940904,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_jurisprudence,0.32407407407407407,0.045245960070300476,"acc,none",6921720704,5.0,answer,0.32407407407407407,0.045245960070300476,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_logical_fallacies,0.25153374233128833,0.03408997886857529,"acc,none",6921720704,5.0,answer,0.25153374233128833,0.03408997886857529,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_machine_learning,0.35714285714285715,0.04547960999764376,"acc,none",6921720704,5.0,answer,0.35714285714285715,0.04547960999764376,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_management,0.2524271844660194,0.04301250399690878,"acc,none",6921720704,5.0,answer,0.2524271844660194,0.04301250399690878,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_marketing,0.3162393162393162,0.030463656747340254,"acc,none",6921720704,5.0,answer,0.3162393162393162,0.030463656747340254,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_medical_genetics,0.29,0.045604802157206845,"acc,none",6921720704,5.0,answer,0.29,0.045604802157206845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_miscellaneous,0.30779054916985954,0.01650604504515563,"acc,none",6921720704,5.0,answer,0.30779054916985954,0.01650604504515563,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_moral_disputes,0.2832369942196532,0.02425790170532337,"acc,none",6921720704,5.0,answer,0.2832369942196532,0.02425790170532337,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_moral_scenarios,0.24134078212290502,0.014310999547961452,"acc,none",6921720704,5.0,answer,0.24134078212290502,0.014310999547961452,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_nutrition,0.27450980392156865,0.025553169991826528,"acc,none",6921720704,5.0,answer,0.27450980392156865,0.025553169991826528,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_other,0.30447376890891537,0.008215009170523717,"acc,none",6921720704,,,0.30447376890891537,0.008215009170523717,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_philosophy,0.3054662379421222,0.026160584450140488,"acc,none",6921720704,5.0,answer,0.3054662379421222,0.026160584450140488,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_prehistory,0.32098765432098764,0.025976566010862737,"acc,none",6921720704,5.0,answer,0.32098765432098764,0.025976566010862737,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_professional_accounting,0.29432624113475175,0.0271871270115038,"acc,none",6921720704,5.0,answer,0.29432624113475175,0.0271871270115038,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_professional_law,0.24771838331160365,0.011025499291443744,"acc,none",6921720704,5.0,answer,0.24771838331160365,0.011025499291443744,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_professional_medicine,0.2536764705882353,0.026431329870789538,"acc,none",6921720704,5.0,answer,0.2536764705882353,0.026431329870789538,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_professional_psychology,0.2647058823529412,0.01784808957491322,"acc,none",6921720704,5.0,answer,0.2647058823529412,0.01784808957491322,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_public_relations,0.34545454545454546,0.04554619617541054,"acc,none",6921720704,5.0,answer,0.34545454545454546,0.04554619617541054,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_security_studies,0.27755102040816326,0.02866685779027465,"acc,none",6921720704,5.0,answer,0.27755102040816326,0.02866685779027465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_social_sciences,0.26519337016574585,0.007928747651234531,"acc,none",6921720704,,,0.26519337016574585,0.007928747651234531,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_sociology,0.3582089552238806,0.03390393042268814,"acc,none",6921720704,5.0,answer,0.3582089552238806,0.03390393042268814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_stem,0.2527751347922613,0.007723026418937003,"acc,none",6921720704,,,0.2527751347922613,0.007723026418937003,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_us_foreign_policy,0.4,0.049236596391733084,"acc,none",6921720704,5.0,answer,0.4,0.049236596391733084,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_virology,0.37349397590361444,0.037658451171688624,"acc,none",6921720704,5.0,answer,0.37349397590361444,0.037658451171688624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,mmlu_world_religions,0.38011695906432746,0.037229657413855394,"acc,none",6921720704,5.0,answer,0.38011695906432746,0.037229657413855394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,other,0.46829739298358547,0.008651722737544136,"acc,none",6921720704,,,0.46829739298358547,0.008651722737544136,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,social sciences,0.42151446213844657,0.00876142163006902,"acc,none",6921720704,,,0.42151446213844657,0.00876142163006902,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,stem,0.32952743418966063,0.008227069450364508,"acc,none",6921720704,,,0.32952743418966063,0.008227069450364508,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,truthfulqa_gen,0.26193390452876375,0.015392118805015032,"rouge1_acc,none",6921720704,0.0, ,,,,,,,,,,,,,,,,,,,,,,,,,,,22.94988386811439,0.7615448861452995,0.31456548347613217,0.016255241993179157,-9.394852578921569,0.8212595840283824,46.38152710528668,0.904128478635869,0.26193390452876375,0.015392118805015032,-12.884574235910073,0.8966997947833258,29.27481347413483,1.0080179575484558,0.19706242350061198,0.013925080734473742,-15.135500681470209,1.0640462391363208,43.58154340232294,0.8984783551873023,0.25458996328029376,0.015250117079156494,-13.22334928041737,0.9087456114696895
tiiuae/falcon-7b,3000000000000,openllm,truthfulqa_mc1,0.22399020807833536,0.014594964329474203,"acc,none",6921720704,0.0,0,0.22399020807833536,0.014594964329474203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,truthfulqa_mc2,0.34277162913779985,0.013272681916634024,"acc,none",6921720704,0.0,0,0.34277162913779985,0.013272681916634024,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tiiuae/falcon-7b,3000000000000,openllm,winogrande,0.7308602999210734,0.01246491195126874,"acc,none",6921720704,5.0,,0.7308602999210734,0.01246491195126874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
