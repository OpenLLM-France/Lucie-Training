{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves of Lucie-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load all benchmark results\n",
    "\n",
    "Load CSV files ([evaluation_learning_curve_lucie.csv](evaluation_learning_curve_lucie.csv) and [evaluation_baselines.csv](evaluation_baselines.csv))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 35 Lucie checkpoints evaluated on 6 benchmarks (228 datasets)\n",
      "✅ 9 baseline checkpoints evaluated on 6 benchmarks (184 datasets)\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas matplotlib seaborn python-slugify\n",
    " \n",
    "import pandas as pd\n",
    "\n",
    "lucie_results = pd.read_csv(\n",
    "    \"evaluation_learning_curve_lucie.csv\"\n",
    ")\n",
    "baseline_results = pd.read_csv(\n",
    "    \"evaluation_baselines.csv\"\n",
    ")\n",
    "\n",
    "# Print some statistics\n",
    "def unique(field): return len(lucie_results[field].unique())\n",
    "print(f\"✅ {unique('training_tokens')} Lucie checkpoints evaluated on {unique('benchmark')} benchmarks ({unique('dataset')} datasets)\")\n",
    "def unique(field): return len(baseline_results[field].unique())\n",
    "print(f\"✅ {unique('model_name')} baseline checkpoints evaluated on {unique('benchmark')} benchmarks ({unique('dataset')} datasets)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup a plot config (and some normalization of model/dataset names)\n",
    "\n",
    "In the code cell below, `benchmarks` must be a dictionary:\n",
    "* key: the name of the benchmark (will be plotted as a title)\n",
    "* values: a list of dataset names that will be plotted together (see column `dataset` of [the CSV file](evaluation_learning_curve_lucie.csv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What to plot\n",
    "BENCHMARKS = {\n",
    "    \"benchmarks in English\":\n",
    "        [\n",
    "            \"arc_challenge\",        \"hellaswag\",\n",
    "            \"mmlu_continuation\",    \"winogrande\",\n",
    "            \"gsm8k\",                \"truthfulqa_mc2\",\n",
    "        ],\n",
    "    \"French Bench\":\n",
    "        [\n",
    "            \"french_bench_arc_challenge\",   \"french_bench_hellaswag\",\n",
    "            \"french_bench_grammar\",         \"french_bench_vocab\",\n",
    "        ],\n",
    "    \"Multilingual ARC benchmark\":\n",
    "        [\n",
    "            \"arc_fr\",   \"arc_es\",\n",
    "            \"arc_de\",   \"arc_it\",\n",
    "        ],\n",
    "}\n",
    "\n",
    "# Output folder to save figures\n",
    "OUTPUT_FOLDER = \"figs\"\n",
    "\n",
    "\n",
    "PLOT_FIRST_PRETRAINING = True\n",
    "\n",
    "# Some niceties to normalize dataset names\n",
    "def normalize_dataset_name(name):\n",
    "    words = []\n",
    "    _languages = {\n",
    "        \"fr\": \"French\",\n",
    "        \"es\": \"Spanish\",\n",
    "        \"de\": \"German\",\n",
    "        \"it\": \"Italian\",\n",
    "        \"en\": \"English\",\n",
    "    }\n",
    "    for w in name.split(\"_\"):\n",
    "        if w in _languages:\n",
    "            w = _languages[w]\n",
    "        elif len(w) <= 2 or w in [\"arc\", \"mmlu\", \"mmmlu\", \"mc2\", \"mc1\"]:\n",
    "            w = w.upper()\n",
    "        else:\n",
    "            w = w.replace(\"gsm\", \"GSM\")\n",
    "            w = w[0].upper() + w[1:]\n",
    "        words.append(w)\n",
    "    return \" \".join(words)\n",
    "    \n",
    "\n",
    "# Some niceties to normalize baseline names\n",
    "def normalize_baseline_name(name):\n",
    "    common = normalize_baseline_name_no_version(name)\n",
    "    if \"instruct\" in name.lower():\n",
    "        return common + \"-Instruct\"\n",
    "    if \"Chat\" in name:\n",
    "        return common + \"-Chat\"\n",
    "    if \"annealing\" in name.lower():\n",
    "        return common + \"-Annealing\"\n",
    "    return common\n",
    "\n",
    "def normalize_baseline_name_no_version(name):\n",
    "    name = name.replace(\"__\", \"/\").split(\"/\")[-1]\n",
    "    for common in [\"Bloom\", \"Croissant\", \"Pythia-6.9B\", \"Llama-3\", \"Mistral-7B\", \"Falcon-7B\"]:\n",
    "        if common.lower() in name.lower():\n",
    "            return common\n",
    "    return name\n",
    "\n",
    "def key_order_model_name(name):\n",
    "    nlower = name.lower()\n",
    "    if \"lucie\" in nlower:\n",
    "        return (0, name)\n",
    "    if \"falcon\" in nlower or \"pythia\" in nlower:\n",
    "        return (3, name)\n",
    "    if \"croissant\" in nlower:\n",
    "        return (2, name)\n",
    "    return (1, name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plot the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import slugify\n",
    "\n",
    "\n",
    "def plot_learning_curve(\n",
    "        lucie_results,\n",
    "        baseline_results,\n",
    "        xlabel=True,\n",
    "        xlogscale=True,\n",
    "        use_lines=True,\n",
    "        title=None,\n",
    "    ):\n",
    "    global _cached_baseline_indices\n",
    "    if \"_cached_baseline_indices\" not in globals():\n",
    "        _cached_baseline_indices = {}\n",
    "\n",
    "    norm_tokens = 1e09 if not xlogscale else 1\n",
    "\n",
    "    score_name = set(lucie_results[\"score_name\"])\n",
    "    assert len(score_name) == 1, f\"Multiple score names found: {score_name}\"\n",
    "    score_name = list(score_name)[0]\n",
    "    score_name = score_name.replace(\",none\", \"\")\n",
    "\n",
    "    num_fewshot = set(lucie_results[\"num_fewshot\"])\n",
    "    num_fewshot = list(num_fewshot)[0]\n",
    "    if num_fewshot:\n",
    "        try:\n",
    "            num_fewshot = int(num_fewshot)\n",
    "        except Exception:\n",
    "            num_fewshot = None\n",
    "\n",
    "    # Set colorblind-friendly color palette\n",
    "    color_palette = sns.color_palette(\"colorblind\", n_colors=6)  # Colorblind-friendly palette\n",
    "    lucie_color = color_palette[0]\n",
    "    baseline_colors = color_palette[1:] + ['blue']\n",
    "    baseline_markers = ['_', '*', 's', '^', 'D', 'v', '*', 'o']  # Circle, Square, Diamond, Down-Triangle, Up-Triangle, Circle again\n",
    "    baseline_markersizes = [6] * len(baseline_markers) # [6, 6, 6, 6, 6, 6, 6]  # Adjust sizes for each marker type\n",
    "    max_styles = len(baseline_colors)\n",
    "    \n",
    "    baseline_names = sorted(baseline_results[\"model_name\"].unique(), key=key_order_model_name)\n",
    "    norm_baseline_names = [normalize_baseline_name_no_version(name) for name in baseline_names]\n",
    "    has_several_versions = len(set(norm_baseline_names)) != len(norm_baseline_names)\n",
    "\n",
    "    # Lucie\n",
    "    if not PLOT_FIRST_PRETRAINING:\n",
    "        lucie_results = lucie_results[lucie_results[\"training_tokens\"] > 3_121_742_086_100]\n",
    "    for i, model_name in enumerate(lucie_results[\"model_name\"].unique()):\n",
    "\n",
    "        name_no_version = normalize_baseline_name_no_version(model_name)\n",
    "        name = normalize_baseline_name(model_name)\n",
    "\n",
    "        results = lucie_results[lucie_results[\"model_name\"] == model_name] \\\n",
    "            .sort_values(by=[\"training_tokens\", \"score\"], ascending=True)\n",
    "        \n",
    "        is_main = (name == name_no_version)\n",
    "        if is_main:\n",
    "            assert i == 0, f\"Multiple main models found, mapping to {name_no_version} (<- {name} <- {model_name})\"\n",
    "            marker = '.-'\n",
    "        else:\n",
    "            has_several_versions = True\n",
    "            marker = baseline_markers[i-1]\n",
    "\n",
    "        plt.plot(\n",
    "            results['training_tokens'] / norm_tokens,\n",
    "            results['score'],\n",
    "            marker,\n",
    "            label=model_name,\n",
    "            color=lucie_color,\n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "        if is_main:\n",
    "\n",
    "            last_x = list(results['training_tokens'])[-1] / norm_tokens\n",
    "            last_perf = list(results['score'])[-1]\n",
    "            if use_lines:\n",
    "                plt.axvline(x=last_x / norm_tokens, color=lucie_color, linestyle=':')  # Add vertical line\n",
    "                plt.axhline(y=last_perf, color=lucie_color, linestyle=':')  # Add horizontal line\n",
    "            else:\n",
    "                plt.plot([last_x], [last_perf], \"+\", markersize=10, color=lucie_color)\n",
    "\n",
    "    if xlogscale:\n",
    "        plt.xscale('log')  # Set x-axis to log scale\n",
    "        xticks_coordinates, _ = plt.xticks()\n",
    "        previous = xticks_coordinates[0]\n",
    "        new_xticks = [previous]\n",
    "        for x in xticks_coordinates[1:]:\n",
    "            if x / previous >= 10 - 1e-06:\n",
    "                new_xticks.append(3 * previous)\n",
    "            new_xticks.append(x)\n",
    "            previous = x\n",
    "        if norm_tokens == 1:\n",
    "            new_xticks_labels = [format_big_integer(x) for x in new_xticks]\n",
    "            plt.xticks(new_xticks, new_xticks_labels)\n",
    "        else:\n",
    "            plt.xticks(new_xticks)\n",
    "\n",
    "    # Baselines with different colors, shapes, and adjusted marker sizes\n",
    "    current_version_idx = {}\n",
    "    for model_name in baseline_names:\n",
    "        results = baseline_results[baseline_results[\"model_name\"] == model_name]\n",
    "        name_no_version = normalize_baseline_name_no_version(model_name)\n",
    "        name = normalize_baseline_name(model_name)\n",
    "        if name_no_version in _cached_baseline_indices:\n",
    "            idx = _cached_baseline_indices[name_no_version]\n",
    "        else:\n",
    "            idx = _cached_baseline_indices[name_no_version] = len(_cached_baseline_indices)\n",
    "        if idx >= max_styles:\n",
    "            raise RuntimeError(f\"Not enought baseline styles (max: {max_styles}, already collected: {sorted(_cached_baseline_indices.keys())}, new: '{name_no_version}')!\")\n",
    "        color = baseline_colors[idx]\n",
    "        current_version_idx[name_no_version] = current_version_idx.get(name_no_version, -1) + 1\n",
    "        if has_several_versions:\n",
    "            idx_marker = current_version_idx[name_no_version]\n",
    "        else:\n",
    "            idx_marker = idx\n",
    "        marker = baseline_markers[idx_marker]\n",
    "        markersize = baseline_markersizes[idx_marker]\n",
    "        plt.plot(\n",
    "            results['training_tokens'] / norm_tokens,\n",
    "            results['score'],\n",
    "            marker,\n",
    "            label=name,\n",
    "            markersize=markersize,\n",
    "            color=color,\n",
    "            # linestyle=None\n",
    "        )\n",
    "    if xlabel:\n",
    "        plt.xlabel(\"# training tokens\" + (\" (in billions)\" if norm_tokens == 1e09 else (\" (log scale)\" if xlogscale else \"\")))\n",
    "\n",
    "    if title is not False:\n",
    "        if not title: title = \"\"\n",
    "        else: title += \"\\n\"\n",
    "        title += f\"({score_name}\"\n",
    "        if num_fewshot:\n",
    "            title += f\", {num_fewshot}-shot\"\n",
    "        title += \")\"\n",
    "        plt.title(title, weight='bold')\n",
    "\n",
    "\n",
    "def can_be_rounded(x, ratio):\n",
    "    return abs(x / ratio) % 1 <= 0.05\n",
    "\n",
    "def format_big_integer(x):\n",
    "    if x <= 1000: return str(int(x))\n",
    "    if x <= 950_000 and can_be_rounded(x, 1000): return f\"{x / 1_000:.0f}K\"\n",
    "    if x <= 950_000_000 and can_be_rounded(x, 1_000_000): return f\"{x / 1_000_000:.0f}M\"\n",
    "    if x <= 950_000_000_000 and can_be_rounded(x, 1_000_000_000): return f\"{x / 1_000_000_000:.0f}B\"\n",
    "    if x <= 950_000_000_000: return f\"{x / 1_000_000_000:.1f}B\"\n",
    "    if can_be_rounded(x, 1_000_000_000_000): return f\"{x / 1_000_000_000_000:.0f}T\"\n",
    "    return f\"{x / 1_000_000_000_000:.1f}T\"\n",
    "\n",
    "\n",
    "def plot_learning_curves_batch(\n",
    "        lucie_results,\n",
    "        baseline_results,\n",
    "        benchmark_names,\n",
    "        title=None,\n",
    "        filename=None,\n",
    "    ):\n",
    "\n",
    "    ncols = min(2, len(benchmark_names))\n",
    "    nrows = (len(benchmark_names) + 1) // ncols\n",
    "\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(3 * ncols, 3.5 * nrows))\n",
    "\n",
    "    for i_bench, benchmark_name in enumerate(benchmark_names):\n",
    "        plt.subplot(nrows, ncols, i_bench+1)\n",
    "        lucie_selec = lucie_results[lucie_results[\"dataset\"] == benchmark_name]\n",
    "        baseline_selec = baseline_results[baseline_results[\"dataset\"] == benchmark_name]\n",
    "        assert len(lucie_selec) > 0, f\"No results for Lucie for {benchmark_name}\"\n",
    "        assert len(baseline_selec) > 0, f\"No baseline results for Lucie for {benchmark_name}\"\n",
    "        plot_learning_curve(\n",
    "            lucie_selec, baseline_selec,\n",
    "            xlabel=(i_bench >= ((len(benchmark_names) + 1) // 2) * 2 - 2),\n",
    "            title=normalize_dataset_name(benchmark_name),\n",
    "        )\n",
    "        \n",
    "\n",
    "    # Add the overall title with reduced spacing\n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=16, weight='bold', y=0.9)  # Reduce y value to bring the title closer\n",
    "\n",
    "    # Adjust layout to add space for legend\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "\n",
    "    # Set a single legend after the plots\n",
    "    ax = plt.subplot(nrows, ncols, 1)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', fontsize=10, ncol=3)  # Legend after plots\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0.1, 1, 0.9])  # Adjust the plot to fit in the figure\n",
    "    if filename:\n",
    "        print(f\"Saving {filename}...\")\n",
    "        plt.savefig(filename, facecolor='w', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        filename = os.path.join(OUTPUT_FOLDER, slugify.slugify(f\"learning_curve_evaluation_{title}\") + \".png\")\n",
    "\n",
    "    plot_learning_curves_batch(\n",
    "        lucie_results,\n",
    "        baseline_results,\n",
    "        dataset_names,\n",
    "        title=title,\n",
    "        filename=filename,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
