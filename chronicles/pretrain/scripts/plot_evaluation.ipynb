{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves of Lucie-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import slugify\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load all benchmark results\n",
    "\n",
    "Load CSV files ([evaluation_learning_curve_lucie.csv](evaluation_learning_curve_lucie.csv) and [evaluation_baselines.csv](evaluation_baselines.csv))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 43 Lucie checkpoints evaluated on 5 benchmarks (192 datasets)\n",
      "✅ 9 baseline checkpoints evaluated on 5 benchmarks (184 datasets)\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas matplotlib seaborn python-slugify\n",
    "lucie_results = pd.read_csv(\n",
    "    \"../evaluation_learning_curve_lucie.csv\"\n",
    ")\n",
    "baseline_results = pd.read_csv(\n",
    "    \"../evaluation_baselines.csv\"\n",
    ")\n",
    "\n",
    "# Print some statistics\n",
    "def unique(field): return len(lucie_results[field].unique())\n",
    "print(f\"✅ {unique('training_tokens')} Lucie checkpoints evaluated on {unique('benchmark')} benchmarks ({unique('dataset')} datasets)\")\n",
    "def unique(field): return len(baseline_results[field].unique())\n",
    "print(f\"✅ {unique('model_name')} baseline checkpoints evaluated on {unique('benchmark')} benchmarks ({unique('dataset')} datasets)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction for the baselines !\n",
    "\n",
    "baseline_results['training_phase'] = baseline_results['training_phase'].map({'main': '1_main', 'instruction': '4_instruct_full'})\n",
    "def map_baseline_name_to_expe(model_name):\n",
    "    if 'Croissant' in model_name:\n",
    "        return 'CroissantLLM'\n",
    "    if 'pythia' in model_name:\n",
    "        return 'Pythia'\n",
    "    if 'Mistral' in model_name:\n",
    "        return 'Mistral'\n",
    "    if 'Llama-3.1' in model_name:\n",
    "        return 'Llama-3.1'\n",
    "    if 'bloom' in model_name:\n",
    "        return 'Bloom'\n",
    "    if 'falcon' in model_name:\n",
    "        return 'Falcon'\n",
    "    else:\n",
    "        return model_name\n",
    "\n",
    "baseline_results['expe_name'] = baseline_results['model_name'].apply(map_baseline_name_to_expe)\n",
    "baseline_results['model_name'] = 'Baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([lucie_results, baseline_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup a plot config (and some normalization of model/dataset names)\n",
    "\n",
    "In the code cell below, `benchmarks` must be a dictionary:\n",
    "* key: the name of the benchmark (will be plotted as a title)\n",
    "* values: a list of dataset names that will be plotted together (see column `dataset` of [the CSV file](evaluation_learning_curve_lucie.csv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What to plot\n",
    "BENCHMARKS = {\n",
    "    \"Leaderboard v1\":\n",
    "        [\n",
    "            \"arc_challenge\",        \"hellaswag\",\n",
    "            \"mmlu\",\n",
    "            \"mmlu_continuation\",    \"winogrande\",\n",
    "            \"gsm8k\",                \"truthfulqa_mc2\",\n",
    "        ],\n",
    "    \"Leaderbooard\":\n",
    "        [\n",
    "            \"leaderboard_bbh\", \"leaderboard_gpqa\", \"leaderboard_math_hard\", \"leaderboard_musr\"\n",
    "        ],\n",
    "    \"French Bench\":\n",
    "        [\n",
    "            \"french_bench_arc_challenge\",   \"french_bench_hellaswag\",\n",
    "            \"french_bench_grammar\",         \"french_bench_vocab\",\n",
    "        ],\n",
    "    \"French Bench Generative\":\n",
    "        [\n",
    "            \"french_bench_fquadv2_genq\",   \"french_bench_multifquad\",\n",
    "            \"french_bench_orangesum_abstract\",         \"french_bench_trivia\",\n",
    "        ],\n",
    "    \"Multilingual ARC benchmark\":\n",
    "        [\n",
    "            \"arc_fr\",   \"arc_es\",\n",
    "            \"arc_de\",   \"arc_it\",\n",
    "        ],\n",
    "}\n",
    "\n",
    "# Output folder to save figures\n",
    "OUTPUT_FOLDER = \"../figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameters\n",
    "marker_types = {\n",
    "    '1_main': 'o', \n",
    "    '2_extension': '*', \n",
    "    '3_annealing': 's', \n",
    "    '4_instruct_full': '^', \n",
    "    '4_instruct_full_deprecated': 'D', \n",
    "    '4_instruct_lora': 'v'\n",
    "    }  \n",
    "\n",
    "pattern_types = {\n",
    "    '1_main': None, \n",
    "    '2_extension': '//', \n",
    "    '3_annealing': '--', \n",
    "    '4_instruct_full': '..', \n",
    "    '4_instruct_full_deprecated': '**', \n",
    "    '4_instruct_lora': '\\\\\\\\'\n",
    "    }  \n",
    "\n",
    "def is_valid(model_name, phase, expe, restrict_to):\n",
    "    for restrict in restrict_to:\n",
    "        r_model_name, r_phase, r_expe = restrict\n",
    "        if (r_model_name is None or r_model_name == model_name) and \\\n",
    "           (r_phase is None or r_phase == phase) and \\\n",
    "           (r_expe is None or r_expe == expe):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def can_be_rounded(x, ratio):\n",
    "    return abs(x / ratio) % 1 <= 0.05\n",
    "\n",
    "def format_big_integer(x):\n",
    "    if x <= 1000: return str(int(x))\n",
    "    if x <= 950_000 and can_be_rounded(x, 1000): return f\"{x / 1_000:.0f}K\"\n",
    "    if x <= 950_000_000 and can_be_rounded(x, 1_000_000): return f\"{x / 1_000_000:.0f}M\"\n",
    "    if x <= 950_000_000_000 and can_be_rounded(x, 1_000_000_000): return f\"{x / 1_000_000_000:.0f}B\"\n",
    "    if x <= 950_000_000_000: return f\"{x / 1_000_000_000:.1f}B\"\n",
    "    if can_be_rounded(x, 1_000_000_000_000): return f\"{x / 1_000_000_000_000:.0f}T\"\n",
    "    return f\"{x / 1_000_000_000_000:.1f}T\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(model_name, training_phase, expe_name):\n",
    "    if model_name == 'Baseline':\n",
    "        if 'main' in training_phase:\n",
    "            return f\"{expe_name}\"\n",
    "        if 'instruct' in training_phase:\n",
    "            return f\"{expe_name}-Instruct\"\n",
    "    elif model_name == 'Lucie-7B':\n",
    "        if 'main' in training_phase:\n",
    "            return f\"{model_name}-Pretraining\"\n",
    "        if 'extension' in training_phase:\n",
    "            return f\"{model_name}-Extension ({expe_name})\"\n",
    "        if 'annealing' in training_phase:\n",
    "            return f\"{model_name}-Annealing ({expe_name})\"\n",
    "        if 'instruct' in training_phase:\n",
    "            if 'lora' in training_phase:\n",
    "                return f\"{model_name}-Instruct (Lora - {expe_name})\"\n",
    "            if 'full' in training_phase:\n",
    "                return f\"{model_name}-Instruct (Full - {expe_name})\"\n",
    "            if 'deprecated' in training_phase:\n",
    "                return f\"{model_name}-Instruct (Full - Deprecated)\"\n",
    "            return f\"{model_name}-Instruct ({expe_name})\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(lucie_results, title, benchmark_names, filename, restrict_to=None, bar_plot=False, xlogscale=True):\n",
    "    lucie_results['allowed'] = lucie_results.apply(lambda row: is_valid(row['model_name'], row['training_phase'], row['expe_name'], restrict_to), axis=1)\n",
    "    lucie_results = lucie_results[lucie_results['allowed']]\n",
    "\n",
    "    ncols = min(2, len(benchmark_names))\n",
    "    nrows = (len(benchmark_names) + 1) // ncols\n",
    "\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(3 * ncols, 3.5 * nrows))\n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=16, weight='bold', y=0.9)  # Reduce y value to bring the title closer\n",
    "    \n",
    "    # Ensure at least 50 different colors\n",
    "    if len(lucie_results['expe_name'].unique()) > 20:\n",
    "        warnings.warn(\"More than 20 experiments, using a larger color palette.\")\n",
    "\n",
    "    palette = sns.color_palette(\"tab20\", n_colors=20)\n",
    "    # colors = sns.color_palette(\"Spectral\", num_colors)\n",
    "    color_mapping = {expe: palette[i % 20] for i, expe in enumerate(lucie_results['expe_name'].unique())}\n",
    "    xlabel = True\n",
    "    norm_tokens = 1e09 if not xlogscale else 1\n",
    "\n",
    "    # Score names and few shot\n",
    "    benchmark2score = dict(zip(lucie_results['dataset'], lucie_results['score_name']))\n",
    "    benchmark2fewshot = dict(zip(lucie_results['dataset'], lucie_results['num_fewshot']))\n",
    "\n",
    "    # Select the benchmark\n",
    "    for i_bench, benchmark_name in enumerate(benchmark_names):\n",
    "        plt.subplot(nrows, ncols, i_bench+1)\n",
    "        lucie_results_bench = lucie_results[lucie_results[\"dataset\"] == benchmark_name].copy()\n",
    "\n",
    "        title = f\"{benchmark_name}\\n\"\n",
    "        if benchmark_name in benchmark2score:\n",
    "            score_name = benchmark2score[benchmark_name].replace(\",none\", \"\")\n",
    "            title += f\"({score_name}\"\n",
    "        if benchmark_name in benchmark2fewshot:\n",
    "            num_fewshot = benchmark2fewshot[benchmark_name]\n",
    "            if num_fewshot:\n",
    "                try:\n",
    "                    num_fewshot = int(num_fewshot)\n",
    "                    title += f\", {num_fewshot}-shot\"\n",
    "                except Exception:\n",
    "                    num_fewshot = None\n",
    "        title += \")\"\n",
    "        plt.title(title, weight='bold')\n",
    "\n",
    "        # Loop on expe\n",
    "        for (model_name, phase, expe), df in lucie_results_bench.groupby(['model_name', 'training_phase', 'expe_name']):\n",
    "            df.sort_values(by=['training_tokens', 'add_bos_token', 'chat_template', 'fewshot_as_multiturn'],\n",
    "                           ascending=[True, False, True, True], inplace=True)\n",
    "            label = create_label(model_name, phase, expe)\n",
    "            \n",
    "            if len(df) > 1 and phase != \"1_main\":\n",
    "                warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
    "                df = df.iloc[-1]\n",
    "                warnings.warn(f\"Selecting: {df[['model_name', 'training_phase', 'expe_name', 'training_tokens', 'add_bos_token', 'chat_template', 'fewshot_as_multiturn']]}\")\n",
    "\n",
    "            if phase not in marker_types:\n",
    "                raise ValueError(f\"Unknown phase: {phase}. Please define a marker for this phase.\")\n",
    "            if phase not in pattern_types:\n",
    "                raise ValueError(f\"Unknown phase: {phase}. Please define a marker for this phase.\")\n",
    "            marker = marker_types[phase]\n",
    "            pattern = pattern_types[phase]\n",
    "            color = color_mapping.get(expe, 'black')  # Default color\n",
    "\n",
    "            #### PLOT IS HERE ####\n",
    "            if bar_plot:\n",
    "                df = df.iloc[-1]\n",
    "                plt.bar(label, df['score'],  yerr=df['stderr'], label=label, color=color, edgecolor='grey', hatch=pattern)\n",
    "                \n",
    "            else:\n",
    "                if len(df) > 1 and phase == \"1_main\":\n",
    "                    plt.plot(df['training_tokens'], df['score'], label=label, color=color, linewidth=1)\n",
    "                    plt.plot(df.iloc[-1]['training_tokens'], df.iloc[-1]['score'], marker, label=label, color=color, linewidth=1)\n",
    "                else:\n",
    "                    plt.plot(df['training_tokens'], df['score'], marker, label=label, color=color)\n",
    "\n",
    "        if xlogscale:\n",
    "            plt.xscale('log')  # Set x-axis to log scale\n",
    "            xticks_coordinates, _ = plt.xticks()\n",
    "            previous = xticks_coordinates[0]\n",
    "            new_xticks = [previous]\n",
    "            for x in xticks_coordinates[1:]:\n",
    "                if x / previous >= 10 - 1e-06:\n",
    "                    new_xticks.append(3 * previous)\n",
    "                new_xticks.append(x)\n",
    "                previous = x\n",
    "            if norm_tokens == 1:\n",
    "                new_xticks_labels = [format_big_integer(x) for x in new_xticks]\n",
    "                plt.xticks(new_xticks, new_xticks_labels)\n",
    "            else:\n",
    "                plt.xticks(new_xticks)\n",
    "\n",
    "        if bar_plot:\n",
    "            plt.xticks([])\n",
    "            plt.grid(True, axis='y', linestyle='--', alpha=0.7)  # You can adjust grid style (line type, transparency)\n",
    "        else:\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)  # You can adjust grid style (line type, transparency)\n",
    "            m, M = 100*1e9, 20*1e12\n",
    "            plt.xlim(m, M)\n",
    "            if xlabel:\n",
    "                plt.xlabel(\"# training tokens\" + (\" (in billions)\" if norm_tokens == 1e09 else (\" (log scale)\" if xlogscale else \"\")))\n",
    "\n",
    "    # Adjust layout to add space for legend\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "\n",
    "    # Set a single legend after the plots\n",
    "    ax = plt.subplot(nrows, ncols, 1)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', fontsize=10, ncol=3)  # Legend after plots\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0.1, 1, 0.9])  # Adjust the plot to fit in the figure\n",
    "    if filename:\n",
    "        print(f\"Saving {filename}...\")\n",
    "        plt.savefig(filename, facecolor='w', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/v2-learning-curve-evaluation-leaderboard-v1.png...\n",
      "Saving ../figs/v2-learning-curve-evaluation-leaderbooard.png...\n",
      "Saving ../figs/v2-learning-curve-evaluation-french-bench.png...\n",
      "Saving ../figs/v2-learning-curve-evaluation-french-bench-generative.png...\n",
      "Saving ../figs/v2-learning-curve-evaluation-multilingual-arc-benchmark.png...\n"
     ]
    }
   ],
   "source": [
    "restrict_to = [\n",
    "    ('Lucie-7B', '1_main', None),\n",
    "    ('Baseline', None, None),\n",
    "]\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        filename = os.path.join(OUTPUT_FOLDER, slugify.slugify(f\"v2_learning_curve_evaluation_{title}\") + \".png\")\n",
    "\n",
    "    plot_results(results, title, dataset_names, filename, restrict_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/v2-bar-instruction-leaderboard-v1.png...\n",
      "Saving ../figs/v2-bar-instruction-leaderbooard.png...\n",
      "Saving ../figs/v2-bar-instruction-french-bench.png...\n",
      "Saving ../figs/v2-bar-instruction-french-bench-generative.png...\n",
      "Saving ../figs/v2-bar-instruction-multilingual-arc-benchmark.png...\n"
     ]
    }
   ],
   "source": [
    "restrict_to = [\n",
    "    ('Lucie-7B', '1_main', None),\n",
    "    ('Lucie-7B', '4_instruct_full', None),\n",
    "]\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        filename = os.path.join(OUTPUT_FOLDER, slugify.slugify(f\"v2_bar_instruction_{title}\") + \".png\")\n",
    "\n",
    "    plot_results(results, title, dataset_names, filename, restrict_to, bar_plot=True, xlogscale=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
