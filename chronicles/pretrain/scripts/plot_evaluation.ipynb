{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves of Lucie-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import slugify\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load all benchmark results\n",
    "\n",
    "Load CSV files ([evaluation_learning_curve_lucie.csv](evaluation_learning_curve_lucie.csv) and [evaluation_baselines.csv](evaluation_baselines.csv))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 45 Lucie checkpoints evaluated on 6 benchmarks (192 datasets)\n",
      "✅ 9 baseline checkpoints evaluated on 5 benchmarks (184 datasets)\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas matplotlib seaborn python-slugify\n",
    "lucie_results = pd.read_csv(\n",
    "    \"../evaluation_learning_curve_lucie.csv\"\n",
    ")\n",
    "baseline_results = pd.read_csv(\n",
    "    \"../evaluation_baselines.csv\"\n",
    ")\n",
    "\n",
    "# Print some statistics\n",
    "def unique(field): return len(lucie_results[field].unique())\n",
    "print(f\"✅ {unique('training_tokens')} Lucie checkpoints evaluated on {unique('benchmark')} benchmarks ({unique('dataset')} datasets)\")\n",
    "def unique(field): return len(baseline_results[field].unique())\n",
    "print(f\"✅ {unique('model_name')} baseline checkpoints evaluated on {unique('benchmark')} benchmarks ({unique('dataset')} datasets)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction for the baselines !\n",
    "\n",
    "baseline_results['training_phase'] = baseline_results['training_phase'].map({'main': '1_main', 'instruction': '4_instruct_full'})\n",
    "def map_baseline_name_to_expe(model_name):\n",
    "    if 'Croissant' in model_name:\n",
    "        return 'CroissantLLM'\n",
    "    if 'pythia' in model_name:\n",
    "        return 'Pythia'\n",
    "    if 'Mistral' in model_name:\n",
    "        return 'Mistral'\n",
    "    if 'Llama-3.1' in model_name:\n",
    "        return 'Llama-3.1'\n",
    "    if 'bloom' in model_name:\n",
    "        return 'Bloom'\n",
    "    if 'falcon' in model_name:\n",
    "        return 'Falcon'\n",
    "    else:\n",
    "        return model_name\n",
    "\n",
    "baseline_results['expe_name'] = baseline_results['model_name'].apply(map_baseline_name_to_expe)\n",
    "baseline_results['model_name'] = 'Baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction for Lucie names\n",
    "\n",
    "def map_formal_names(expe_name):\n",
    "    if expe_name == 'nocomma/recipe1_fullpipe':\n",
    "        return 'mix1' \n",
    "    if expe_name == 'nocomma/recipe2_fullpipe':\n",
    "        return 'mix2a'\n",
    "    if expe_name == 'nocomma/recipe4_fullpipe' or expe_name == 'mix2':\n",
    "        return 'mix2b'\n",
    "    if expe_name == 'nocomma/recipe3_fullpipe':\n",
    "        return 'mix3' \n",
    "    if expe_name == 'nocomma/recipe7_fullpipe':\n",
    "        return 'mix4'\n",
    "    if expe_name == '40M_tokens-mix2':\n",
    "        return '40M_tokens-mix2b'\n",
    "    if expe_name == '5B_tokens-mix2':\n",
    "        return '5B_tokens-mix2b'\n",
    "    if expe_name == 'recipe1': # no fp\n",
    "        return 'R1'\n",
    "    if expe_name == 'recipe2': # no fp\n",
    "        return 'R2a'\n",
    "    if expe_name == 'recipe2_fullpipe': \n",
    "        return 'R2afp'\n",
    "    if expe_name == 'recipe4_fullpipe':\n",
    "        return 'R2bfp'\n",
    "    if expe_name == 'recipe7_fullpipe':\n",
    "        return 'R7m4fp'\n",
    "    else:\n",
    "        return expe_name\n",
    "            \n",
    "lucie_results['expe_name'] = lucie_results['expe_name'].apply(map_formal_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([lucie_results, baseline_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: ['Lucie-7B' 'Baseline']\n",
      "\n",
      "Traning phase: ['1_main' '2_extension' '3_annealing' '4_instruct_lora' '4_instruct_full'\n",
      " '4_instruct_full_deprecated']\n",
      "\n",
      "Expe name: ['pretraining' 'rope20M' 'rope500k' '40M_tokens-mix1' '40M_tokens-mix2b'\n",
      " '40M_tokens-mix3' '40M_tokens-mix4' '5B_tokens-mix1' '5B_tokens-mix2b'\n",
      " '5B_tokens-mix3' '5B_tokens-mix4' '5B_tokens-mix6' '5B_tokens-mix5' 'R1'\n",
      " 'R2a' 'R2afp' 'R2bfp' 'R7m4fp' 'mix1' 'mix2a' 'mix2b' 'mix4' 'mix3'\n",
      " 'mix5' 'DemoCredi2Small' 'DemoCredi2Small_v2' 'DemoCredi2' 'CroissantLLM'\n",
      " 'Llama-3.1' 'Mistral' 'Bloom' 'Falcon' 'Pythia']\n",
      "\n",
      "model_name             training_phase          expe_name\n",
      "  Baseline                     1_main              Bloom\n",
      "  Baseline                     1_main       CroissantLLM\n",
      "  Baseline                     1_main             Falcon\n",
      "  Baseline                     1_main          Llama-3.1\n",
      "  Baseline                     1_main            Mistral\n",
      "  Baseline                     1_main             Pythia\n",
      "  Baseline            4_instruct_full       CroissantLLM\n",
      "  Baseline            4_instruct_full          Llama-3.1\n",
      "  Baseline            4_instruct_full            Mistral\n",
      "  Lucie-7B                     1_main        pretraining\n",
      "  Lucie-7B                2_extension            rope20M\n",
      "  Lucie-7B                2_extension           rope500k\n",
      "  Lucie-7B                3_annealing    40M_tokens-mix1\n",
      "  Lucie-7B                3_annealing   40M_tokens-mix2b\n",
      "  Lucie-7B                3_annealing    40M_tokens-mix3\n",
      "  Lucie-7B                3_annealing    40M_tokens-mix4\n",
      "  Lucie-7B                3_annealing     5B_tokens-mix1\n",
      "  Lucie-7B                3_annealing    5B_tokens-mix2b\n",
      "  Lucie-7B                3_annealing     5B_tokens-mix3\n",
      "  Lucie-7B                3_annealing     5B_tokens-mix4\n",
      "  Lucie-7B                3_annealing     5B_tokens-mix5\n",
      "  Lucie-7B                3_annealing     5B_tokens-mix6\n",
      "  Lucie-7B            4_instruct_full               mix1\n",
      "  Lucie-7B            4_instruct_full              mix2b\n",
      "  Lucie-7B            4_instruct_full               mix3\n",
      "  Lucie-7B            4_instruct_full               mix4\n",
      "  Lucie-7B            4_instruct_full               mix5\n",
      "  Lucie-7B 4_instruct_full_deprecated               mix1\n",
      "  Lucie-7B 4_instruct_full_deprecated              mix2b\n",
      "  Lucie-7B 4_instruct_full_deprecated               mix3\n",
      "  Lucie-7B            4_instruct_lora         DemoCredi2\n",
      "  Lucie-7B            4_instruct_lora    DemoCredi2Small\n",
      "  Lucie-7B            4_instruct_lora DemoCredi2Small_v2\n",
      "  Lucie-7B            4_instruct_lora                 R1\n",
      "  Lucie-7B            4_instruct_lora                R2a\n",
      "  Lucie-7B            4_instruct_lora              R2afp\n",
      "  Lucie-7B            4_instruct_lora              R2bfp\n",
      "  Lucie-7B            4_instruct_lora             R7m4fp\n",
      "  Lucie-7B            4_instruct_lora               mix1\n",
      "  Lucie-7B            4_instruct_lora              mix2a\n",
      "  Lucie-7B            4_instruct_lora              mix2b\n",
      "  Lucie-7B            4_instruct_lora               mix3\n",
      "  Lucie-7B            4_instruct_lora               mix4\n"
     ]
    }
   ],
   "source": [
    "# Some print that can be useful\n",
    "print(f\"Model name: {results['model_name'].unique()}\")\n",
    "print(f\"\\nTraning phase: {results['training_phase'].unique()}\")\n",
    "print(f\"\\nExpe name: {results['expe_name'].unique()}\")\n",
    "\n",
    "print(f\"\\n{results[['model_name', 'training_phase', 'expe_name']].sort_values(['model_name', 'training_phase', 'expe_name']).drop_duplicates().to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup a plot config (and some normalization of model/dataset names)\n",
    "\n",
    "In the code cell below, `benchmarks` must be a dictionary:\n",
    "* key: the name of the benchmark (will be plotted as a title)\n",
    "* values: a list of dataset names that will be plotted together (see column `dataset` of [the CSV file](evaluation_learning_curve_lucie.csv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What to plot\n",
    "BENCHMARKS = {\n",
    "    \"HF Leaderboard v1\":\n",
    "        [\n",
    "            \"arc_challenge\",        \"hellaswag\",\n",
    "            \"mmlu\",\n",
    "            \"mmlu_continuation\",    \"winogrande\",\n",
    "            \"gsm8k\",                \"truthfulqa_mc2\",\n",
    "        ],\n",
    "    \"HF Leaderboard v2\":\n",
    "        [\n",
    "            \"leaderboard_bbh\", \"leaderboard_gpqa\", \"leaderboard_math_hard\", \"leaderboard_musr\"\n",
    "        ],\n",
    "    \"French Bench\":\n",
    "        [\n",
    "            \"french_bench_arc_challenge\",   \"french_bench_hellaswag\",\n",
    "            \"french_bench_grammar\",         \"french_bench_vocab\",\n",
    "        ],\n",
    "    \"French Bench Generative\":\n",
    "        [\n",
    "            \"french_bench_fquadv2_genq\",   \"french_bench_multifquad\",\n",
    "            \"french_bench_orangesum_abstract\",         \"french_bench_trivia\",\n",
    "        ],\n",
    "    \"Multilingual ARC benchmark\":\n",
    "        [\n",
    "            \"arc_fr\",   \"arc_es\",\n",
    "            \"arc_de\",   \"arc_it\",\n",
    "        ],\n",
    "    \"MMMMLU\":\n",
    "        [\n",
    "            \"m_mmlu_fr\", \"m_mmlu_es\", \"m_mmlu_de\", \"m_mmlu_it\",\n",
    "        ],\n",
    "    \"HF OpenLLMFrenchLeaderboard\":\n",
    "        [\n",
    "            \"leaderboard_mmlu_fr\", \"leaderboard_bbh_fr\", \"leaderboard_gpqa_fr\", \n",
    "            \"leaderboard_math_hard_fr\", \"leaderboard_ifeval_fr\", \"leaderboard_musr_fr\",\n",
    "        ]\n",
    "}\n",
    "\n",
    "# Output folder to save figures\n",
    "OUTPUT_FOLDER = \"../figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameters\n",
    "marker_types = {\n",
    "    '1_main': 'o', \n",
    "    '2_extension': '*', \n",
    "    '3_annealing': 's', \n",
    "    '4_instruct_full': '^', \n",
    "    '4_instruct_full_deprecated': 'D', \n",
    "    '4_instruct_lora': 'v'\n",
    "    }  \n",
    "\n",
    "pattern_types = {\n",
    "    '1_main': None, \n",
    "    '2_extension': '//', \n",
    "    '3_annealing': '--', \n",
    "    '4_instruct_full': '..', \n",
    "    '4_instruct_full_deprecated': '**', \n",
    "    '4_instruct_lora': '\\\\\\\\'\n",
    "    }  \n",
    "\n",
    "def is_valid_with_index(model_name, phase, expe, restrict_to):\n",
    "    for idx, restrict in enumerate(restrict_to):\n",
    "        r_model_name, r_phase, r_expe = restrict\n",
    "        if (r_model_name is None or r_model_name == model_name) and \\\n",
    "           (r_phase is None or r_phase == phase) and \\\n",
    "           (r_expe is None or r_expe == expe):\n",
    "            return True, idx  # Return True and the index\n",
    "    return False, None  # Return False and None if no match is found\n",
    "\n",
    "def can_be_rounded(x, ratio):\n",
    "    return abs(x / ratio) % 1 <= 0.05\n",
    "\n",
    "def format_big_integer(x):\n",
    "    if x <= 1000: return str(int(x))\n",
    "    if x <= 950_000 and can_be_rounded(x, 1000): return f\"{x / 1_000:.0f}K\"\n",
    "    if x <= 950_000_000 and can_be_rounded(x, 1_000_000): return f\"{x / 1_000_000:.0f}M\"\n",
    "    if x <= 950_000_000_000 and can_be_rounded(x, 1_000_000_000): return f\"{x / 1_000_000_000:.0f}B\"\n",
    "    if x <= 950_000_000_000: return f\"{x / 1_000_000_000:.1f}B\"\n",
    "    if can_be_rounded(x, 1_000_000_000_000): return f\"{x / 1_000_000_000_000:.0f}T\"\n",
    "    return f\"{x / 1_000_000_000_000:.1f}T\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(model_name, training_phase, expe_name):\n",
    "    if model_name == 'Baseline':\n",
    "        if 'main' in training_phase:\n",
    "            return f\"{expe_name}\"\n",
    "        if 'instruct' in training_phase:\n",
    "            return f\"{expe_name}-Instruct\"\n",
    "    elif model_name == 'Lucie-7B':\n",
    "        if 'main' in training_phase:\n",
    "            return f\"{model_name}-Pretraining\"\n",
    "        if 'extension' in training_phase:\n",
    "            return f\"{model_name}-Extension ({expe_name})\"\n",
    "        if 'annealing' in training_phase:\n",
    "            return f\"{model_name}-Annealing ({expe_name})\"\n",
    "        if 'instruct' in training_phase:\n",
    "            if 'lora' in training_phase:\n",
    "                return f\"{model_name}-Instruct (Lora - {expe_name})\"\n",
    "            if 'full' in training_phase:\n",
    "                return f\"{model_name}-Instruct (Full - {expe_name})\"\n",
    "            if 'deprecated' in training_phase:\n",
    "                return f\"{model_name}-Instruct (Full - Deprecated)\"\n",
    "            return f\"{model_name}-Instruct ({expe_name})\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(lucie_results, title, benchmark_names, filename, restrict_to=None, bar_plot=False, add_values_on_bar_plot=False, xlogscale=True, display=False):\n",
    "    lucie_results['allowed'], lucie_results['matched_index'] = zip(*lucie_results.apply(\n",
    "        lambda row: is_valid_with_index(row['model_name'], row['training_phase'], row['expe_name'], restrict_to), \n",
    "        axis=1\n",
    "    ))\n",
    "    lucie_results = lucie_results[lucie_results['allowed']].sort_values('matched_index', ascending=True)\n",
    "\n",
    "    ncols = min(2, len(benchmark_names))\n",
    "    nrows = (len(benchmark_names) + 1) // ncols\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(3 * ncols, 3.5 * nrows))\n",
    "    axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=16, weight='bold', y=0.9)\n",
    "\n",
    "    if len(lucie_results['expe_name'].unique()) > 20:\n",
    "        warnings.warn(\"More than 20 experiments, using a larger color palette.\")\n",
    "\n",
    "    palette = sns.color_palette(\"tab20\", n_colors=20)\n",
    "    color_mapping = {expe: palette[i % 20] for i, expe in enumerate(lucie_results['expe_name'].unique())}\n",
    "    xlabel = True\n",
    "    norm_tokens = 1e09 if not xlogscale else 1\n",
    "\n",
    "    benchmark2score = dict(zip(lucie_results['dataset'], lucie_results['score_name']))\n",
    "    benchmark2fewshot = dict(zip(lucie_results['dataset'], lucie_results['num_fewshot']))\n",
    "\n",
    "    for i_bench, (benchmark_name, ax) in enumerate(zip(benchmark_names, axes)):\n",
    "        lucie_results_bench = lucie_results[lucie_results[\"dataset\"] == benchmark_name].copy()\n",
    "\n",
    "        title = f\"{benchmark_name}\\n\"\n",
    "        if benchmark_name in benchmark2score:\n",
    "            score_name = benchmark2score[benchmark_name].replace(\",none\", \"\")\n",
    "            title += f\"({score_name}\"\n",
    "        if benchmark_name in benchmark2fewshot:\n",
    "            num_fewshot = benchmark2fewshot[benchmark_name]\n",
    "            if num_fewshot:\n",
    "                try:\n",
    "                    num_fewshot = int(num_fewshot)\n",
    "                    title += f\", {num_fewshot}-shot\"\n",
    "                except Exception:\n",
    "                    num_fewshot = None\n",
    "        title += \")\"\n",
    "        ax.set_title(title, weight='bold')\n",
    "\n",
    "        for (model_name, phase, expe), df in lucie_results_bench.groupby(['model_name', 'training_phase', 'expe_name'], sort=False):\n",
    "            df.sort_values(by=['training_tokens', 'add_bos_token', 'chat_template', 'fewshot_as_multiturn'],\n",
    "                           ascending=[True, True, False, False], inplace=True)\n",
    "            label = create_label(model_name, phase, expe)\n",
    "            \n",
    "            if len(df) > 1 and phase != \"1_main\":\n",
    "                warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
    "                df = df.iloc[[-1]]\n",
    "                selected_data = df[['model_name', 'training_phase', 'expe_name', 'training_tokens', 'add_bos_token', 'chat_template', 'fewshot_as_multiturn']]\n",
    "                formatted_data = \"\\n\".join([f\"{col}: {value}\" for col, value in selected_data.iloc[0].items()])\n",
    "                warnings.warn(f\"Selecting: \\n{formatted_data}\")\n",
    "            if phase not in marker_types:\n",
    "                raise ValueError(f\"Unknown phase: {phase}. Please define a marker for this phase.\")\n",
    "            if phase not in pattern_types:\n",
    "                raise ValueError(f\"Unknown phase: {phase}. Please define a pattern for this phase.\")\n",
    "            marker = marker_types[phase]\n",
    "            pattern = pattern_types[phase]\n",
    "            color = color_mapping.get(expe, 'black')\n",
    "\n",
    "            if bar_plot:\n",
    "                df = df.iloc[-1]\n",
    "                bars = ax.bar(label, df['score'], yerr=df['stderr'], label=label, \n",
    "                            color=color, edgecolor='grey', hatch=pattern, error_kw={'ecolor': 'grey', 'elinewidth': 1.5})\n",
    "                \n",
    "                # Add labels on top of bars\n",
    "                if add_values_on_bar_plot:\n",
    "                    ax.set_ylim([0, max(df['score'] * 1.2, ax.get_ylim()[1])])  # Increase upper limit by 20%\n",
    "                    for bar in bars:\n",
    "                        ax.text(\n",
    "                            bar.get_x() + bar.get_width() / 2,\n",
    "                            bar.get_height() + 0.05 * (ax.get_ylim()[1] - ax.get_ylim()[0]),  # Adjust vertical position\n",
    "                            f\"{bar.get_height():.2f}\",  # Format to 2 decimal places\n",
    "                            ha=\"center\",\n",
    "                            va=\"bottom\",\n",
    "                            fontsize=10,\n",
    "                            weight=\"bold\",\n",
    "                            color=\"black\"\n",
    "                        )\n",
    "            else:\n",
    "                if len(df) > 1 and phase == \"1_main\":\n",
    "                    ax.plot(df['training_tokens'], df['score'], label=f\"{label} (checkpoints)\", color=color, linewidth=1)\n",
    "                    ax.plot(df.iloc[-1]['training_tokens'], df.iloc[-1]['score'], marker, label=label, color=color, linewidth=1)\n",
    "                else:\n",
    "                    ax.plot(df['training_tokens'], df['score'], marker, label=label, color=color)\n",
    "\n",
    "        if xlogscale:\n",
    "            ax.set_xscale('log')\n",
    "            xticks_coordinates, _ = ax.get_xticks(), ax.get_xticklabels()\n",
    "            ax.set_xticks(xticks_coordinates)\n",
    "            ax.set_xticklabels([format_big_integer(x) for x in xticks_coordinates])\n",
    "        \n",
    "        if bar_plot:\n",
    "            ax.set_xticks([])\n",
    "            ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "        else:\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "            ax.set_xlim(100*1e9, 20*1e12)\n",
    "            if xlabel:\n",
    "                ax.set_xlabel(\"# training tokens\" + (\" (in billions)\" if norm_tokens == 1e09 else \" (log scale)\"))\n",
    "    \n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', fontsize=10, ncol=3)\n",
    "    fig.tight_layout(rect=[0, 0.1, 1, 0.9])\n",
    "    \n",
    "    if filename:\n",
    "        print(f\"Saving {filename}...\")\n",
    "        fig.savefig(filename, facecolor='w', bbox_inches='tight')\n",
    "    \n",
    "    if display:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/lucie_chkpt/hf-leaderboard-v1.png...\n",
      "Saving ../figs/lucie_chkpt/hf-leaderboard-v2.png...\n",
      "Saving ../figs/lucie_chkpt/french-bench.png...\n",
      "Saving ../figs/lucie_chkpt/french-bench-generative.png...\n",
      "Saving ../figs/lucie_chkpt/multilingual-arc-benchmark.png...\n",
      "Saving ../figs/lucie_chkpt/mmmmlu.png...\n",
      "Saving ../figs/lucie_chkpt/hf-openllmfrenchleaderboard.png...\n"
     ]
    }
   ],
   "source": [
    "restrict_to = [\n",
    "    ('Lucie-7B', '1_main', None),\n",
    "    ('Baseline', None, None),\n",
    "]\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        directory = os.path.join(OUTPUT_FOLDER, 'lucie_chkpt')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = os.path.join(directory ,slugify.slugify(title) + \".png\")\n",
    "\n",
    "    plot_results(results, title, dataset_names, filename, restrict_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/lucie_pipeline_official/hf-leaderboard-v1.png...\n",
      "Saving ../figs/lucie_pipeline_official/hf-leaderboard-v2.png...\n",
      "Saving ../figs/lucie_pipeline_official/french-bench.png...\n",
      "Saving ../figs/lucie_pipeline_official/french-bench-generative.png...\n",
      "Saving ../figs/lucie_pipeline_official/multilingual-arc-benchmark.png...\n",
      "Saving ../figs/lucie_pipeline_official/mmmmlu.png...\n",
      "Saving ../figs/lucie_pipeline_official/hf-openllmfrenchleaderboard.png...\n"
     ]
    }
   ],
   "source": [
    "restrict_to = [\n",
    "    ('Lucie-7B', '1_main', None),\n",
    "    ('Lucie-7B', '2_extension', 'rope20M'),\n",
    "    ('Lucie-7B', '3_annealing', '5B_tokens-mix6'),\n",
    "]\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        directory = os.path.join(OUTPUT_FOLDER, 'lucie_pipeline_official')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = os.path.join(directory ,slugify.slugify(title) + \".png\")\n",
    "\n",
    "    plot_results(results, title, dataset_names, filename, restrict_to, bar_plot=True, xlogscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/lucie_pipeline_vs_baseline/hf-leaderboard-v1.png...\n",
      "Saving ../figs/lucie_pipeline_vs_baseline/hf-leaderboard-v2.png...\n",
      "Saving ../figs/lucie_pipeline_vs_baseline/french-bench.png...\n",
      "Saving ../figs/lucie_pipeline_vs_baseline/french-bench-generative.png...\n",
      "Saving ../figs/lucie_pipeline_vs_baseline/multilingual-arc-benchmark.png...\n",
      "Saving ../figs/lucie_pipeline_vs_baseline/mmmmlu.png...\n",
      "Saving ../figs/lucie_pipeline_vs_baseline/hf-openllmfrenchleaderboard.png...\n"
     ]
    }
   ],
   "source": [
    "restrict_to = [\n",
    "    ('Lucie-7B', '1_main', None),\n",
    "    ('Lucie-7B', '2_extension', 'rope20M'),\n",
    "    ('Lucie-7B', '3_annealing', '5B_tokens-mix6'),\n",
    "    ('Baseline', None, None),\n",
    "]\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        directory = os.path.join(OUTPUT_FOLDER, 'lucie_pipeline_vs_baseline')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = os.path.join(directory ,slugify.slugify(title) + \".png\")\n",
    "\n",
    "    plot_results(results, title, dataset_names, filename, restrict_to, bar_plot=True, xlogscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix4 in arc_challenge.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix4 in hellaswag.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix4 in mmlu.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix4 in mmlu_continuation.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix4 in winogrande.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix4 in gsm8k.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix4 in truthfulqa_mc2.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/lucie_annealing/hf-leaderboard-v1.png...\n",
      "Saving ../figs/lucie_annealing/hf-leaderboard-v2.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix1 in french_bench_arc_challenge.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix4 in french_bench_arc_challenge.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix1 in french_bench_hellaswag.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix4 in french_bench_hellaswag.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix1 in french_bench_grammar.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix4 in french_bench_grammar.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix1 in french_bench_vocab.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix4 in french_bench_vocab.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/lucie_annealing/french-bench.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix1 in french_bench_fquadv2_genq.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix1 in french_bench_multifquad.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix1 in french_bench_orangesum_abstract.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n",
      "/tmp/ipykernel_1561008/2516334958.py:52: UserWarning: Multiple results for Lucie-7B 3_annealing 5B_tokens-mix1 in french_bench_trivia.\n",
      "  warnings.warn(f\"Multiple results for {model_name} {phase} {expe} in {benchmark_name}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/lucie_annealing/french-bench-generative.png...\n",
      "Saving ../figs/lucie_annealing/multilingual-arc-benchmark.png...\n",
      "Saving ../figs/lucie_annealing/mmmmlu.png...\n",
      "Saving ../figs/lucie_annealing/hf-openllmfrenchleaderboard.png...\n"
     ]
    }
   ],
   "source": [
    "restrict_to = [\n",
    "    ('Lucie-7B', '1_main', None),\n",
    "    ('Lucie-7B', '2_extension', 'rope20M'),\n",
    "    ('Lucie-7B', '3_annealing', None),\n",
    "]\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        directory = os.path.join(OUTPUT_FOLDER, 'lucie_annealing')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = os.path.join(directory ,slugify.slugify(title) + \".png\")\n",
    "\n",
    "    plot_results(results, title, dataset_names, filename, restrict_to, bar_plot=True, xlogscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/lucie_instruct_full/hf-leaderboard-v1.png...\n",
      "Saving ../figs/lucie_instruct_full/hf-leaderboard-v2.png...\n",
      "Saving ../figs/lucie_instruct_full/french-bench.png...\n",
      "Saving ../figs/lucie_instruct_full/french-bench-generative.png...\n",
      "Saving ../figs/lucie_instruct_full/multilingual-arc-benchmark.png...\n",
      "Saving ../figs/lucie_instruct_full/mmmmlu.png...\n",
      "Saving ../figs/lucie_instruct_full/hf-openllmfrenchleaderboard.png...\n"
     ]
    }
   ],
   "source": [
    "restrict_to = [\n",
    "    ('Lucie-7B', '3_annealing', '5B_tokens-mix6'),\n",
    "    ('Lucie-7B', '4_instruct_full', None),\n",
    "]\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        directory = os.path.join(OUTPUT_FOLDER, 'lucie_instruct_full')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = os.path.join(directory ,slugify.slugify(title) + \".png\")\n",
    "\n",
    "    plot_results(results, title, dataset_names, filename, restrict_to, bar_plot=True, xlogscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/lucie_instruct_full_vs_baselines/hf-leaderboard-v1.png...\n",
      "Saving ../figs/lucie_instruct_full_vs_baselines/hf-leaderboard-v2.png...\n",
      "Saving ../figs/lucie_instruct_full_vs_baselines/french-bench.png...\n",
      "Saving ../figs/lucie_instruct_full_vs_baselines/french-bench-generative.png...\n",
      "Saving ../figs/lucie_instruct_full_vs_baselines/multilingual-arc-benchmark.png...\n",
      "Saving ../figs/lucie_instruct_full_vs_baselines/mmmmlu.png...\n",
      "Saving ../figs/lucie_instruct_full_vs_baselines/hf-openllmfrenchleaderboard.png...\n"
     ]
    }
   ],
   "source": [
    "restrict_to = [\n",
    "    ('Lucie-7B', '3_annealing', '5B_tokens-mix6'),\n",
    "    ('Lucie-7B', '4_instruct_full', None),\n",
    "    ('Baseline', '4_instruct_full', None),\n",
    "]\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        directory = os.path.join(OUTPUT_FOLDER, 'lucie_instruct_full_vs_baselines')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = os.path.join(directory ,slugify.slugify(title) + \".png\")\n",
    "\n",
    "    plot_results(results, title, dataset_names, filename, restrict_to, bar_plot=True, xlogscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/lucie_instruct_lora_vs_full_vs_baselines/hf-leaderboard-v1.png...\n",
      "Saving ../figs/lucie_instruct_lora_vs_full_vs_baselines/hf-leaderboard-v2.png...\n",
      "Saving ../figs/lucie_instruct_lora_vs_full_vs_baselines/french-bench.png...\n",
      "Saving ../figs/lucie_instruct_lora_vs_full_vs_baselines/french-bench-generative.png...\n",
      "Saving ../figs/lucie_instruct_lora_vs_full_vs_baselines/multilingual-arc-benchmark.png...\n",
      "Saving ../figs/lucie_instruct_lora_vs_full_vs_baselines/mmmmlu.png...\n",
      "Saving ../figs/lucie_instruct_lora_vs_full_vs_baselines/hf-openllmfrenchleaderboard.png...\n"
     ]
    }
   ],
   "source": [
    "restrict_to = [\n",
    "    #('Baseline', '4_instruct_full', None),\n",
    "    ('Lucie-7B', '1_main', None),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'mix1'),\n",
    "    ('Lucie-7B', '4_instruct_full', 'mix1'),\n",
    "    #('Lucie-7B', '4_instruct_lora', 'mix2a'),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'mix2b'),\n",
    "    ('Lucie-7B', '4_instruct_full', 'mix2b'),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'mix3'),\n",
    "    ('Lucie-7B', '4_instruct_full', 'mix3'),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'mix4'),\n",
    "    ('Lucie-7B', '4_instruct_full', 'mix4')\n",
    "    #('Lucie-7B', '4_instruct_full', 'mix5'),\n",
    "]\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        directory = os.path.join(OUTPUT_FOLDER, 'lucie_instruct_lora_vs_full_vs_baselines')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = os.path.join(directory ,slugify.slugify(title) + \".png\")\n",
    "\n",
    "    plot_results(results, title, dataset_names, filename, restrict_to, bar_plot=True, xlogscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/lucie_instruct_lora_comma_vs_nocomma/hf-leaderboard-v1.png...\n",
      "Saving ../figs/lucie_instruct_lora_comma_vs_nocomma/hf-leaderboard-v2.png...\n",
      "Saving ../figs/lucie_instruct_lora_comma_vs_nocomma/french-bench.png...\n",
      "Saving ../figs/lucie_instruct_lora_comma_vs_nocomma/french-bench-generative.png...\n",
      "Saving ../figs/lucie_instruct_lora_comma_vs_nocomma/multilingual-arc-benchmark.png...\n",
      "Saving ../figs/lucie_instruct_lora_comma_vs_nocomma/mmmmlu.png...\n",
      "Saving ../figs/lucie_instruct_lora_comma_vs_nocomma/hf-openllmfrenchleaderboard.png...\n"
     ]
    }
   ],
   "source": [
    "restrict_to = [\n",
    "    #('Baseline', '4_instruct_full', None),\n",
    "    #('Lucie-7B', '1_main', None),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'mix1'),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'R1'),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'mix2a'),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'R2a'),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'R2afp'),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'mix2b'),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'R2bfp'),\n",
    "    #('Lucie-7B', '4_instruct_lora', 'mix3'),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'mix4'),\n",
    "    ('Lucie-7B', '4_instruct_lora', 'R7m4fp')\n",
    "    #('Lucie-7B', '4_instruct_full', 'mix5'),\n",
    "]\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        directory = os.path.join(OUTPUT_FOLDER, 'lucie_instruct_lora_comma_vs_nocomma')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = os.path.join(directory ,slugify.slugify(title) + \".png\")\n",
    "\n",
    "    plot_results(results, title, dataset_names, filename, restrict_to, bar_plot=True, xlogscale=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ../figs/lucie_instruct_paper/hf-leaderboard-v1.png...\n",
      "Saving ../figs/lucie_instruct_paper/hf-leaderboard-v2.png...\n",
      "Saving ../figs/lucie_instruct_paper/french-bench.png...\n",
      "Saving ../figs/lucie_instruct_paper/french-bench-generative.png...\n",
      "Saving ../figs/lucie_instruct_paper/multilingual-arc-benchmark.png...\n",
      "Saving ../figs/lucie_instruct_paper/mmmmlu.png...\n",
      "Saving ../figs/lucie_instruct_paper/hf-openllmfrenchleaderboard.png...\n"
     ]
    }
   ],
   "source": [
    "restrict_to = [\n",
    "    ('Baseline', '4_instruct_full', None),\n",
    "    #('Baseline', '4_instruct_full', None),\n",
    "    ('Lucie-7B', '1_main', None),\n",
    "    ('Lucie-7B', '4_instruct_full', 'mix1'), # Lucie-7B-Instruct-human-data\n",
    "    #('Lucie-7B', '4_instruct_lora', 'R1'),\n",
    "    #('Lucie-7B', '4_instruct_full', 'mix2b'),\n",
    "    #('Lucie-7B', '4_instruct_lora', 'R2a'),\n",
    "    #('Lucie-7B', '4_instruct_lora', 'R2afp'),\n",
    "    #('Lucie-7B', '4_instruct_full', 'mix'),\n",
    "    #('Lucie-7B', '4_instruct_lora', 'R2bfp'),\n",
    "    #('Lucie-7B', '4_instruct_lora', 'mix3'),\n",
    "    #('Lucie-7B', '4_instruct_full', 'mix3'),\n",
    "    ('Lucie-7B', '4_instruct_full', 'mix4'), # Lucie-7B-Instruct\n",
    "    #('Lucie-7B', '4_instruct_lora', 'R7m4fp')\n",
    "    #('Lucie-7B', '4_instruct_full', 'mix5'),\n",
    "]\n",
    "\n",
    "for title, dataset_names in BENCHMARKS.items():\n",
    "\n",
    "    filename = None\n",
    "    if OUTPUT_FOLDER:\n",
    "        directory = os.path.join(OUTPUT_FOLDER, 'lucie_instruct_paper')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = os.path.join(directory ,slugify.slugify(title) + \".png\")\n",
    "\n",
    "    plot_results(results, title, dataset_names, filename, restrict_to, bar_plot=True, xlogscale=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
