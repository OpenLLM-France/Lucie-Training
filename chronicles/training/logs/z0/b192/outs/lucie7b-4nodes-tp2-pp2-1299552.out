++ date
+ echo 'START TIME: Thu Mar 28 23:36:51 CET 2024'
START TIME: Thu Mar 28 23:36:51 CET 2024
+ variant=main
+ CHECKPOINT_PATH=/gpfswork/rech/qgz/urc37ho/checkpoints/
+ LOGS_PATH=/gpfswork/rech/qgz/urc37ho/lucie-logs
+ MEGATRON_DEEPSPEED_REPO=/gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed
+ cd /gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed
+ DATASET=/gpfsscratch/rech/qgz/commun/preprocessed_data/Lucie/lucie_tokens_2.4-space_prefix_all/Wikipedia--fr--026_text_document
+ TOKENIZER_PATH=OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all
++ head -n 1
++ scontrol show hostnames 'jean-zay-iam[04,34,37,49]'
+ MASTER_ADDR=jean-zay-iam04
+ MASTER_PORT=6000
+ GPUS_PER_NODE=8
+ NNODES=4
+ PP=2
+ TP=2
+ MICRO_BATCH_SIZE=6
+ GLOBAL_BATCH_SIZE=192
+ HIDDEN_SIZE=4096
+ FFN_HIDDEN_SIZE=11008
+ NUM_LAYERS=32
+ NUM_HEADS=32
+ SEQ_LENGTH=2048
+ NUM_KV_HEADS=32
+ TRAIN_STEPS=250000
+ LR=3e-4
+ MIN_LR=3e-5
+ LR_WARMUP_STEPS=2000
+ WEIGHT_DECAY=0.1
+ GRAD_CLIP=1
+ SAVE_INTERVAL=100
+ OPTIMIZER_ARGS='     --lr 3e-4     --lr-decay-style cosine     --min-lr 3e-5     --weight-decay 0.1     --clip-grad 1     --lr-warmup-iters 2000     --optimizer adam     --adam-beta1 0.9     --adam-beta2 0.95     '
+ EXIT_OPTS='     --exit-duration-in-mins 1190     '
+ GPT_ARGS='     --data-cache-path ~/.cache     --tensor-model-parallel-size 4     --pipeline-model-parallel-size 4     --num-layers 32     --hidden-size 4096     --ffn-hidden-size 11008     --num-attention-heads 32     --micro-batch-size 6     --global-batch-size 192     --seq-length 2048     --max-position-embeddings 2048     --train-iters 250000     --tokenizer-type PretrainedFromHF      --tokenizer-name-or-path OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all     --bf16     --use-flash-attn-v2     --no-query-key-layer-scaling     --attention-dropout 0     --hidden-dropout 0     --use-rotary-position-embeddings     --untie-embeddings-and-output-weights     --swiglu     --normalization rmsnorm     --disable-bias-linear     --num-key-value-heads 32
         --lr 3e-4     --lr-decay-style cosine     --min-lr 3e-5     --weight-decay 0.1     --clip-grad 1     --lr-warmup-iters 2000     --optimizer adam     --adam-beta1 0.9     --adam-beta2 0.95               --exit-duration-in-mins 1190          '
+ OUTPUT_ARGS='     --log-interval 1     --save-interval 100     --eval-interval 1000     --eval-iters 1     '
+ ZERO_STAGE=0
+ DS_CONFIG=./ds_config.1299552.json
+ activation_checkpoint=false
+ cat
+ ds_args=
+ ds_args=' --deepspeed '
+ ds_args=' --deepspeed_config=./ds_config.1299552.json  --deepspeed '
+ ds_args=' --zero-stage=0  --deepspeed_config=./ds_config.1299552.json  --deepspeed '
+ '[' false = true ']'
+ export 'LAUNCHER=python -u -m torch.distributed.run     --nproc_per_node 8     --nnodes 4     --rdzv_endpoint jean-zay-iam04:6000     --rdzv_backend c10d     --max_restarts 0     --tee 3     '
+ LAUNCHER='python -u -m torch.distributed.run     --nproc_per_node 8     --nnodes 4     --rdzv_endpoint jean-zay-iam04:6000     --rdzv_backend c10d     --max_restarts 0     --tee 3     '
++ pwd
+ export 'CMD=     /gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed/pretrain_gpt.py     --tensor-model-parallel-size      --pipeline-model-parallel-size           --data-cache-path ~/.cache     --tensor-model-parallel-size 4     --pipeline-model-parallel-size 4     --num-layers 32     --hidden-size 4096     --ffn-hidden-size 11008     --num-attention-heads 32     --micro-batch-size 6     --global-batch-size 192     --seq-length 2048     --max-position-embeddings 2048     --train-iters 250000     --tokenizer-type PretrainedFromHF      --tokenizer-name-or-path OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all     --bf16     --use-flash-attn-v2     --no-query-key-layer-scaling     --attention-dropout 0     --hidden-dropout 0     --use-rotary-position-embeddings     --untie-embeddings-and-output-weights     --swiglu     --normalization rmsnorm     --disable-bias-linear     --num-key-value-heads 32
         --lr 3e-4     --lr-decay-style cosine     --min-lr 3e-5     --weight-decay 0.1     --clip-grad 1     --lr-warmup-iters 2000     --optimizer adam     --adam-beta1 0.9     --adam-beta2 0.95               --exit-duration-in-mins 1190                    --log-interval 1     --save-interval 100     --eval-interval 1000     --eval-iters 1          --save /gpfswork/rech/qgz/urc37ho/checkpoints/     --load /gpfswork/rech/qgz/urc37ho/checkpoints/     --data-impl mmap     --distributed-backend nccl       --zero-stage=0  --deepspeed_config=./ds_config.1299552.json  --deepspeed      '
+ CMD='     /gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed/pretrain_gpt.py     --tensor-model-parallel-size      --pipeline-model-parallel-size           --data-cache-path ~/.cache     --tensor-model-parallel-size 4     --pipeline-model-parallel-size 4     --num-layers 32     --hidden-size 4096     --ffn-hidden-size 11008     --num-attention-heads 32     --micro-batch-size 6     --global-batch-size 192     --seq-length 2048     --max-position-embeddings 2048     --train-iters 250000     --tokenizer-type PretrainedFromHF      --tokenizer-name-or-path OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all     --bf16     --use-flash-attn-v2     --no-query-key-layer-scaling     --attention-dropout 0     --hidden-dropout 0     --use-rotary-position-embeddings     --untie-embeddings-and-output-weights     --swiglu     --normalization rmsnorm     --disable-bias-linear     --num-key-value-heads 32
         --lr 3e-4     --lr-decay-style cosine     --min-lr 3e-5     --weight-decay 0.1     --clip-grad 1     --lr-warmup-iters 2000     --optimizer adam     --adam-beta1 0.9     --adam-beta2 0.95               --exit-duration-in-mins 1190                    --log-interval 1     --save-interval 100     --eval-interval 1000     --eval-iters 1          --save /gpfswork/rech/qgz/urc37ho/checkpoints/     --load /gpfswork/rech/qgz/urc37ho/checkpoints/     --data-impl mmap     --distributed-backend nccl       --zero-stage=0  --deepspeed_config=./ds_config.1299552.json  --deepspeed      '
+ echo /gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed/pretrain_gpt.py --tensor-model-parallel-size --pipeline-model-parallel-size --data-cache-path '~/.cache' --tensor-model-parallel-size 4 --pipeline-model-parallel-size 4 --num-layers 32 --hidden-size 4096 --ffn-hidden-size 11008 --num-attention-heads 32 --micro-batch-size 6 --global-batch-size 192 --seq-length 2048 --max-position-embeddings 2048 --train-iters 250000 --tokenizer-type PretrainedFromHF --tokenizer-name-or-path OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all --bf16 --use-flash-attn-v2 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 32 --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 2000 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --exit-duration-in-mins 1190 --log-interval 1 --save-interval 100 --eval-interval 1000 --eval-iters 1 --save /gpfswork/rech/qgz/urc37ho/checkpoints/ --load /gpfswork/rech/qgz/urc37ho/checkpoints/ --data-impl mmap --distributed-backend nccl --zero-stage=0 --deepspeed_config=./ds_config.1299552.json --deepspeed
/gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed/pretrain_gpt.py --tensor-model-parallel-size --pipeline-model-parallel-size --data-cache-path ~/.cache --tensor-model-parallel-size 4 --pipeline-model-parallel-size 4 --num-layers 32 --hidden-size 4096 --ffn-hidden-size 11008 --num-attention-heads 32 --micro-batch-size 6 --global-batch-size 192 --seq-length 2048 --max-position-embeddings 2048 --train-iters 250000 --tokenizer-type PretrainedFromHF --tokenizer-name-or-path OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all --bf16 --use-flash-attn-v2 --no-query-key-layer-scaling --attention-dropout 0 --hidden-dropout 0 --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization rmsnorm --disable-bias-linear --num-key-value-heads 32 --lr 3e-4 --lr-decay-style cosine --min-lr 3e-5 --weight-decay 0.1 --clip-grad 1 --lr-warmup-iters 2000 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --exit-duration-in-mins 1190 --log-interval 1 --save-interval 100 --eval-interval 1000 --eval-iters 1 --save /gpfswork/rech/qgz/urc37ho/checkpoints/ --load /gpfswork/rech/qgz/urc37ho/checkpoints/ --data-impl mmap --distributed-backend nccl --zero-stage=0 --deepspeed_config=./ds_config.1299552.json --deepspeed
+ export CUDA_LAUNCH_BLOCKING=1
+ CUDA_LAUNCH_BLOCKING=1
+ export TORCHELASTIC_ERROR_FILE=/tmp/torch-elastic-error.json
+ TORCHELASTIC_ERROR_FILE=/tmp/torch-elastic-error.json
+ module purge
+ unset _mlshdbg
+ '[' 0 = 1 ']'
+ unset _mlre _mlIFS
+ '[' -n x ']'
+ _mlIFS=' 	
'
+ IFS=' '
+ for _mlv in ${MODULES_RUN_QUARANTINE:-}
+ '[' LD_LIBRARY_PATH = LD_LIBRARY_PATH -a LD_LIBRARY_PATH = LD_LIBRARY_PATH ']'
++ eval 'echo ${LD_LIBRARY_PATH+x}'
+++ echo x
+ '[' -n x ']'
++ eval 'echo ${LD_LIBRARY_PATH}'
+++ echo /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' '
+ _mlrv=MODULES_RUNENV_LD_LIBRARY_PATH
++ eval 'echo ${MODULES_RUNENV_LD_LIBRARY_PATH:-}'
+++ echo
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' '
+ '[' -n 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' ' ']'
++ eval 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\''' 'LD_LIBRARY_PATH='\'''\''' /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash '"$@"'
+++ LD_LIBRARY_PATH_modquar=/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+++ LD_LIBRARY_PATH=
+++ /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash purge
+ eval
+ _mlstatus=0
+ '[' -n x ']'
+ IFS=' 	
'
+ unset _mlre _mlv _mlrv _mlIFS
+ '[' -n '' ']'
+ unset _mlshdbg
+ return 0
+ module load cpuarch/amd
+ unset _mlshdbg
+ '[' 0 = 1 ']'
+ unset _mlre _mlIFS
+ '[' -n x ']'
+ _mlIFS=' 	
'
+ IFS=' '
+ for _mlv in ${MODULES_RUN_QUARANTINE:-}
+ '[' LD_LIBRARY_PATH = LD_LIBRARY_PATH -a LD_LIBRARY_PATH = LD_LIBRARY_PATH ']'
++ eval 'echo ${LD_LIBRARY_PATH+x}'
+++ echo x
+ '[' -n x ']'
++ eval 'echo ${LD_LIBRARY_PATH}'
+++ echo /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' '
+ _mlrv=MODULES_RUNENV_LD_LIBRARY_PATH
++ eval 'echo ${MODULES_RUNENV_LD_LIBRARY_PATH:-}'
+++ echo
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' '
+ '[' -n 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' ' ']'
++ eval 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\''' 'LD_LIBRARY_PATH='\'''\''' /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash '"$@"'
+++ LD_LIBRARY_PATH_modquar=/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+++ LD_LIBRARY_PATH=
+++ /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash load cpuarch/amd
+ eval '_LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1;' export '_LMFILES__modshare;
LOADEDMODULES_modshare=cpuarch/amd:1;' export 'LOADEDMODULES_modshare;
unset' 'MODULEPATH_modshare;
_LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd;' export '_LMFILES_;
LOADEDMODULES=cpuarch/amd;' export 'LOADEDMODULES;
MODULEPATH=/gpfslocalsup/pub/module-rh/modulefiles:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64;' export 'MODULEPATH;
test' '0;'
++ _LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1
++ export _LMFILES__modshare
++ LOADEDMODULES_modshare=cpuarch/amd:1
++ export LOADEDMODULES_modshare
++ unset MODULEPATH_modshare
++ _LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd
++ export _LMFILES_
++ LOADEDMODULES=cpuarch/amd
++ export LOADEDMODULES
++ MODULEPATH=/gpfslocalsup/pub/module-rh/modulefiles:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64
++ export MODULEPATH
++ test 0
+ _mlstatus=0
+ '[' -n x ']'
+ IFS=' 	
'
+ unset _mlre _mlv _mlrv _mlIFS
+ '[' -n '' ']'
+ unset _mlshdbg
+ return 0
+ module load anaconda-py3/2023.09
+ unset _mlshdbg
+ '[' 0 = 1 ']'
+ unset _mlre _mlIFS
+ '[' -n x ']'
+ _mlIFS=' 	
'
+ IFS=' '
+ for _mlv in ${MODULES_RUN_QUARANTINE:-}
+ '[' LD_LIBRARY_PATH = LD_LIBRARY_PATH -a LD_LIBRARY_PATH = LD_LIBRARY_PATH ']'
++ eval 'echo ${LD_LIBRARY_PATH+x}'
+++ echo x
+ '[' -n x ']'
++ eval 'echo ${LD_LIBRARY_PATH}'
+++ echo /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' '
+ _mlrv=MODULES_RUNENV_LD_LIBRARY_PATH
++ eval 'echo ${MODULES_RUNENV_LD_LIBRARY_PATH:-}'
+++ echo
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' '
+ '[' -n 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' ' ']'
++ eval 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\''' 'LD_LIBRARY_PATH='\'''\''' /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash '"$@"'
+++ LD_LIBRARY_PATH_modquar=/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+++ LD_LIBRARY_PATH=
+++ /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash load anaconda-py3/2023.09
+ eval '_LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:1;' export '_LMFILES__modshare;
LOADEDMODULES_modshare=anaconda-py3/2023.09:1:cpuarch/amd:1;' export 'LOADEDMODULES_modshare;
MODULES_LMCONFLICT_modshare=anaconda-py3/2023.09\&anaconda-py3\&anaconda-py2:1;' export 'MODULES_LMCONFLICT_modshare;
_LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09;' export '_LMFILES_;
LOADEDMODULES=cpuarch/amd:anaconda-py3/2023.09;' export 'LOADEDMODULES;
MODULES_LMCONFLICT=anaconda-py3/2023.09\&anaconda-py3\&anaconda-py2;' export 'MODULES_LMCONFLICT;
.' '/gpfslocalsup/pub/anaconda-py3/2023.09/etc/profile.d/conda.sh;
conda' activate 'base;
test' '0;'
++ _LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:1
++ export _LMFILES__modshare
++ LOADEDMODULES_modshare=anaconda-py3/2023.09:1:cpuarch/amd:1
++ export LOADEDMODULES_modshare
++ MODULES_LMCONFLICT_modshare='anaconda-py3/2023.09&anaconda-py3&anaconda-py2:1'
++ export MODULES_LMCONFLICT_modshare
++ _LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09
++ export _LMFILES_
++ LOADEDMODULES=cpuarch/amd:anaconda-py3/2023.09
++ export LOADEDMODULES
++ MODULES_LMCONFLICT='anaconda-py3/2023.09&anaconda-py3&anaconda-py2'
++ export MODULES_LMCONFLICT
++ . /gpfslocalsup/pub/anaconda-py3/2023.09/etc/profile.d/conda.sh
+++ export CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
+++ CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
+++ CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
++++ dirname /gpfslocalsup/pub/anaconda-py3/2023.09/bin
+++ PATH=/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate base
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate base
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate base
+++ /gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda shell.posix activate base
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin'\''
export CONDA_PREFIX='\''/gpfslocalsup/pub/anaconda-py3/2023.09'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin'\''
export CONDA_PREFIX='\''/gpfslocalsup/pub/anaconda-py3/2023.09'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/gpfslocalsup/pub/anaconda-py3/2023.09/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
+++ PATH=/gpfslocalsup/pub/anaconda-py3/2023.09/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
+++ export CONDA_PREFIX=/gpfslocalsup/pub/anaconda-py3/2023.09
+++ CONDA_PREFIX=/gpfslocalsup/pub/anaconda-py3/2023.09
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
+++ CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
+++ CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
++ test 0
+ _mlstatus=0
+ '[' -n x ']'
+ IFS=' 	
'
+ unset _mlre _mlv _mlrv _mlIFS
+ '[' -n '' ']'
+ unset _mlshdbg
+ return 0
+ conda activate lucie-torch211
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate lucie-torch211
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate lucie-torch211
++ /gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda shell.posix activate lucie-torch211
+ ask_conda='PS1='\''(lucie-torch211) '\''
export PATH='\''/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin'\''
export CONDA_PREFIX='\''/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''lucie-torch211'\''
export CONDA_PROMPT_MODIFIER='\''(lucie-torch211) '\''
export CONDA_PREFIX_1='\''/gpfslocalsup/pub/anaconda-py3/2023.09'\''
export CONDA_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python'\''
. "/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libblas_mkl_activate.sh"
. "/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libxml2_activate.sh"'
+ eval 'PS1='\''(lucie-torch211) '\''
export PATH='\''/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin'\''
export CONDA_PREFIX='\''/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''lucie-torch211'\''
export CONDA_PROMPT_MODIFIER='\''(lucie-torch211) '\''
export CONDA_PREFIX_1='\''/gpfslocalsup/pub/anaconda-py3/2023.09'\''
export CONDA_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python'\''
. "/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libblas_mkl_activate.sh"
. "/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libxml2_activate.sh"'
++ PS1='(lucie-torch211) '
++ export PATH=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
++ PATH=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
++ export CONDA_PREFIX=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211
++ CONDA_PREFIX=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=lucie-torch211
++ CONDA_DEFAULT_ENV=lucie-torch211
++ export 'CONDA_PROMPT_MODIFIER=(lucie-torch211) '
++ CONDA_PROMPT_MODIFIER='(lucie-torch211) '
++ export CONDA_PREFIX_1=/gpfslocalsup/pub/anaconda-py3/2023.09
++ CONDA_PREFIX_1=/gpfslocalsup/pub/anaconda-py3/2023.09
++ export CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
++ CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
++ CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
++ . /linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libblas_mkl_activate.sh
+++ export CONDA_MKL_INTERFACE_LAYER_BACKUP=
+++ CONDA_MKL_INTERFACE_LAYER_BACKUP=
+++ export MKL_INTERFACE_LAYER=LP64,GNU
+++ MKL_INTERFACE_LAYER=LP64,GNU
++ . /linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libxml2_activate.sh
+++ test -n ''
+++ xml_catalog_files_libxml2=
+++ XML_CATALOG_FILES=
+++ conda_catalog_files=
+++ ifs_libxml2=' 	
'
+++ IFS=' '
+++ rem=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211
+++ for pre in ${rem}
+++ test '' = /linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211
+++ conda_catalog_files=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211
+++ rem=
+++ IFS=' 	
'
+++ conda_catalog_files='file:///linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/xml/catalog file:///etc/xml/catalog'
+++ export 'XML_CATALOG_FILES=file:///linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/xml/catalog file:///etc/xml/catalog'
+++ XML_CATALOG_FILES='file:///linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/xml/catalog file:///etc/xml/catalog'
+++ unset conda_catalog_files ifs_libxml2 rem
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ module load cuda/12.1.0
+ unset _mlshdbg
+ '[' 0 = 1 ']'
+ unset _mlre _mlIFS
+ '[' -n x ']'
+ _mlIFS=' 	
'
+ IFS=' '
+ for _mlv in ${MODULES_RUN_QUARANTINE:-}
+ '[' LD_LIBRARY_PATH = LD_LIBRARY_PATH -a LD_LIBRARY_PATH = LD_LIBRARY_PATH ']'
++ eval 'echo ${LD_LIBRARY_PATH+x}'
+++ echo x
+ '[' -n x ']'
++ eval 'echo ${LD_LIBRARY_PATH}'
+++ echo /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' '
+ _mlrv=MODULES_RUNENV_LD_LIBRARY_PATH
++ eval 'echo ${MODULES_RUNENV_LD_LIBRARY_PATH:-}'
+++ echo
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' '
+ '[' -n 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' ' ']'
++ eval 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\''' 'LD_LIBRARY_PATH='\'''\''' /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash '"$@"'
+++ LD_LIBRARY_PATH_modquar=/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+++ LD_LIBRARY_PATH=
+++ /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash load cuda/12.1.0
+ eval 'LD_LIBRARY_PATH=/gpfslocalsys/cuda/12.1.0/nvvm/lib64:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:/gpfslocalsys/cuda/12.1.0/lib64:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib;' export 'LD_LIBRARY_PATH;
MANPATH=/gpfslocalsys/cuda/12.1.0/doc/man::/opt/c3/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man:/gpfslocalsys/slurm/current/share/man:/usr/share/catman:/usr/share/man:/usr/catman:/usr/man;' export 'MANPATH;
LIBRARY_PATH=/gpfslocalsys/cuda/12.1.0/lib64/stubs:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:/gpfslocalsys/cuda/12.1.0/lib64:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib;' export 'LIBRARY_PATH;
_LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:/gpfslocalsup/pub/module-rh/modulefiles/cuda/12.1.0;' export '_LMFILES_;
LOADEDMODULES=cpuarch/amd:anaconda-py3/2023.09:cuda/12.1.0;' export 'LOADEDMODULES;
MODULES_LMCONFLICT=anaconda-py3/2023.09\&anaconda-py3\&anaconda-py2:cuda/12.1.0\&cuda;' export 'MODULES_LMCONFLICT;
MANPATH_modshare=:1:/opt/sgi/share/man:1:/gpfslocalsys/slurm/current/share/man:1:/opt/c3/man:1:/opt/clmgr/lib/cm-cli/man:1:/opt/clmgr/share/man:1:/usr/man:1:/usr/catman:1:/opt/clmgr/man:1:/gpfslocalsys/cuda/12.1.0/doc/man:1:/usr/share/man:1:/usr/share/catman:1;' export 'MANPATH_modshare;
NVHPC_CUDA_HOME=/gpfslocalsys/cuda/12.1.0;' export 'NVHPC_CUDA_HOME;
LIBRARY_PATH_modshare=/gpfslocalsys/cuda/12.1.0/lib64/stubs:1:/gpfslocalsys/cuda/12.1.0/lib64:1:/gpfslocalsys/slurm/current/lib/slurm:1:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:1:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:1:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:1:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:1:/gpfslocalsys/slurm/current/lib:1;' export 'LIBRARY_PATH_modshare;
MODULES_LMCONFLICT_modshare=anaconda-py3/2023.09\&anaconda-py3\&anaconda-py2:1:cuda/12.1.0\&cuda:1;' export 'MODULES_LMCONFLICT_modshare;
CPLUS_INCLUDE_PATH=/gpfslocalsys/cuda/12.1.0/include;' export 'CPLUS_INCLUDE_PATH;
CUDA_INSTALL_PATH=/gpfslocalsys/cuda/12.1.0;' export 'CUDA_INSTALL_PATH;
CUDA_ROOT=/gpfslocalsys/cuda/12.1.0;' export 'CUDA_ROOT;
CUDA_PATH=/gpfslocalsys/cuda/12.1.0;' export 'CUDA_PATH;
LD_LIBRARY_PATH_modshare=/gpfslocalsys/cuda/12.1.0/lib64:1:/gpfslocalsys/slurm/current/lib/slurm:1:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:1:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:1:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:1:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:1:/gpfslocalsys/slurm/current/lib:1;' export 'LD_LIBRARY_PATH_modshare;
C_INCLUDE_PATH=/gpfslocalsys/cuda/12.1.0/include;' export 'C_INCLUDE_PATH;
_LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1:/gpfslocalsup/pub/module-rh/modulefiles/cuda/12.1.0:1:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:1;' export '_LMFILES__modshare;
LOADEDMODULES_modshare=cuda/12.1.0:1:cpuarch/amd:1:anaconda-py3/2023.09:1;' export 'LOADEDMODULES_modshare;
PATH=/gpfslocalsys/cuda/12.1.0/samples:/gpfslocalsys/cuda/12.1.0/nvvm/bin:/gpfslocalsys/cuda/12.1.0/bin:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin;' export 'PATH;
CUDA_HOME=/gpfslocalsys/cuda/12.1.0;' export 'CUDA_HOME;
PATH_modshare=/usr/bin:1:/gpfslocalsup/bin:1:/usr/local/bin:1:/gpfslocalsys/cuda/12.1.0/bin:1:/opt/sgi/bin:1:/gpfslocalsys/slurm/current/bin:1:/opt/clmgr/bin:1:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:1:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:1:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:1:/opt/sgi/sbin:1:/gpfslocalsys/cuda/12.1.0/samples:1:/bin:1:/opt/clmgr/sbin:1:/gpfslocalsys/bin:1:/gpfslocalsys/cuda/12.1.0/nvvm/bin:1:/sbin:1:/usr/sbin:1:/usr/local/sbin:1:/usr/lpp/mmfs/bin:1:/opt/c3/bin:1;' export 'PATH_modshare;
test' '0;'
++ LD_LIBRARY_PATH=/gpfslocalsys/cuda/12.1.0/nvvm/lib64:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:/gpfslocalsys/cuda/12.1.0/lib64:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
++ export LD_LIBRARY_PATH
++ MANPATH=/gpfslocalsys/cuda/12.1.0/doc/man::/opt/c3/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man:/gpfslocalsys/slurm/current/share/man:/usr/share/catman:/usr/share/man:/usr/catman:/usr/man
++ export MANPATH
++ LIBRARY_PATH=/gpfslocalsys/cuda/12.1.0/lib64/stubs:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:/gpfslocalsys/cuda/12.1.0/lib64:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
++ export LIBRARY_PATH
++ _LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:/gpfslocalsup/pub/module-rh/modulefiles/cuda/12.1.0
++ export _LMFILES_
++ LOADEDMODULES=cpuarch/amd:anaconda-py3/2023.09:cuda/12.1.0
++ export LOADEDMODULES
++ MODULES_LMCONFLICT='anaconda-py3/2023.09&anaconda-py3&anaconda-py2:cuda/12.1.0&cuda'
++ export MODULES_LMCONFLICT
++ MANPATH_modshare=:1:/opt/sgi/share/man:1:/gpfslocalsys/slurm/current/share/man:1:/opt/c3/man:1:/opt/clmgr/lib/cm-cli/man:1:/opt/clmgr/share/man:1:/usr/man:1:/usr/catman:1:/opt/clmgr/man:1:/gpfslocalsys/cuda/12.1.0/doc/man:1:/usr/share/man:1:/usr/share/catman:1
++ export MANPATH_modshare
++ NVHPC_CUDA_HOME=/gpfslocalsys/cuda/12.1.0
++ export NVHPC_CUDA_HOME
++ LIBRARY_PATH_modshare=/gpfslocalsys/cuda/12.1.0/lib64/stubs:1:/gpfslocalsys/cuda/12.1.0/lib64:1:/gpfslocalsys/slurm/current/lib/slurm:1:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:1:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:1:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:1:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:1:/gpfslocalsys/slurm/current/lib:1
++ export LIBRARY_PATH_modshare
++ MODULES_LMCONFLICT_modshare='anaconda-py3/2023.09&anaconda-py3&anaconda-py2:1:cuda/12.1.0&cuda:1'
++ export MODULES_LMCONFLICT_modshare
++ CPLUS_INCLUDE_PATH=/gpfslocalsys/cuda/12.1.0/include
++ export CPLUS_INCLUDE_PATH
++ CUDA_INSTALL_PATH=/gpfslocalsys/cuda/12.1.0
++ export CUDA_INSTALL_PATH
++ CUDA_ROOT=/gpfslocalsys/cuda/12.1.0
++ export CUDA_ROOT
++ CUDA_PATH=/gpfslocalsys/cuda/12.1.0
++ export CUDA_PATH
++ LD_LIBRARY_PATH_modshare=/gpfslocalsys/cuda/12.1.0/lib64:1:/gpfslocalsys/slurm/current/lib/slurm:1:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:1:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:1:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:1:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:1:/gpfslocalsys/slurm/current/lib:1
++ export LD_LIBRARY_PATH_modshare
++ C_INCLUDE_PATH=/gpfslocalsys/cuda/12.1.0/include
++ export C_INCLUDE_PATH
++ _LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1:/gpfslocalsup/pub/module-rh/modulefiles/cuda/12.1.0:1:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:1
++ export _LMFILES__modshare
++ LOADEDMODULES_modshare=cuda/12.1.0:1:cpuarch/amd:1:anaconda-py3/2023.09:1
++ export LOADEDMODULES_modshare
++ PATH=/gpfslocalsys/cuda/12.1.0/samples:/gpfslocalsys/cuda/12.1.0/nvvm/bin:/gpfslocalsys/cuda/12.1.0/bin:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
++ export PATH
++ CUDA_HOME=/gpfslocalsys/cuda/12.1.0
++ export CUDA_HOME
++ PATH_modshare=/usr/bin:1:/gpfslocalsup/bin:1:/usr/local/bin:1:/gpfslocalsys/cuda/12.1.0/bin:1:/opt/sgi/bin:1:/gpfslocalsys/slurm/current/bin:1:/opt/clmgr/bin:1:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:1:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:1:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:1:/opt/sgi/sbin:1:/gpfslocalsys/cuda/12.1.0/samples:1:/bin:1:/opt/clmgr/sbin:1:/gpfslocalsys/bin:1:/gpfslocalsys/cuda/12.1.0/nvvm/bin:1:/sbin:1:/usr/sbin:1:/usr/local/sbin:1:/usr/lpp/mmfs/bin:1:/opt/c3/bin:1
++ export PATH_modshare
++ test 0
+ _mlstatus=0
+ '[' -n x ']'
+ IFS=' 	
'
+ unset _mlre _mlv _mlrv _mlIFS
+ '[' -n '' ']'
+ unset _mlshdbg
+ return 0
+ DISTRIBUTED_ARGS='--nproc_per_node 8 --nnodes 4 --node_rank $SLURM_PROCID --rdzv_endpoint jean-zay-iam04:6000 --rdzv_backend c10d --max_restarts 0 --tee 3'
++ pwd
+ export 'RUN=torchrun --nproc_per_node 8 --nnodes 4 --node_rank $SLURM_PROCID --rdzv_endpoint jean-zay-iam04:6000 --rdzv_backend c10d --max_restarts 0 --tee 3        /gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed/pretrain_gpt.py        --data-cache-path /linkhome/rech/genlor01/urc37ho/.cache        --tensor-model-parallel-size 2        --pipeline-model-parallel-size 2        --num-layers 32        --hidden-size 4096        --ffn-hidden-size 11008        --num-attention-heads 32        --micro-batch-size 6        --global-batch-size 192        --seq-length 2048        --max-position-embeddings 2048        --train-iters 250000        --save /gpfswork/rech/qgz/urc37ho/checkpoints/        --load /gpfswork/rech/qgz/urc37ho/checkpoints/        --data-path /gpfsscratch/rech/qgz/commun/preprocessed_data/Lucie/lucie_tokens_2.4-space_prefix_all/Wikipedia--fr--026_text_document        --data-impl mmap        --tokenizer-type PretrainedFromHF         --tokenizer-name-or-path OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all        --distributed-backend nccl        --lr 3e-4        --lr-decay-style cosine        --min-lr 3e-5        --weight-decay 0.1        --clip-grad 1        --lr-warmup-iters 2000        --optimizer adam        --adam-beta1 0.9        --adam-beta2 0.95        --log-interval 1        --save-interval 1000        --eval-interval 50        --eval-iters 50        --bf16        --use-flash-attn-v2        --no-query-key-layer-scaling        --attention-dropout 0        --hidden-dropout 0        --use-rotary-position-embeddings        --untie-embeddings-and-output-weights        --swiglu        --normalization rmsnorm        --disable-bias-linear        --num-key-value-heads 32         --zero-stage=0  --deepspeed_config=./ds_config.1299552.json  --deepspeed              --exit-duration-in-mins 1190             '
+ RUN='torchrun --nproc_per_node 8 --nnodes 4 --node_rank $SLURM_PROCID --rdzv_endpoint jean-zay-iam04:6000 --rdzv_backend c10d --max_restarts 0 --tee 3        /gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed/pretrain_gpt.py        --data-cache-path /linkhome/rech/genlor01/urc37ho/.cache        --tensor-model-parallel-size 2        --pipeline-model-parallel-size 2        --num-layers 32        --hidden-size 4096        --ffn-hidden-size 11008        --num-attention-heads 32        --micro-batch-size 6        --global-batch-size 192        --seq-length 2048        --max-position-embeddings 2048        --train-iters 250000        --save /gpfswork/rech/qgz/urc37ho/checkpoints/        --load /gpfswork/rech/qgz/urc37ho/checkpoints/        --data-path /gpfsscratch/rech/qgz/commun/preprocessed_data/Lucie/lucie_tokens_2.4-space_prefix_all/Wikipedia--fr--026_text_document        --data-impl mmap        --tokenizer-type PretrainedFromHF         --tokenizer-name-or-path OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all        --distributed-backend nccl        --lr 3e-4        --lr-decay-style cosine        --min-lr 3e-5        --weight-decay 0.1        --clip-grad 1        --lr-warmup-iters 2000        --optimizer adam        --adam-beta1 0.9        --adam-beta2 0.95        --log-interval 1        --save-interval 1000        --eval-interval 50        --eval-iters 50        --bf16        --use-flash-attn-v2        --no-query-key-layer-scaling        --attention-dropout 0        --hidden-dropout 0        --use-rotary-position-embeddings        --untie-embeddings-and-output-weights        --swiglu        --normalization rmsnorm        --disable-bias-linear        --num-key-value-heads 32         --zero-stage=0  --deepspeed_config=./ds_config.1299552.json  --deepspeed              --exit-duration-in-mins 1190             '
+ srun --jobid 1299552 bash -c 'torchrun --nproc_per_node 8 --nnodes 4 --node_rank $SLURM_PROCID --rdzv_endpoint jean-zay-iam04:6000 --rdzv_backend c10d --max_restarts 0 --tee 3        /gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed/pretrain_gpt.py        --data-cache-path /linkhome/rech/genlor01/urc37ho/.cache        --tensor-model-parallel-size 2        --pipeline-model-parallel-size 2        --num-layers 32        --hidden-size 4096        --ffn-hidden-size 11008        --num-attention-heads 32        --micro-batch-size 6        --global-batch-size 192        --seq-length 2048        --max-position-embeddings 2048        --train-iters 250000        --save /gpfswork/rech/qgz/urc37ho/checkpoints/        --load /gpfswork/rech/qgz/urc37ho/checkpoints/        --data-path /gpfsscratch/rech/qgz/commun/preprocessed_data/Lucie/lucie_tokens_2.4-space_prefix_all/Wikipedia--fr--026_text_document        --data-impl mmap        --tokenizer-type PretrainedFromHF         --tokenizer-name-or-path OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all        --distributed-backend nccl        --lr 3e-4        --lr-decay-style cosine        --min-lr 3e-5        --weight-decay 0.1        --clip-grad 1        --lr-warmup-iters 2000        --optimizer adam        --adam-beta1 0.9        --adam-beta2 0.95        --log-interval 1        --save-interval 1000        --eval-interval 50        --eval-iters 50        --bf16        --use-flash-attn-v2        --no-query-key-layer-scaling        --attention-dropout 0        --hidden-dropout 0        --use-rotary-position-embeddings        --untie-embeddings-and-output-weights        --swiglu        --normalization rmsnorm        --disable-bias-linear        --num-key-value-heads 32         --zero-stage=0  --deepspeed_config=./ds_config.1299552.json  --deepspeed              --exit-duration-in-mins 1190             '
+ tee -a /gpfswork/rech/qgz/urc37ho/lucie-logs/main_log.txt
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] 
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] *****************************************
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] *****************************************
[2024-03-28 23:37:11,855] torch.distributed.run: [WARNING] 
[2024-03-28 23:37:11,855] torch.distributed.run: [WARNING] *****************************************
[2024-03-28 23:37:11,855] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-28 23:37:11,855] torch.distributed.run: [WARNING] *****************************************
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] 
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] *****************************************
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] *****************************************
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] 
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] *****************************************
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-28 23:37:11,854] torch.distributed.run: [WARNING] *****************************************
[default1]:[2024-03-28 23:37:18,308] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default7]:[2024-03-28 23:37:18,324] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2024-03-28 23:37:18,413] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-03-28 23:37:18,369] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-03-28 23:37:18,411] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:[2024-03-28 23:37:18,336] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:[2024-03-28 23:37:18,415] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-03-28 23:37:18,412] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[2024-03-28 23:37:18,321] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-03-28 23:37:18,323] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2024-03-28 23:37:18,415] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2024-03-28 23:37:18,418] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-03-28 23:37:18,414] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:[2024-03-28 23:37:18,417] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-03-28 23:37:18,417] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:[2024-03-28 23:37:18,419] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default1]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default5]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default5]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default5]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default5]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default1]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:/bin/sh: line 0: type: git: not found
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default5]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default5]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default5]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default5]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default7]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default7]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default7]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:DeepSpeed general environment info:
[default7]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default7]:torch version .................... 2.2.1
[default7]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default7]:deepspeed info ................... 0.12.6, unknown, unknown
[default7]:torch cuda version ............... 12.1
[default7]:torch hip version ................ None
[default7]:nvcc version ..................... 12.1
[default7]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default7]:shared memory (/dev/shm) size .... 251.60 GB
[default7]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default1]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default3]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 2.2.1
[default3]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.12.6, unknown, unknown
[default3]:torch cuda version ............... 12.1
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 12.1
[default3]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default3]:shared memory (/dev/shm) size .... 251.60 GB
[default3]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default2]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:/bin/sh: line 0: type: git: not found
[default0]:/bin/sh: line 0: type: git: not found
[default6]:/bin/sh: line 0: type: git: not found
[default2]:/bin/sh: line 0: type: git: not found
[default4]:/bin/sh: line 0: type: git: not found
[default5]:/bin/sh: line 0: type: git: not found
[default3]:/bin/sh: line 0: type: git: not found
[default1]:/bin/sh: line 0: type: git: not found
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default1]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 2.2.1
[default1]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.12.6, unknown, unknown
[default1]:torch cuda version ............... 12.1
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 12.1
[default1]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default1]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default7]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default7]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default7]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:DeepSpeed general environment info:
[default7]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default7]:torch version .................... 2.2.1
[default7]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default7]:deepspeed info ................... 0.12.6, unknown, unknown
[default7]:torch cuda version ............... 12.1
[default7]:torch hip version ................ None
[default7]:nvcc version ..................... 12.1
[default7]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default7]:shared memory (/dev/shm) size .... 251.60 GB
[default7]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:/bin/sh: line 0: type: git: not found
[default4]:/bin/sh: line 0: type: git: not found
[default6]:/bin/sh: line 0: type: git: not found
[default7]:/bin/sh: line 0: type: git: not found
[default5]:/bin/sh: line 0: type: git: not found
[default0]:/bin/sh: line 0: type: git: not found
[default2]:/bin/sh: line 0: type: git: not found
[default2]:[2024-03-28 23:37:18,415] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2024-03-28 23:37:18,385] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-03-28 23:37:18,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:[2024-03-28 23:37:18,381] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-03-28 23:37:18,394] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-03-28 23:37:18,362] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:[2024-03-28 23:37:18,416] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default7]:[2024-03-28 23:37:18,384] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:[2024-03-28 23:37:18,316] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:[2024-03-28 23:37:18,300] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2024-03-28 23:37:18,350] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-03-28 23:37:18,344] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2024-03-28 23:37:18,409] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-03-28 23:37:18,364] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-03-28 23:37:18,378] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default7]:[2024-03-28 23:37:18,403] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default1]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default5]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default5]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default5]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default4]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default4]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:/bin/sh: line 0: type: git: not found
[default5]:/bin/sh: line 0: type: git: not found
[default4]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:DeepSpeed general environment info:
[default4]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default4]:torch version .................... 2.2.1
[default4]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default4]:deepspeed info ................... 0.12.6, unknown, unknown
[default4]:torch cuda version ............... 12.1
[default4]:torch hip version ................ None
[default4]:nvcc version ..................... 12.1
[default4]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default4]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default1]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:DeepSpeed general environment info:
[default5]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default5]:torch version .................... 2.2.1
[default5]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default5]:deepspeed info ................... 0.12.6, unknown, unknown
[default5]:torch cuda version ............... 12.1
[default5]:torch hip version ................ None
[default5]:nvcc version ..................... 12.1
[default5]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default5]:shared memory (/dev/shm) size .... 251.60 GB
[default5]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:/bin/sh: line 0: type: git: not found
[default4]:/bin/sh: line 0: type: git: not found
[default7]:/bin/sh: line 0: type: git: not found
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default1]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default5]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default5]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default5]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:using world size: 32, data-parallel-size: 8, sequence-parallel size: 1, tensor-model-parallel size: 2, pipeline-model-parallel size: 2 
[default0]:accumulate and all-reduce gradients in fp32 for bfloat16 data type.
[default0]:using torch.bfloat16 for parameters ...
[default0]:------------------------ arguments ------------------------
[default0]:  accumulate_allreduce_grads_in_fp32 .............. True
[default0]:  adam_beta1 ...................................... 0.9
[default0]:  adam_beta2 ...................................... 0.95
[default0]:  adam_eps ........................................ 1e-08
[default0]:  add_bias_linear ................................. False
[default0]:  add_position_embedding .......................... False
[default0]:  adlr_autoresume ................................. False
[default0]:  adlr_autoresume_interval ........................ 1000
[default0]:  aml_data_download_path .......................... None
[default0]:  apply_layernorm_1p .............................. False
[default0]:  apply_query_key_layer_scaling ................... False
[default0]:  apply_residual_connection_post_layernorm ........ False
[default0]:  async_tensor_model_parallel_allreduce ........... False
[default0]:  attention_dropout ............................... 0.0
[default0]:  attention_softmax_in_fp32 ....................... False
[default0]:  barrier_with_L1_time ............................ True
[default0]:  bert_binary_head ................................ True
[default0]:  bert_embedder_type .............................. megatron
[default0]:  bert_load ....................................... None
[default0]:  bf16 ............................................ True
[default0]:  bias_dropout_fusion ............................. True
[default0]:  bias_gelu_fusion ................................ False
[default0]:  biencoder_projection_dim ........................ 0
[default0]:  biencoder_shared_query_context_model ............ False
[default0]:  block_data_path ................................. None
[default0]:  checkpoint_activations .......................... False
[default0]:  checkpoint_in_cpu ............................... False
[default0]:  checkpoint_num_layers ........................... 1
[default0]:  classes_fraction ................................ 1.0
[default0]:  clip_grad ....................................... 1.0
[default0]:  compression_training ............................ False
[default0]:  consumed_train_samples .......................... 0
[default0]:  consumed_train_tokens ........................... 0
[default0]:  consumed_valid_samples .......................... 0
[default0]:  contigious_checkpointing ........................ False
[default0]:  cpu_optimizer ................................... False
[default0]:  cpu_torch_adam .................................. False
[default0]:  create_moe_param_group .......................... False
[default0]:  curriculum_learning_legacy ...................... False
[default0]:  data_cache_path ................................. /linkhome/rech/genlor01/urc37ho/.cache
[default0]:  data_efficiency_curriculum_learning ............. False
[default0]:  data_impl ....................................... mmap
[default0]:  data_parallel_random_init ....................... False
[default0]:  data_parallel_size .............................. 8
[default0]:  data_path ....................................... ['/gpfsscratch/rech/qgz/commun/preprocessed_data/Lucie/lucie_tokens_2.4-space_prefix_all/Wikipedia--fr--026_text_document']
[default0]:  data_per_class_fraction ......................... 1.0
[default0]:  data_sharding ................................... True
[default0]:  dataloader_type ................................. single
[default0]:  DDP_impl ........................................ local
[default0]:  decoder_num_layers .............................. None
[default0]:  decoder_seq_length .............................. None
[default0]:  deepscale ....................................... False
[default0]:  deepscale_config ................................ None
[default0]:  deepspeed ....................................... True
[default0]:  deepspeed_activation_checkpointing .............. False
[default0]:  deepspeed_config ................................ ./ds_config.1299552.json
[default0]:  deepspeed_mpi ................................... False
[default0]:  dino_bottleneck_size ............................ 256
[default0]:  dino_freeze_last_layer .......................... 1
[default0]:  dino_head_hidden_size ........................... 2048
[default0]:  dino_local_crops_number ......................... 10
[default0]:  dino_local_img_size ............................. 96
[default0]:  dino_norm_last_layer ............................ False
[default0]:  dino_teacher_temp ............................... 0.07
[default0]:  dino_warmup_teacher_temp ........................ 0.04
[default0]:  dino_warmup_teacher_temp_epochs ................. 30
[default0]:  disable_mem_efficient_ln ........................ True
[default0]:  distribute_checkpointed_activations ............. False
[default0]:  distribute_saved_activations .................... False
[default0]:  distributed_backend ............................. nccl
[default0]:  distributed_timeout_minutes ..................... 10
[default0]:  ds_inference .................................... False
[default0]:  ds_pipeline_enabled ............................. True
[default0]:  ds_sequence_parallel_size ....................... 1
[default0]:  embedding_path .................................. None
[default0]:  embedding_weights_in_fp32 ....................... False
[default0]:  empty_unused_memory_level ....................... 0
[default0]:  enable_expert_tensor_parallelism ................ False
[default0]:  encoder_num_layers .............................. 32
[default0]:  encoder_seq_length .............................. 2048
[default0]:  end_weight_decay ................................ 0.1
[default0]:  eod_mask_loss ................................... False
[default0]:  eval_interval ................................... 50
[default0]:  eval_iters ...................................... 50
[default0]:  evidence_data_path .............................. None
[default0]:  exit_duration_in_mins ........................... 1190
[default0]:  exit_interval ................................... None
[default0]:  exit_on_missing_checkpoint ...................... False
[default0]:  exit_signal_handler ............................. False
[default0]:  expert_interval ................................. 2
[default0]:  ffn_hidden_size ................................. 11008
[default0]:  finetune ........................................ False
[default0]:  force_ds_sequence_parallel ...................... False
[default0]:  fp16 ............................................ False
[default0]:  fp16_lm_cross_entropy ........................... False
[default0]:  fp32_residual_connection ........................ False
[default0]:  fp8_amax_compute_algo ........................... most_recent
[default0]:  fp8_amax_history_len ............................ 1
[default0]:  fp8_e4m3 ........................................ False
[default0]:  fp8_hybrid ...................................... False
[default0]:  fp8_interval .................................... 1
[default0]:  fp8_margin ...................................... 0
[default0]:  fp8_wgrad ....................................... True
[default0]:  global_batch_size ............................... 192
[default0]:  gradient_accumulation_fusion .................... True
[default0]:  head_lr_mult .................................... 1.0
[default0]:  hidden_dropout .................................. 0.0
[default0]:  hidden_size ..................................... 4096
[default0]:  hidden_size_teacher ............................. None
[default0]:  hysteresis ...................................... 2
[default0]:  ict_head_size ................................... None
[default0]:  ict_load ........................................ None
[default0]:  img_h ........................................... 224
[default0]:  img_w ........................................... 224
[default0]:  indexer_batch_size .............................. 128
[default0]:  indexer_log_interval ............................ 1000
[default0]:  inference ....................................... False
[default0]:  inference_batch_times_seqlen_threshold .......... 512
[default0]:  init_method_std ................................. 0.02
[default0]:  init_method_xavier_uniform ...................... False
[default0]:  initial_loss_scale .............................. 4294967296
[default0]:  iter_per_epoch .................................. 1250
[default0]:  kd .............................................. False
[default0]:  kd_alpha_ce ..................................... 1
[default0]:  kd_beta_ce ...................................... 1
[default0]:  kd_temp ......................................... 1.0
[default0]:  kv_channels ..................................... 128
[default0]:  layernorm_epsilon ............................... 1e-05
[default0]:  lazy_mpu_init ................................... None
[default0]:  load ............................................ /gpfswork/rech/qgz/urc37ho/checkpoints/
[default0]:  load_iteration .................................. None
[default0]:  load_teacher .................................... None
[default0]:  local_rank ...................................... None
[default0]:  log_batch_size_to_tensorboard ................... False
[default0]:  log_interval .................................... 1
[default0]:  log_learning_rate_to_tensorboard ................ True
[default0]:  log_loss_scale_to_tensorboard ................... True
[default0]:  log_memory_to_tensorboard ....................... False
[default0]:  log_num_zeros_in_grad ........................... False
[default0]:  log_optimizer_states_to_tensorboard ............. False
[default0]:  log_params_norm ................................. False
[default0]:  log_timers_to_tensorboard ....................... False
[default0]:  log_validation_ppl_to_tensorboard ............... False
[default0]:  log_world_size_to_tensorboard ................... False
[default0]:  loss_scale ...................................... None
[default0]:  loss_scale_window ............................... 1000
[default0]:  lr .............................................. 0.0003
[default0]:  lr_decay_iters .................................. None
[default0]:  lr_decay_samples ................................ None
[default0]:  lr_decay_style .................................. cosine
[default0]:  lr_decay_tokens ................................. None
[default0]:  lr_warmup_fraction .............................. None
[default0]:  lr_warmup_iters ................................. 2000
[default0]:  lr_warmup_samples ............................... 0
[default0]:  lr_warmup_tokens ................................ None
[default0]:  make_vocab_size_divisible_by .................... 128
[default0]:  mask_factor ..................................... 1.0
[default0]:  mask_prob ....................................... 0.15
[default0]:  mask_type ....................................... random
[default0]:  masked_softmax_fusion ........................... True
[default0]:  max_position_embeddings ......................... 2048
[default0]:  max_tokens_to_oom ............................... 12000
[default0]:  memory_centric_tiled_linear ..................... False
[default0]:  merge_file ...................................... None
[default0]:  micro_batch_size ................................ 6
[default0]:  min_loss_scale .................................. 1.0
[default0]:  min_lr .......................................... 3e-05
[default0]:  mlp_type ........................................ standard
[default0]:  mmap_warmup ..................................... False
[default0]:  moe_eval_capacity_factor ........................ 1.0
[default0]:  moe_expert_parallel_size ........................ 1
[default0]:  moe_loss_coeff .................................. 0.1
[default0]:  moe_min_capacity ................................ 4
[default0]:  moe_token_dropping .............................. True
[default0]:  moe_train_capacity_factor ....................... 1.0
[default0]:  mos ............................................. False
[default0]:  multiple_valid_sets ............................. False
[default0]:  no_load_lr_state ................................ False
[default0]:  no_load_optim ................................... None
[default0]:  no_load_rng ..................................... None
[default0]:  no_persist_layer_norm ........................... False
[default0]:  no_pipeline_parallel ............................ False
[default0]:  no_save_optim ................................... None
[default0]:  no_save_rng ..................................... None
[default0]:  normalization ................................... rmsnorm
[default0]:  num_attention_heads ............................. 32
[default0]:  num_attention_heads_teacher ..................... None
[default0]:  num_channels .................................... 3
[default0]:  num_classes ..................................... 1000
[default0]:  num_experts ..................................... [1]
[default0]:  num_experts_switch .............................. None
[default0]:  num_experts_teacher ............................. [1]
[default0]:  num_key_value_heads ............................. 32
[default0]:  num_layers ...................................... 32
[default0]:  num_layers_per_virtual_pipeline_stage ........... None
[default0]:  num_layers_teacher .............................. None
[default0]:  num_workers ..................................... 2
[default0]:  onnx_safe ....................................... None
[default0]:  openai_gelu ..................................... False
[default0]:  optimizer ....................................... adam
[default0]:  output_bert_embeddings .......................... False
[default0]:  overlap_p2p_comm ................................ False
[default0]:  override_opt_param_scheduler .................... False
[default0]:  params_dtype .................................... torch.bfloat16
[default0]:  partition_activations ........................... False
[default0]:  patch_dim ....................................... 16
[default0]:  perform_initialization .......................... True
[default0]:  pipeline_model_parallel_size .................... 2
[default0]:  pipeline_model_parallel_split_rank .............. None
[default0]:  profile_backward ................................ False
[default0]:  query_in_block_prob ............................. 0.1
[default0]:  rampup_batch_size ............................... None
[default0]:  random_ltd ...................................... False
[default0]:  rank ............................................ 0
[default0]:  recompute_granularity ........................... None
[default0]:  recompute_method ................................ None
[default0]:  recompute_num_layers ............................ 1
[default0]:  remote_device ................................... none
[default0]:  reset_attention_mask ............................ False
[default0]:  reset_iteration ................................. False
[default0]:  reset_position_ids .............................. False
[default0]:  retriever_report_topk_accuracies ................ []
[default0]:  retriever_score_scaling ......................... False
[default0]:  retriever_seq_length ............................ 256
[default0]:  retro_add_retriever ............................. False
[default0]:  retro_cyclic_train_iters ........................ None
[default0]:  retro_encoder_attention_dropout ................. 0.1
[default0]:  retro_encoder_hidden_dropout .................... 0.1
[default0]:  retro_encoder_layers ............................ 2
[default0]:  retro_num_neighbors ............................. 2
[default0]:  retro_num_retrieved_chunks ...................... 2
[default0]:  retro_return_doc_ids ............................ False
[default0]:  retro_workdir ................................... None
[default0]:  return_data_index ............................... False
[default0]:  rotary_percent .................................. 1.0
[default0]:  sample_rate ..................................... 1.0
[default0]:  save ............................................ /gpfswork/rech/qgz/urc37ho/checkpoints/
[default0]:  save_interval ................................... 1000
[default0]:  scatter_gather_tensors_in_pipeline .............. True
[default0]:  scattered_embeddings ............................ False
[default0]:  seed ............................................ 1234
[default0]:  seq_length ...................................... 2048
[default0]:  sequence_parallel ............................... False
[default0]:  sgd_momentum .................................... 0.9
[default0]:  short_seq_prob .................................. 0.1
[default0]:  skip_train ...................................... False
[default0]:  split ........................................... 969, 30, 1
[default0]:  split_transformers .............................. False
[default0]:  squared_relu .................................... False
[default0]:  standalone_embedding_stage ...................... False
[default0]:  start_weight_decay .............................. 0.1
[default0]:  swiglu .......................................... True
[default0]:  swin_backbone_type .............................. tiny
[default0]:  synchronize_each_layer .......................... False
[default0]:  tensor_model_parallel_size ...................... 2
[default0]:  tensorboard_dir ................................. None
[default0]:  tensorboard_log_interval ........................ 1
[default0]:  tensorboard_queue_size .......................... 1000
[default0]:  test_data_path .................................. None
[default0]:  tile_factor ..................................... 1
[default0]:  timing_log_level ................................ 0
[default0]:  timing_log_option ............................... minmax
[default0]:  titles_data_path ................................ None
[default0]:  tokenizer_model ................................. None
[default0]:  tokenizer_name_or_path .......................... OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all
[default0]:  tokenizer_type .................................. PretrainedFromHF
[default0]:  topk ............................................ 1
[default0]:  train_data_exact_num_epochs ..................... None
[default0]:  train_data_path ................................. None
[default0]:  train_desc_path ................................. None
[default0]:  train_doc_idx_path .............................. None
[default0]:  train_idx_path .................................. None
[default0]:  train_iters ..................................... 250000
[default0]:  train_sample_idx_path ........................... None
[default0]:  train_samples ................................... None
[default0]:  train_shuffle_idx_path .......................... None
[default0]:  train_tokens .................................... None
[default0]:  transformer_impl ................................ local
[default0]:  transformer_pipeline_model_parallel_size ........ 2
[default0]:  universal_checkpoint ............................ False
[default0]:  untie_embeddings_and_output_weights ............. True
[default0]:  use_checkpoint_args ............................. False
[default0]:  use_checkpoint_opt_param_scheduler .............. False
[default0]:  use_contiguous_buffers_in_local_ddp ............. True
[default0]:  use_cpu_initialization .......................... None
[default0]:  use_dataset_only ................................ False
[default0]:  use_distributed_optimizer ....................... False
[default0]:  use_flash_attn .................................. True
[default0]:  use_flash_attn_triton ........................... False
[default0]:  use_flash_attn_v1 ............................... False
[default0]:  use_flash_attn_v2 ............................... True
[default0]:  use_one_sent_docs ............................... False
[default0]:  use_pin_memory .................................. False
[default0]:  use_ring_exchange_p2p ........................... False
[default0]:  use_rotary_position_embeddings .................. True
[default0]:  use_tutel ....................................... False
[default0]:  valid_data_path ................................. None
[default0]:  variable_seq_lengths ............................ False
[default0]:  virtual_pipeline_model_parallel_size ............ None
[default0]:  vision_backbone_type ............................ vit
[default0]:  vision_pretraining .............................. False
[default0]:  vision_pretraining_type ......................... classify
[default0]:  vocab_extra_ids ................................. 0
[default0]:  vocab_file ...................................... None
[default0]:  vocab_size ...................................... None
[default0]:  wandb_api_key ................................... None
[default0]:  wandb_entity .................................... None
[default0]:  wandb_id ........................................ None
[default0]:  wandb_logger .................................... False
[default0]:  wandb_project ................................... megatron-ds-training
[default0]:  wandb_resume .................................... None
[default0]:  wandb_run_name .................................. None
[default0]:  weight_decay .................................... 0.1
[default0]:  weight_decay_incr_style ......................... constant
[default0]:  world_size ...................................... 32
[default0]:  zero_allgather_bucket_size ...................... 0.0
[default0]:  zero_contigious_gradients ....................... False
[default0]:  zero_reduce_bucket_size ......................... 0.0
[default0]:  zero_reduce_scatter ............................. False
[default0]:  zero_stage ...................................... 0
[default0]:-------------------- end of arguments ---------------------
[default0]:setting number of micro-batches to constant 4
[default0]:> building PretrainedFromHF tokenizer ...
[default0]: vocab file is un-used. loading tokenizer from pre-trained model
[default1]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default1]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 2.2.1
[default1]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.12.6, unknown, unknown
[default1]:torch cuda version ............... 12.1
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 12.1
[default1]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default1]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:DeepSpeed general environment info:
[default5]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default5]:torch version .................... 2.2.1
[default5]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default5]:deepspeed info ................... 0.12.6, unknown, unknown
[default5]:torch cuda version ............... 12.1
[default5]:torch hip version ................ None
[default5]:nvcc version ..................... 12.1
[default5]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default5]:shared memory (/dev/shm) size .... 251.60 GB
[default5]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default2]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default3]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default3]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:/bin/sh: line 0: type: git: not found
[default4]:/bin/sh: line 0: type: git: not found
[default7]:/bin/sh: line 0: type: git: not found
[default1]:/bin/sh: line 0: type: git: not found
[default2]:/bin/sh: line 0: type: git: not found
[default1]:/bin/sh: line 0: type: git: not found
[default3]:/bin/sh: line 0: type: git: not found
[default6]:/bin/sh: line 0: type: git: not found
[default3]:/bin/sh: line 0: type: git: not found
[default5]:/bin/sh: line 0: type: git: not found
[default2]:/bin/sh: line 0: type: git: not found
[default0]:loading file tokenizer.model from cache at None
[default0]:loading file tokenizer.json from cache at /linkhome/rech/genlor01/urc37ho/.cache/huggingface/hub/models--OpenLLM-France--Lucie-tokenizer-v2.4-space_prefix_all/snapshots/edf1b3d40698a2eec64a63aa8731c510e5810ae6/tokenizer.json
[default0]:loading file added_tokens.json from cache at None
[default0]:loading file special_tokens_map.json from cache at /linkhome/rech/genlor01/urc37ho/.cache/huggingface/hub/models--OpenLLM-France--Lucie-tokenizer-v2.4-space_prefix_all/snapshots/edf1b3d40698a2eec64a63aa8731c510e5810ae6/special_tokens_map.json
[default0]:loading file tokenizer_config.json from cache at /linkhome/rech/genlor01/urc37ho/.cache/huggingface/hub/models--OpenLLM-France--Lucie-tokenizer-v2.4-space_prefix_all/snapshots/edf1b3d40698a2eec64a63aa8731c510e5810ae6/tokenizer_config.json
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:Loading extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:Loading extension module scaled_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:Loading extension module scaled_softmax_cuda...
[default0]:[rank24]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank31]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank26]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank25]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank28]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:[rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank14]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank13]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank12]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank15]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank22]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank20]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank21]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank23]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank30]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank29]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank27]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:torch version .................... 2.2.1
[default3]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.12.6, unknown, unknown
[default3]:torch cuda version ............... 12.1
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 12.1
[default3]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default3]:shared memory (/dev/shm) size .... 251.60 GB
[default3]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default4]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default4]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default4]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:DeepSpeed general environment info:
[default4]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default4]:torch version .................... 2.2.1
[default4]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default4]:deepspeed info ................... 0.12.6, unknown, unknown
[default4]:torch cuda version ............... 12.1
[default4]:torch hip version ................ None
[default4]:nvcc version ..................... 12.1
[default4]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default4]:shared memory (/dev/shm) size .... 251.60 GB
[default4]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default7]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default7]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default7]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:DeepSpeed general environment info:
[default7]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default7]:torch version .................... 2.2.1
[default7]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default7]:deepspeed info ................... 0.12.6, unknown, unknown
[default7]:torch cuda version ............... 12.1
[default7]:torch hip version ................ None
[default7]:nvcc version ..................... 12.1
[default7]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default7]:shared memory (/dev/shm) size .... 251.60 GB
[default7]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]: > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
[default0]:> initializing torch distributed ...
[default0]:[2024-03-28 23:37:45,901] [INFO] [comm.py:637:init_distributed] cdb=None
[default0]:[2024-03-28 23:37:45,901] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default0]:> initialized tensor model parallel with size 2
[default0]:> initialized pipeline model parallel with size 2
[default0]:> setting random seeds to 1234 ...
[default0]:> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[default0]:> compiling dataset index builder ...
[default1]:[2024-03-28 23:37:46,806] [INFO] [comm.py:637:init_distributed] cdb=None
[default6]:[2024-03-28 23:37:46,776] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-03-28 23:37:46,793] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-03-28 23:37:46,801] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-03-28 23:37:46,801] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:[2024-03-28 23:37:46,778] [INFO] [comm.py:637:init_distributed] cdb=None
[default7]:[2024-03-28 23:37:46,784] [INFO] [comm.py:637:init_distributed] cdb=None
[default0]:make: Entering directory '/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/data'
[default0]:make: Nothing to be done for 'default'.
[default0]:make: Leaving directory '/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed/megatron/data'
[default0]:>>> done with dataset index builder. Compilation time: 0.153 seconds
[default0]:> compiling and loading fused kernels ...
[default0]:ninja: no work to do.
[default0]:ninja: no work to do.
[default0]:ninja: no work to do.
[default0]:>>> done with compiling and loading fused kernels. Compilation time: 38.419 seconds
[default0]:time to initialize megatron (seconds): 53.828
[default0]:[after megatron is initialized] datetime: 2024-03-28 23:38:25 
[default0]:building GPT model ...
[default0]:[2024-03-28 23:38:25,931] [INFO] [utils.py:791:see_memory_usage] Before Building Model
[default0]:[2024-03-28 23:38:25,932] [INFO] [utils.py:792:see_memory_usage] MA 0.0 GB         Max_MA 2.16 GB         CA 0.0 GB         Max_CA 2 GB 
[default0]:[2024-03-28 23:38:25,932] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.0 GB, percent = 5.6%
[default0]:SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default3]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default3]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default7]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default7]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:DeepSpeed general environment info:
[default7]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default7]:torch version .................... 2.2.1
[default7]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default7]:deepspeed info ................... 0.12.6, unknown, unknown
[default7]:torch cuda version ............... 12.1
[default7]:torch hip version ................ None
[default7]:nvcc version ..................... 12.1
[default7]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default7]:shared memory (/dev/shm) size .... 251.60 GB
[default7]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default4]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 2.2.1
[default1]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.12.6, unknown, unknown
[default1]:torch cuda version ............... 12.1
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 12.1
[default1]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default1]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:[2024-03-28 23:37:45,813] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 2.2.1
[default3]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.12.6, unknown, unknown
[default3]:torch cuda version ............... 12.1
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 12.1
[default3]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default3]:shared memory (/dev/shm) size .... 251.60 GB
[default3]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default2]:[2024-03-28 23:37:46,697] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:[2024-03-28 23:37:46,712] [INFO] [comm.py:637:init_distributed] cdb=None
[default6]:[2024-03-28 23:37:46,701] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-03-28 23:37:46,700] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-03-28 23:37:46,692] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-03-28 23:37:46,698] [INFO] [comm.py:637:init_distributed] cdb=None
[default7]:[2024-03-28 23:37:46,694] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]: > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1684672512
[default1]: > total number of parameters in model: 1684672512
[default0]: > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1684672512
[default0]: > total number of parameters in model: 1684672512
[default2]:[2024-03-28 23:38:27,947] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default4]:[2024-03-28 23:38:27,949] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default5]:[2024-03-28 23:38:27,944] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default1]:[2024-03-28 23:38:27,944] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default0]:[2024-03-28 23:38:27,946] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default3]:[2024-03-28 23:38:27,944] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default7]:[2024-03-28 23:38:27,945] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default0]:Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=0, model=1): 1, ProcessCoord(pipe=0, data=1, model=0): 2, ProcessCoord(pipe=0, data=1, model=1): 3, ProcessCoord(pipe=0, data=2, model=0): 4, ProcessCoord(pipe=0, data=2, model=1): 5, ProcessCoord(pipe=0, data=3, model=0): 6, ProcessCoord(pipe=0, data=3, model=1): 7, ProcessCoord(pipe=0, data=4, model=0): 8, ProcessCoord(pipe=0, data=4, model=1): 9, ProcessCoord(pipe=0, data=5, model=0): 10, ProcessCoord(pipe=0, data=5, model=1): 11, ProcessCoord(pipe=0, data=6, model=0): 12, ProcessCoord(pipe=0, data=6, model=1): 13, ProcessCoord(pipe=0, data=7, model=0): 14, ProcessCoord(pipe=0, data=7, model=1): 15, ProcessCoord(pipe=1, data=0, model=0): 16, ProcessCoord(pipe=1, data=0, model=1): 17, ProcessCoord(pipe=1, data=1, model=0): 18, ProcessCoord(pipe=1, data=1, model=1): 19, ProcessCoord(pipe=1, data=2, model=0): 20, ProcessCoord(pipe=1, data=2, model=1): 21, ProcessCoord(pipe=1, data=3, model=0): 22, ProcessCoord(pipe=1, data=3, model=1): 23, ProcessCoord(pipe=1, data=4, model=0): 24, ProcessCoord(pipe=1, data=4, model=1): 25, ProcessCoord(pipe=1, data=5, model=0): 26, ProcessCoord(pipe=1, data=5, model=1): 27, ProcessCoord(pipe=1, data=6, model=0): 28, ProcessCoord(pipe=1, data=6, model=1): 29, ProcessCoord(pipe=1, data=7, model=0): 30, ProcessCoord(pipe=1, data=7, model=1): 31}
[default0]:[2024-03-28 23:38:25,935] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
[default0]:stage=0 layers=18
[default0]:     0: _to_float16
[default0]:     1: EmbeddingPipe
[default0]:     2: ParallelTransformerLayerPipe
[default0]:     3: ParallelTransformerLayerPipe
[default0]:     4: ParallelTransformerLayerPipe
[default0]:     5: ParallelTransformerLayerPipe
[default0]:     6: ParallelTransformerLayerPipe
[default0]:     7: ParallelTransformerLayerPipe
[default0]:     8: ParallelTransformerLayerPipe
[default0]:     9: ParallelTransformerLayerPipe
[default0]:    10: ParallelTransformerLayerPipe
[default0]:    11: ParallelTransformerLayerPipe
[default0]:    12: ParallelTransformerLayerPipe
[default0]:    13: ParallelTransformerLayerPipe
[default0]:    14: ParallelTransformerLayerPipe
[default0]:    15: ParallelTransformerLayerPipe
[default0]:    16: ParallelTransformerLayerPipe
[default0]:    17: ParallelTransformerLayerPipe
[default0]:stage=1 layers=19
[default0]:    18: ParallelTransformerLayerPipe
[default0]:    19: ParallelTransformerLayerPipe
[default0]:    20: ParallelTransformerLayerPipe
[default0]:    21: ParallelTransformerLayerPipe
[default0]:    22: ParallelTransformerLayerPipe
[default0]:    23: ParallelTransformerLayerPipe
[default0]:    24: ParallelTransformerLayerPipe
[default0]:    25: ParallelTransformerLayerPipe
[default0]:    26: ParallelTransformerLayerPipe
[default0]:    27: ParallelTransformerLayerPipe
[default0]:    28: ParallelTransformerLayerPipe
[default0]:    29: ParallelTransformerLayerPipe
[default0]:    30: ParallelTransformerLayerPipe
[default0]:    31: ParallelTransformerLayerPipe
[default0]:    32: ParallelTransformerLayerPipe
[default0]:    33: ParallelTransformerLayerPipe
[default0]:    34: MixedFusedRMSNorm
[default0]:    35: LMHeadPipe
[default0]:    36: float16_to_fp32
[default0]:  loss: CrossEntropy
[default1]: > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1684668416
[default1]: > total number of parameters in model: 1684668416
[default0]:[2024-03-28 23:38:26,181] [INFO] [utils.py:791:see_memory_usage] After Building Model
[default0]:[2024-03-28 23:38:26,181] [INFO] [utils.py:792:see_memory_usage] MA 3.17 GB         Max_MA 3.19 GB         CA 3.19 GB         Max_CA 3 GB 
[default0]:[2024-03-28 23:38:26,181] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.27 GB, percent = 5.6%
[default0]: > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1684668416
[default0]: > total number of parameters in model: 1684668416
[default0]:> learning rate decay style: cosine
[default0]:DeepSpeed is enabled.
[default0]:[2024-03-28 23:38:26,183] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown
[default0]:[2024-03-28 23:38:27,739] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[default0]:[2024-03-28 23:38:27,739] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[default0]:[2024-03-28 23:38:27,739] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[default0]:[2024-03-28 23:38:27,741] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[default0]:[2024-03-28 23:38:27,741] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[default1]:[2024-03-28 23:38:27,832] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default3]:[2024-03-28 23:38:27,832] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default0]:[2024-03-28 23:38:27,845] [INFO] [utils.py:791:see_memory_usage] begin bf16_optimizer
[default0]:[2024-03-28 23:38:27,845] [INFO] [utils.py:792:see_memory_usage] MA 3.16 GB         Max_MA 3.17 GB         CA 3.19 GB         Max_CA 3 GB 
[default0]:[2024-03-28 23:38:27,845] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.9 GB, percent = 5.7%
[default6]:[2024-03-28 23:38:27,832] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default2]:[2024-03-28 23:38:27,832] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default5]:[2024-03-28 23:38:27,831] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default4]:[2024-03-28 23:38:27,832] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default7]:[2024-03-28 23:38:27,831] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default0]:[2024-03-28 23:38:27,928] [INFO] [utils.py:791:see_memory_usage] before initializing group 0
[default0]:[2024-03-28 23:38:27,929] [INFO] [utils.py:792:see_memory_usage] MA 3.16 GB         Max_MA 3.16 GB         CA 3.19 GB         Max_CA 3 GB 
[default0]:[2024-03-28 23:38:27,929] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.91 GB, percent = 5.7%
[default0]:[2024-03-28 23:38:28,035] [INFO] [utils.py:791:see_memory_usage] after initializing group 0
[default0]:[2024-03-28 23:38:28,036] [INFO] [utils.py:792:see_memory_usage] MA 10.2 GB         Max_MA 10.2 GB         CA 13.78 GB         Max_CA 14 GB 
[default0]:[2024-03-28 23:38:28,036] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.9 GB, percent = 5.7%
[default0]:[2024-03-28 23:38:28,108] [INFO] [utils.py:791:see_memory_usage] before initializing group 1
[default0]:[2024-03-28 23:38:28,109] [INFO] [utils.py:792:see_memory_usage] MA 10.2 GB         Max_MA 10.2 GB         CA 13.78 GB         Max_CA 14 GB 
[default0]:[2024-03-28 23:38:28,109] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.9 GB, percent = 5.7%
[default0]:[2024-03-28 23:38:28,191] [INFO] [utils.py:791:see_memory_usage] after initializing group 1
[default0]:[2024-03-28 23:38:28,191] [INFO] [utils.py:792:see_memory_usage] MA 10.2 GB         Max_MA 10.2 GB         CA 13.78 GB         Max_CA 14 GB 
[default0]:[2024-03-28 23:38:28,192] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.9 GB, percent = 5.7%
[default0]:[2024-03-28 23:38:28,260] [INFO] [utils.py:791:see_memory_usage] before initialize_optimizer
[default0]:[2024-03-28 23:38:28,260] [INFO] [utils.py:792:see_memory_usage] MA 10.2 GB         Max_MA 10.2 GB         CA 13.78 GB         Max_CA 14 GB 
[default0]:[2024-03-28 23:38:28,261] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 28.91 GB, percent = 5.7%
[default0]:[2024-03-28 23:38:28,361] [INFO] [utils.py:791:see_memory_usage] end initialize_optimizer
[default0]:[2024-03-28 23:38:28,361] [INFO] [utils.py:792:see_memory_usage] MA 11.78 GB         Max_MA 11.78 GB         CA 15.35 GB         Max_CA 15 GB 
[default0]:[2024-03-28 23:38:28,362] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 29.3 GB, percent = 5.8%
[default0]:[2024-03-28 23:38:28,440] [INFO] [utils.py:791:see_memory_usage] end bf16_optimizer
[default0]:[2024-03-28 23:38:28,440] [INFO] [utils.py:792:see_memory_usage] MA 11.78 GB         Max_MA 11.78 GB         CA 15.35 GB         Max_CA 15 GB 
[default0]:[2024-03-28 23:38:28,441] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 29.34 GB, percent = 5.8%
[default0]:[2024-03-28 23:38:28,441] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[default0]:[2024-03-28 23:38:28,441] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[default0]:[2024-03-28 23:38:28,441] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x15377419b7f0>
[default0]:[2024-03-28 23:38:28,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2024-03-28 23:38:28,441] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[default0]:[2024-03-28 23:38:28,441] [INFO] [config.py:988:print]   activation_checkpointing_config  {
[default0]:    "partition_activations": false, 
[default0]:    "contiguous_memory_optimization": false, 
[default0]:    "cpu_checkpointing": false, 
[default0]:    "number_checkpoints": null, 
[default0]:    "synchronize_checkpoint_boundary": false, 
[default0]:    "profile": false
[default0]:}
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   amp_enabled .................. False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   amp_params ................... False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   autotuning_config ............ {
[default0]:    "enabled": false, 
[default0]:    "start_step": null, 
[default0]:    "end_step": null, 
[default0]:    "metric_path": null, 
[default0]:    "arg_mappings": null, 
[default0]:    "metric": "throughput", 
[default0]:    "model_info": null, 
[default0]:    "results_dir": "autotuning_results", 
[default0]:    "exps_dir": "autotuning_exps", 
[default0]:    "overwrite": true, 
[default0]:    "fast": true, 
[default0]:    "start_profile_step": 3, 
[default0]:    "end_profile_step": 5, 
[default0]:    "tuner_type": "gridsearch", 
[default0]:    "tuner_early_stopping": 5, 
[default0]:    "tuner_num_trials": 50, 
[default0]:    "model_info_path": null, 
[default0]:    "mp_size": 1, 
[default0]:    "max_train_batch_size": null, 
[default0]:    "min_train_batch_size": 1, 
[default0]:    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[default0]:    "min_train_micro_batch_size_per_gpu": 1, 
[default0]:    "num_tuning_micro_batch_sizes": 3
[default0]:}
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   bfloat16_enabled ............. True
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x153720539d80>
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   communication_data_type ...... None
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   disable_allgather ............ False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   dump_state ................... False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   flops_profiler_config ........ {
[default0]:    "enabled": false, 
[default0]:    "recompute_fwd_factor": 0.0, 
[default0]:    "profile_step": 1, 
[default0]:    "module_depth": -1, 
[default0]:    "top_modules": 1, 
[default0]:    "detailed": true, 
[default0]:    "output_file": null
[default0]:}
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   fp16_enabled ................. False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   global_rank .................. 0
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 4
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:DeepSpeed general environment info:
[default5]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default5]:torch version .................... 2.2.1
[default5]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default5]:deepspeed info ................... 0.12.6, unknown, unknown
[default5]:torch cuda version ............... 12.1
[default5]:torch hip version ................ None
[default5]:nvcc version ..................... 12.1
[default5]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default5]:shared memory (/dev/shm) size .... 251.60 GB
[default5]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default4]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default4]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default4]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:DeepSpeed general environment info:
[default4]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default4]:torch version .................... 2.2.1
[default4]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default4]:deepspeed info ................... 0.12.6, unknown, unknown
[default4]:torch cuda version ............... 12.1
[default4]:torch hip version ................ None
[default4]:nvcc version ..................... 12.1
[default4]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default4]:shared memory (/dev/shm) size .... 251.60 GB
[default4]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default1]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default1]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 2.2.1
[default1]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.12.6, unknown, unknown
[default1]:torch cuda version ............... 12.1
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 12.1
[default1]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default1]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:[2024-03-28 23:37:45,490] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-03-28 23:37:46,338] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-03-28 23:37:46,381] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-03-28 23:37:46,388] [INFO] [comm.py:637:init_distributed] cdb=None
[default6]:[2024-03-28 23:37:46,385] [INFO] [comm.py:637:init_distributed] cdb=None
[default7]:[2024-03-28 23:37:46,369] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:[2024-03-28 23:37:46,400] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-03-28 23:37:46,388] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-03-28 23:38:27,771] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default0]:[2024-03-28 23:38:27,782] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default2]:[2024-03-28 23:38:27,782] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default7]:[2024-03-28 23:38:27,771] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default5]:[2024-03-28 23:38:27,770] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default6]:[2024-03-28 23:38:27,782] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default4]:[2024-03-28 23:38:27,783] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default1]:[2024-03-28 23:38:27,770] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   graph_harvesting ............. False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 1
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[default0]:[2024-03-28 23:38:28,442] [INFO] [config.py:988:print]   loss_scale ................... 1.0
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   memory_breakdown ............. False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   nebula_config ................ {
[default0]:    "enabled": false, 
[default0]:    "persistent_storage_path": null, 
[default0]:    "persistent_time_interval": 100, 
[default0]:    "num_of_version_in_retention": 2, 
[default0]:    "enable_nebula_load": true, 
[default0]:    "load_path": null
[default0]:}
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   optimizer_name ............... None
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   optimizer_params ............. None
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   pld_enabled .................. False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   pld_params ................... False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   prescale_gradients ........... False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   scheduler_name ............... None
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   scheduler_params ............. None
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   sparse_attention ............. None
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   steps_per_print .............. 1
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   train_batch_size ............. 192
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  6
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   weight_quantization_config ... None
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   world_size ................... 8
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   zero_enabled ................. False
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[default0]:[2024-03-28 23:38:28,443] [INFO] [config.py:974:print_user_config]   json = {
[default0]:    "train_batch_size": 192, 
[default0]:    "train_micro_batch_size_per_gpu": 6, 
[default0]:    "steps_per_print": 1, 
[default0]:    "zero_optimization": {
[default0]:        "stage": 0
[default0]:    }, 
[default0]:    "bf16": {
[default0]:        "enabled": true
[default0]:    }
[default0]:}
[default0]:[2024-03-28 23:38:28,443] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=4 micro_batch_size=6
[default0]:[2024-03-28 23:38:28,443] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default1]:[2024-03-28 23:38:28,765] [INFO] [engine.py:158:__init__] RANK=1 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1684668416 (1684.668M) TOTAL_PARAMS=6738681856 (6738.682M) UNIQUE_PARAMS=6738681856 (6738.682M)
[default0]:[2024-03-28 23:38:28,765] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1684668416 (1684.668M) TOTAL_PARAMS=6738681856 (6738.682M) UNIQUE_PARAMS=6738681856 (6738.682M)
[default3]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default6]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:WARNING: could not find the metadata file /gpfswork/rech/qgz/urc37ho/checkpoints/ 
[default0]:    will not load any checkpoints and will start from random
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default3]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default3]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 2.2.1
[default3]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.12.6, unknown, unknown
[default3]:torch cuda version ............... 12.1
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 12.1
[default3]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default3]:shared memory (/dev/shm) size .... 251.60 GB
[default3]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default4]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default4]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default4]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:DeepSpeed general environment info:
[default4]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default4]:torch version .................... 2.2.1
[default4]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default4]:deepspeed info ................... 0.12.6, unknown, unknown
[default4]:torch cuda version ............... 12.1
[default4]:torch hip version ................ None
[default4]:nvcc version ..................... 12.1
[default4]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default4]:shared memory (/dev/shm) size .... 251.60 GB
[default4]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:[2024-03-28 23:37:45,491] [INFO] [comm.py:637:init_distributed] cdb=None
[default7]:[2024-03-28 23:37:46,358] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-03-28 23:37:46,354] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-03-28 23:37:46,374] [INFO] [comm.py:637:init_distributed] cdb=None
[default6]:[2024-03-28 23:37:46,392] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:[2024-03-28 23:37:46,392] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-03-28 23:37:46,376] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-03-28 23:37:46,386] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-03-28 23:38:27,945] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default7]:[2024-03-28 23:38:27,945] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default6]:[2024-03-28 23:38:27,955] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default5]:[2024-03-28 23:38:27,945] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default3]:[2024-03-28 23:38:27,944] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default4]:[2024-03-28 23:38:27,955] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default0]:[2024-03-28 23:38:27,955] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default2]:[2024-03-28 23:38:27,954] [INFO] [engine.py:139:__init__] is_pipe_partitioned= True is_grad_partitioned= True
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         A                                                                                                                                                                                                                                                                                     [default1]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default6]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:(min, max) time across ranks (ms):
[default7]:    load-checkpoint ................................: (0.94, 1.17)
[default3]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default5]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default4]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:(min, max) time across ranks (ms):
[default7]:    model-and-optimizer-setup ......................: (3046.00, 3308.64)
[default7]:    train/valid/test-data-iterators-setup ..........: (1737.86, 4450.87)
[default7]: iteration        1/  250000 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 9310.3 | learning rate: 1.500E-07 | global batch size:   192 | lm loss: 1.121016E+01 | grad norm: 16.406 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.622 | TFLOPs: 56.05 |
[default7]: iteration        2/  250000 | consumed samples:          384 | consumed tokens:       786432 | elapsed time per iteration (ms): 5663.3 | learning rate: 3.000E-07 | global batch size:   192 | lm loss: 1.120560E+01 | grad norm: 17.399 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.902 | TFLOPs: 92.14 |
[default7]: iteration        3/  250000 | consumed samples:          576 | consumed tokens:      1179648 | elapsed time per iteration (ms): 5659.2 | learning rate: 4.500E-07 | global batch size:   192 | lm loss: 1.120842E+01 | grad norm: 18.024 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.927 | TFLOPs: 92.21 |
[default7]: iteration        4/  250000 | consumed samples:          768 | consumed tokens:      1572864 | elapsed time per iteration (ms): 5680.7 | learning rate: 6.000E-07 | global batch size:   192 | lm loss: 1.120066E+01 | grad norm: 18.127 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.799 | TFLOPs: 91.86 |
[default7]: iteration        5/  250000 | consumed samples:          960 | consumed tokens:      1966080 | elapsed time per iteration (ms): 5667.4 | learning rate: 7.500E-07 | global batch size:   192 | lm loss: 1.119030E+01 | grad norm: 18.549 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.878 | TFLOPs: 92.07 |
[default7]: iteration        6/  250000 | consumed samples:         1152 | consumed tokens:      2359296 | elapsed time per iteration (ms): 5675.4 | learning rate: 9.000E-07 | global batch size:   192 | lm loss: 1.115401E+01 | grad norm: 15.707 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.830 | TFLOPs: 91.94 |
[default7]: iteration        7/  250000 | consumed samples:         1344 | consumed tokens:      2752512 | elapsed time per iteration (ms): 5674.8 | learning rate: 1.050E-06 | global batch size:   192 | lm loss: 1.101444E+01 | grad norm: 19.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.834 | TFLOPs: 91.95 |
[default7]: iteration        8/  250000 | consumed samples:         1536 | consumed tokens:      3145728 | elapsed time per iteration (ms): 5671.6 | learning rate: 1.200E-06 | global batch size:   192 | lm loss: 1.096410E+01 | grad norm: 16.861 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.853 | TFLOPs: 92.01 |
[default7]: iteration        9/  250000 | consumed samples:         1728 | consumed tokens:      3538944 | elapsed time per iteration (ms): 5673.2 | learning rate: 1.350E-06 | global batch size:   192 | lm loss: 1.058966E+01 | grad norm: 19.264 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.843 | TFLOPs: 91.98 |
[default7]: iteration       10/  250000 | consumed samples:         1920 | consumed tokens:      3932160 | elapsed time per iteration (ms): 5686.6 | learning rate: 1.500E-06 | global batch size:   192 | lm loss: 1.052156E+01 | grad norm: 15.508 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.764 | TFLOPs: 91.76 |
[default7]: iteration       11/  250000 | consumed samples:         2112 | consumed tokens:      4325376 | elapsed time per iteration (ms): 5684.6 | learning rate: 1.650E-06 | global batch size:   192 | lm loss: 1.039925E+01 | grad norm: 19.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.775 | TFLOPs: 91.79 |
[default7]: iteration       12/  250000 | consumed samples:         2304 | consumed tokens:      4718592 | elapsed time per iteration (ms): 5677.0 | learning rate: 1.800E-06 | global batch size:   192 | lm loss: 1.011390E+01 | grad norm: 27.251 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.821 | TFLOPs: 91.92 |
[default7]: iteration       13/  250000 | consumed samples:         2496 | consumed tokens:      5111808 | elapsed time per iteration (ms): 5675.2 | learning rate: 1.950E-06 | global batch size:   192 | lm loss: 9.697062E+00 | grad norm: 37.077 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.831 | TFLOPs: 91.95 |
[default2]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default5]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default4]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-03-28 23:38:28,868] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-28 23:38:29 
[default0]:> building train, validation, and test datasets ...
[default0]: > datasets target sizes (minimum size):
[default0]:    train:      48000000
[default0]:    validation: 48009600
[default0]:    test:       9600
[default0]:> building train, validation, and test datasets for GPT ...
[default0]:Single data path provided for train, valid & test
[default0]: > building dataset index ...
[default0]:    reading sizes...
[default0]:    reading pointers...
[default0]:    reading document index...
[default0]:    creating numpy buffer of mmap...
[default0]:    creating memory view of numpy buffer...
[default0]: > finished creating indexed dataset in 0.007944 seconds
[default0]:    number of documents: 32615
[default0]: > dataset split:
[default0]:    train:
[default0]:     document indices in [0, 31604) total of 31604 documents
[default0]:    validation:
[default0]:     document indices in [31604, 32582) total of 978 documents
[default0]:    test:
[default0]:     document indices in [32582, 32615) total of 33 documents
[default0]: > loading doc-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/fe6c814c145b154028765c11a92796ba_doc_idx.npy
[default0]: > loading sample-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/fe6c814c145b154028765c11a92796ba_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/fe6c814c145b154028765c11a92796ba_shuffle_idx.npy
[default0]:    loaded indexed file in 0.049 seconds
[default0]:    total number of samples: 48000143
[default0]:    total number of epochs: 5351
[default0]: > loading doc-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/1b5fd6810d33e58956c7fde863bc028e_doc_idx.npy
[default0]: > loading sample-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/1b5fd6810d33e58956c7fde863bc028e_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/1b5fd6810d33e58956c7fde863bc028e_shuffle_idx.npy
[default0]:    loaded indexed file in 0.074 seconds
[default0]:    total number of samples: 48009625
[default0]:    total number of epochs: 182767
[default0]: > loading doc-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/45b127c277a3142470d94a5392ecbf56_doc_idx.npy
[default0]: > loading sample-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/45b127c277a3142470d94a5392ecbf56_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/45b127c277a3142470d94a5392ecbf56_shuffle_idx.npy
[default0]:    loaded indexed file in 0.005 seconds
[default0]:    total number of samples: 9606
[default0]:    total number of epochs: 1339
[default0]:> finished creating GPT datasets ...
[default0]:[after dataloaders are built] datetime: 2024-03-28 23:38:33 
[default0]:done with setup ...
[default0]:training ...
[default0]:[before the start of training step] datetime: 2024-03-28 23:38:33 
[default0]:[2024-03-28 23:38:42,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[1.5e-07, 1.5e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default1]:[Rank 1] (after 1 iterations) memory (MB) | allocated: 15360.81689453125 | max allocated: 55572.05908203125 | reserved: 60422.0 | max reserved: 60422.0
[default0]:steps: 1 loss: 11.2102 iter time (s): 9.308 samples/sec: 20.626
[default0]:[Rank 0] (after 1 iterations) memory (MB) | allocated: 15360.81689453125 | max allocated: 55572.05908203125 | reserved: 60420.0 | max reserved: 60420.0
[default0]:[2024-03-28 23:38:48,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[3e-07, 3e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 2 loss: 11.2056 iter time (s): 5.658 samples/sec: 33.934
[default0]:[2024-03-28 23:38:54,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[4.4999999999999993e-07, 4.4999999999999993e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 3 loss: 11.2084 iter time (s): 5.651 samples/sec: 33.975
[default0]:[2024-03-28 23:38:59,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[6e-07, 6e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 4 loss: 11.2007 iter time (s): 5.676 samples/sec: 33.828
[default0]:[2024-03-28 23:39:05,593] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[7.499999999999999e-07, 7.499999999999999e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 5 loss: 11.1903 iter time (s): 5.663 samples/sec: 33.906
[default0]:[2024-03-28 23:39:11,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[8.999999999999999e-07, 8.999999999999999e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 6 loss: 11.1540 iter time (s): 5.671 samples/sec: 33.857
[default0]:[2024-03-28 23:39:16,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[1.05e-06, 1.05e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 7 loss: 11.0144 iter time (s): 5.670 samples/sec: 33.865
[default0]:[2024-03-28 23:39:22,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[1.2e-06, 1.2e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 8 loss: 10.9641 iter time (s): 5.667 samples/sec: 33.881
[default0]:[2024-03-28 23:39:28,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[1.35e-06, 1.35e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 9 loss: 10.5897 iter time (s): 5.668 samples/sec: 33.872
[default0]:[2024-03-28 23:39:33,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.4999999999999998e-06, 1.4999999999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 10 loss: 10.5216 iter time (s): 5.682 samples/sec: 33.791
[default0]:[2024-03-28 23:39:39,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=11, skipped=0, lr=[1.6499999999999999e-06, 1.6499999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 11 loss: 10.3992 iter time (s): 5.680 samples/sec: 33.803
[default0]:[2024-03-28 23:39:45,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=12, skipped=0, lr=[1.7999999999999997e-06, 1.7999999999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 12 loss: 10.1139 iter time (s): 5.672 samples/sec: 33.850
[default0]:[2024-03-28 23:39:51,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=13, skipped=0, lr=[1.95e-06, 1.95e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 13 loss: 9.6971 iter time (s): 5.671 samples/sec: 33.858
[default0]:[2024-03-28 23:39:56,693] [INFO] [logging.py:96:log_dist] [Rank 0] step=14, skipped=0, lr=[2.1e-06, 2.1e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 14 loss: 9.5139 iter time (s): 5.674 samples/sec: 33.836
[default0]:[2024-03-28 23:40:02,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=15, skipped=0, lr=[2.2499999999999996e-06, 2.2499999999999996e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 15 loss: 9.2730 iter time (s): 5.672 samples/sec: 33.851
[default7]: iteration       14/  250000 | consumed samples:         2688 | consumed tokens:      5505024 | elapsed time per iteration (ms): 5679.4 | learning rate: 2.100E-06 | global batch size:   192 | lm loss: 9.513901E+00 | grad norm: 26.003 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.806 | TFLOPs: 91.88 |
[default7]: iteration       15/  250000 | consumed samples:         2880 | consumed tokens:      5898240 | elapsed time per iteration (ms): 5676.7 | learning rate: 2.250E-06 | global batch size:   192 | lm loss: 9.272977E+00 | grad norm: 24.826 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.822 | TFLOPs: 91.92 |
[default7]: iteration       16/  250000 | consumed samples:         3072 | consumed tokens:      6291456 | elapsed time per iteration (ms): 5667.7 | learning rate: 2.400E-06 | global batch size:   192 | lm loss: 8.960623E+00 | grad norm: 27.994 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.876 | TFLOPs: 92.07 |
[default7]: iteration       17/  250000 | consumed samples:         3264 | consumed tokens:      6684672 | elapsed time per iteration (ms): 5679.9 | learning rate: 2.550E-06 | global batch size:   192 | lm loss: 8.899236E+00 | grad norm: 18.142 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.803 | TFLOPs: 91.87 |
[default7]: iteration       18/  250000 | consumed samples:         3456 | consumed tokens:      7077888 | elapsed time per iteration (ms): 5679.2 | learning rate: 2.700E-06 | global batch size:   192 | lm loss: 8.793050E+00 | grad norm: 17.804 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.808 | TFLOPs: 91.88 |
[default7]: iteration       19/  250000 | consumed samples:         3648 | consumed tokens:      7471104 | elapsed time per iteration (ms): 5679.1 | learning rate: 2.850E-06 | global batch size:   192 | lm loss: 8.475891E+00 | grad norm: 17.631 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.808 | TFLOPs: 91.88 |
[default7]: iteration       20/  250000 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 13821.6 | learning rate: 3.000E-06 | global batch size:   192 | lm loss: 8.332827E+00 | grad norm: 19.323 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.891 | TFLOPs: 37.75 |
[default7]: iteration       21/  250000 | consumed samples:         4032 | consumed tokens:      8257536 | elapsed time per iteration (ms): 5671.8 | learning rate: 3.150E-06 | global batch size:   192 | lm loss: 8.583655E+00 | grad norm: 17.975 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.852 | TFLOPs: 92.00 |
[default7]: iteration       22/  250000 | consumed samples:         4224 | consumed tokens:      8650752 | elapsed time per iteration (ms): 5675.2 | learning rate: 3.300E-06 | global batch size:   192 | lm loss: 8.214067E+00 | grad norm: 16.762 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.832 | TFLOPs: 91.95 |
[default7]: iteration       23/  250000 | consumed samples:         4416 | consumed tokens:      9043968 | elapsed time per iteration (ms): 5672.6 | learning rate: 3.450E-06 | global batch size:   192 | lm loss: 8.264475E+00 | grad norm: 20.752 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.847 | TFLOPs: 91.99 |
[default7]: iteration       24/  250000 | consumed samples:         4608 | consumed tokens:      9437184 | elapsed time per iteration (ms): 5684.5 | learning rate: 3.600E-06 | global batch size:   192 | lm loss: 7.985150E+00 | grad norm: 15.854 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.776 | TFLOPs: 91.80 |
[default7]: iteration       25/  250000 | consumed samples:         4800 | consumed tokens:      9830400 | elapsed time per iteration (ms): 5688.9 | learning rate: 3.750E-06 | global batch size:   192 | lm loss: 7.900833E+00 | grad norm: 12.262 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.750 | TFLOPs: 91.73 |
[default7]: iteration       26/  250000 | consumed samples:         4992 | consumed tokens:     10223616 | elapsed time per iteration (ms): 5686.3 | learning rate: 3.900E-06 | global batch size:   192 | lm loss: 7.906753E+00 | grad norm: 11.602 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.766 | TFLOPs: 91.77 |
[default7]: iteration       27/  250000 | consumed samples:         5184 | consumed tokens:     10616832 | elapsed time per iteration (ms): 5683.5 | learning rate: 4.050E-06 | global batch size:   192 | lm loss: 7.702158E+00 | grad norm: 11.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.782 | TFLOPs: 91.81 |
[default7]: iteration       28/  250000 | consumed samples:         5376 | consumed tokens:     11010048 | elapsed time per iteration (ms): 5682.6 | learning rate: 4.200E-06 | global batch size:   192 | lm loss: 7.672709E+00 | grad norm: 10.849 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.787 | TFLOPs: 91.83 |
[default7]: iteration       29/  250000 | consumed samples:         5568 | consumed tokens:     11403264 | elapsed time per iteration (ms): 5685.7 | learning rate: 4.350E-06 | global batch size:   192 | lm loss: 7.747583E+00 | grad norm: 7.826 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.769 | TFLOPs: 91.78 |
[default7]: iteration       30/  250000 | consumed samples:         5760 | consumed tokens:     11796480 | elapsed time per iteration (ms): 5679.4 | learning rate: 4.500E-06 | global batch size:   192 | lm loss: 7.808384E+00 | grad norm: 9.284 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.806 | TFLOPs: 91.88 |
[default7]: iteration       31/  250000 | consumed samples:         5952 | consumed tokens:     12189696 | elapsed time per iteration (ms): 5672.2 | learning rate: 4.650E-06 | global batch size:   192 | lm loss: 7.679717E+00 | grad norm: 8.085 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.849 | TFLOPs: 92.00 |
[default7]: iteration       32/  250000 | consumed samples:         6144 | consumed tokens:     12582912 | elapsed time per iteration (ms): 5681.1 | learning rate: 4.800E-06 | global batch size:   192 | lm loss: 7.575779E+00 | grad norm: 7.564 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.796 | TFLOPs: 91.85 |
[default7]: iteration       33/  250000 | consumed samples:         6336 | consumed tokens:     12976128 | elapsed time per iteration (ms): 5681.1 | learning rate: 4.950E-06 | global batch size:   192 | lm loss: 7.476647E+00 | grad norm: 7.566 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.796 | TFLOPs: 91.85 |
[default0]:[2024-03-28 23:40:08,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=16, skipped=0, lr=[2.4e-06, 2.4e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 16 loss: 8.9606 iter time (s): 5.663 samples/sec: 33.904
[default0]:[2024-03-28 23:40:13,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=17, skipped=0, lr=[2.5499999999999997e-06, 2.5499999999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 17 loss: 8.8992 iter time (s): 5.675 samples/sec: 33.830
[default0]:[2024-03-28 23:40:19,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=18, skipped=0, lr=[2.7e-06, 2.7e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 18 loss: 8.7930 iter time (s): 5.675 samples/sec: 33.835
[default0]:[2024-03-28 23:40:25,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=19, skipped=0, lr=[2.8499999999999994e-06, 2.8499999999999994e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 19 loss: 8.4759 iter time (s): 5.674 samples/sec: 33.836
[default0]:[2024-03-28 23:40:38,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[2.9999999999999997e-06, 2.9999999999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 20 loss: 8.3328 iter time (s): 13.817 samples/sec: 13.896
[default0]:[2024-03-28 23:40:44,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=21, skipped=0, lr=[3.1499999999999995e-06, 3.1499999999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 21 loss: 8.5837 iter time (s): 5.667 samples/sec: 33.883
[default0]:[2024-03-28 23:40:50,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=22, skipped=0, lr=[3.2999999999999997e-06, 3.2999999999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 22 loss: 8.2141 iter time (s): 5.670 samples/sec: 33.860
[default0]:[2024-03-28 23:40:55,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=23, skipped=0, lr=[3.45e-06, 3.45e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 23 loss: 8.2645 iter time (s): 5.668 samples/sec: 33.873
[default0]:[2024-03-28 23:41:01,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=24, skipped=0, lr=[3.5999999999999994e-06, 3.5999999999999994e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 24 loss: 7.9852 iter time (s): 5.680 samples/sec: 33.805
[default0]:[2024-03-28 23:41:07,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=25, skipped=0, lr=[3.7499999999999997e-06, 3.7499999999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 25 loss: 7.9008 iter time (s): 5.684 samples/sec: 33.778
[default0]:[2024-03-28 23:41:12,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=26, skipped=0, lr=[3.9e-06, 3.9e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 26 loss: 7.9068 iter time (s): 5.681 samples/sec: 33.795
[default0]:[2024-03-28 23:41:18,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=27, skipped=0, lr=[4.05e-06, 4.05e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 27 loss: 7.7022 iter time (s): 5.679 samples/sec: 33.811
[default0]:[2024-03-28 23:41:24,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=28, skipped=0, lr=[4.2e-06, 4.2e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 28 loss: 7.6727 iter time (s): 5.678 samples/sec: 33.815
[default0]:[2024-03-28 23:41:30,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=29, skipped=0, lr=[4.35e-06, 4.35e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 29 loss: 7.7476 iter time (s): 5.681 samples/sec: 33.797
[default0]:[2024-03-28 23:41:35,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[4.499999999999999e-06, 4.499999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 30 loss: 7.8084 iter time (s): 5.675 samples/sec: 33.834
[default0]:[2024-03-28 23:41:41,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=31, skipped=0, lr=[4.6499999999999995e-06, 4.6499999999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 31 loss: 7.6797 iter time (s): 5.667 samples/sec: 33.879
[default0]:[2024-03-28 23:41:47,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=32, skipped=0, lr=[4.8e-06, 4.8e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 32 loss: 7.5758 iter time (s): 5.676 samples/sec: 33.827
[default0]:[2024-03-28 23:41:52,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=33, skipped=0, lr=[4.949999999999999e-06, 4.949999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 33 loss: 7.4766 iter time (s): 5.672 samples/sec: 33.848
[default0]:[2024-03-28 23:41:58,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=34, skipped=0, lr=[5.0999999999999995e-06, 5.0999999999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 34 loss: 7.4401 iter time (s): 5.670 samples/sec: 33.862
[default0]:[2024-03-28 23:42:04,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=35, skipped=0, lr=[5.25e-06, 5.25e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 35 loss: 7.3982 iter time (s): 5.692 samples/sec: 33.732
[default0]:[2024-03-28 23:42:09,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=36, skipped=0, lr=[5.4e-06, 5.4e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 36 loss: 7.3779 iter time (s): 5.679 samples/sec: 33.807
[default0]:[2024-03-28 23:42:15,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=37, skipped=0, lr=[5.549999999999999e-06, 5.549999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 37 loss: 7.1561 iter time (s): 5.679 samples/sec: 33.810
[default0]:[2024-03-28 23:42:21,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=38, skipped=0, lr=[5.699999999999999e-06, 5.699999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 38 loss: 7.3959 iter time (s): 5.672 samples/sec: 33.853
[default0]:[2024-03-28 23:42:26,837] [INFO] [logging.py:96:log_dist] [Rank 0] step=39, skipped=0, lr=[5.85e-06, 5.85e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 39 loss: 7.3203 iter time (s): 5.669 samples/sec: 33.867
[default0]:[2024-03-28 23:42:32,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[5.999999999999999e-06, 5.999999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 40 loss: 7.2976 iter time (s): 5.669 samples/sec: 33.867
[default0]:[2024-03-28 23:42:38,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=41, skipped=0, lr=[6.1499999999999996e-06, 6.1499999999999996e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 41 loss: 7.1539 iter time (s): 5.663 samples/sec: 33.904
[default0]:[2024-03-28 23:42:43,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=42, skipped=0, lr=[6.299999999999999e-06, 6.299999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 42 loss: 7.0670 iter time (s): 5.671 samples/sec: 33.857
[default0]:[2024-03-28 23:42:49,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=43, skipped=0, lr=[6.45e-06, 6.45e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 43 loss: 7.1962 iter time (s): 5.668 samples/sec: 33.877
[default0]:[2024-03-28 23:42:55,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=44, skipped=0, lr=[6.5999999999999995e-06, 6.5999999999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 44 loss: 7.0495 iter time (s): 5.669 samples/sec: 33.870
[default0]:[2024-03-28 23:43:00,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=45, skipped=0, lr=[6.749999999999999e-06, 6.749999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 45 loss: 7.2126 iter time (s): 5.664 samples/sec: 33.899
[default0]:[2024-03-28 23:43:06,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=46, skipped=0, lr=[6.9e-06, 6.9e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 46 loss: 7.1565 iter time (s): 5.681 samples/sec: 33.797
[default0]:[2024-03-28 23:43:12,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=47, skipped=0, lr=[7.049999999999999e-06, 7.049999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 47 loss: 6.9630 iter time (s): 5.672 samples/sec: 33.853
[default0]:[2024-03-28 23:43:17,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=48, skipped=0, lr=[7.199999999999999e-06, 7.199999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 48 loss: 6.8815 iter time (s): 5.668 samples/sec: 33.872
[default7]: iteration       34/  250000 | consumed samples:         6528 | consumed tokens:     13369344 | elapsed time per iteration (ms): 5674.7 | learning rate: 5.100E-06 | global batch size:   192 | lm loss: 7.440109E+00 | grad norm: 7.049 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.834 | TFLOPs: 91.96 |
[default7]: iteration       35/  250000 | consumed samples:         6720 | consumed tokens:     13762560 | elapsed time per iteration (ms): 5696.9 | learning rate: 5.250E-06 | global batch size:   192 | lm loss: 7.398230E+00 | grad norm: 7.346 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.703 | TFLOPs: 91.60 |
[default7]: iteration       36/  250000 | consumed samples:         6912 | consumed tokens:     14155776 | elapsed time per iteration (ms): 5683.9 | learning rate: 5.400E-06 | global batch size:   192 | lm loss: 7.377875E+00 | grad norm: 6.021 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.780 | TFLOPs: 91.81 |
[default7]: iteration       37/  250000 | consumed samples:         7104 | consumed tokens:     14548992 | elapsed time per iteration (ms): 5683.5 | learning rate: 5.550E-06 | global batch size:   192 | lm loss: 7.156075E+00 | grad norm: 7.535 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.782 | TFLOPs: 91.81 |
[default7]: iteration       38/  250000 | consumed samples:         7296 | consumed tokens:     14942208 | elapsed time per iteration (ms): 5676.2 | learning rate: 5.700E-06 | global batch size:   192 | lm loss: 7.395934E+00 | grad norm: 6.065 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.826 | TFLOPs: 91.93 |
[default7]: iteration       39/  250000 | consumed samples:         7488 | consumed tokens:     15335424 | elapsed time per iteration (ms): 5673.8 | learning rate: 5.850E-06 | global batch size:   192 | lm loss: 7.320284E+00 | grad norm: 6.737 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.840 | TFLOPs: 91.97 |
[default7]: iteration       40/  250000 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 5673.8 | learning rate: 6.000E-06 | global batch size:   192 | lm loss: 7.297640E+00 | grad norm: 5.295 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.840 | TFLOPs: 91.97 |
[default7]: iteration       41/  250000 | consumed samples:         7872 | consumed tokens:     16121856 | elapsed time per iteration (ms): 5668.1 | learning rate: 6.150E-06 | global batch size:   192 | lm loss: 7.153875E+00 | grad norm: 6.304 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.874 | TFLOPs: 92.06 |
[default7]: iteration       42/  250000 | consumed samples:         8064 | consumed tokens:     16515072 | elapsed time per iteration (ms): 5675.5 | learning rate: 6.300E-06 | global batch size:   192 | lm loss: 7.066986E+00 | grad norm: 6.482 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.830 | TFLOPs: 91.94 |
[default7]: iteration       43/  250000 | consumed samples:         8256 | consumed tokens:     16908288 | elapsed time per iteration (ms): 5675.2 | learning rate: 6.450E-06 | global batch size:   192 | lm loss: 7.196230E+00 | grad norm: 5.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.831 | TFLOPs: 91.95 |
[default7]: iteration       44/  250000 | consumed samples:         8448 | consumed tokens:     17301504 | elapsed time per iteration (ms): 5673.4 | learning rate: 6.600E-06 | global batch size:   192 | lm loss: 7.049467E+00 | grad norm: 4.296 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.842 | TFLOPs: 91.98 |
[default7]: iteration       45/  250000 | consumed samples:         8640 | consumed tokens:     17694720 | elapsed time per iteration (ms): 5669.5 | learning rate: 6.750E-06 | global batch size:   192 | lm loss: 7.212623E+00 | grad norm: 6.352 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.866 | TFLOPs: 92.04 |
[default7]: iteration       46/  250000 | consumed samples:         8832 | consumed tokens:     18087936 | elapsed time per iteration (ms): 5685.7 | learning rate: 6.900E-06 | global batch size:   192 | lm loss: 7.156471E+00 | grad norm: 5.899 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.769 | TFLOPs: 91.78 |
[default7]: iteration       47/  250000 | consumed samples:         9024 | consumed tokens:     18481152 | elapsed time per iteration (ms): 5676.9 | learning rate: 7.050E-06 | global batch size:   192 | lm loss: 6.962984E+00 | grad norm: 5.532 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.821 | TFLOPs: 91.92 |
[default7]: iteration       48/  250000 | consumed samples:         9216 | consumed tokens:     18874368 | elapsed time per iteration (ms): 5673.1 | learning rate: 7.200E-06 | global batch size:   192 | lm loss: 6.881532E+00 | grad norm: 5.074 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.844 | TFLOPs: 91.98 |
[default7]: iteration       49/  250000 | consumed samples:         9408 | consumed tokens:     19267584 | elapsed time per iteration (ms): 5684.5 | learning rate: 7.350E-06 | global batch size:   192 | lm loss: 6.971451E+00 | grad norm: 4.402 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.776 | TFLOPs: 91.80 |
[default7]: iteration       50/  250000 | consumed samples:         9600 | consumed tokens:     19660800 | elapsed time per iteration (ms): 5681.1 | learning rate: 7.500E-06 | global batch size:   192 | lm loss: 6.909847E+00 | grad norm: 5.523 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.796 | TFLOPs: 91.85 |
[default7]:----------------------------------------------------------------------------------------------
[default7]: validation loss at iteration 50 | lm loss value: 6.959445E+00 | lm loss PPL: 1.053049E+03 | 
[default7]:----------------------------------------------------------------------------------------------
[default7]: iteration       51/  250000 | consumed samples:         9792 | consumed tokens:     20054016 | elapsed time per iteration (ms): 78592.4 | learning rate: 7.650E-06 | global batch size:   192 | lm loss: 6.956328E+00 | grad norm: 6.951 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.443 | TFLOPs: 6.64 |
[default7]: iteration       52/  250000 | consumed samples:         9984 | consumed tokens:     20447232 | elapsed time per iteration (ms): 5685.7 | learning rate: 7.800E-06 | global batch size:   192 | lm loss: 6.718202E+00 | grad norm: 8.390 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.769 | TFLOPs: 91.78 |
[default7]: iteration       53/  250000 | consumed samples:        10176 | consumed tokens:     20840448 | elapsed time per iteration (ms): 5681.0 | learning rate: 7.950E-06 | global batch size:   192 | lm loss: 6.993049E+00 | grad norm: 8.134 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.797 | TFLOPs: 91.85 |
[default7]: iteration       54/  250000 | consumed samples:        10368 | consumed tokens:     21233664 | elapsed time per iteration (ms): 5688.8 | learning rate: 8.100E-06 | global batch size:   192 | lm loss: 6.872223E+00 | grad norm: 5.771 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.751 | TFLOPs: 91.73 |
[default7]: iteration       55/  250000 | consumed samples:        10560 | consumed tokens:     21626880 | elapsed time per iteration (ms): 5682.6 | learning rate: 8.250E-06 | global batch size:   192 | lm loss: 6.788619E+00 | grad norm: 14.564 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.787 | TFLOPs: 91.83 |
[default7]: iteration       56/  250000 | consumed samples:        10752 | consumed tokens:     22020096 | elapsed time per iteration (ms): 5693.1 | learning rate: 8.400E-06 | global batch size:   192 | lm loss: 6.975024E+00 | grad norm: 13.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.725 | TFLOPs: 91.66 |
[default7]: iteration       57/  250000 | consumed samples:        10944 | consumed tokens:     22413312 | elapsed time per iteration (ms): 5685.9 | learning rate: 8.550E-06 | global batch size:   192 | lm loss: 7.062508E+00 | grad norm: 10.947 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.768 | TFLOPs: 91.77 |
[default7]: iteration       58/  250000 | consumed samples:        11136 | consumed tokens:     22806528 | elapsed time per iteration (ms): 5678.3 | learning rate: 8.700E-06 | global batch size:   192 | lm loss: 6.775141E+00 | grad norm: 9.238 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.813 | TFLOPs: 91.90 |
[default7]: iteration       59/  250000 | consumed samples:        11328 | consumed tokens:     23199744 | elapsed time per iteration (ms): 5676.6 | learning rate: 8.850E-06 | global batch size:   192 | lm loss: 7.075814E+00 | grad norm: 41.727 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.823 | TFLOPs: 91.92 |
[default7]: iteration       60/  250000 | consumed samples:        11520 | consumed tokens:     23592960 | elapsed time per iteration (ms): 5675.2 | learning rate: 9.000E-06 | global batch size:   192 | lm loss: 6.935859E+00 | grad norm: 13.108 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.831 | TFLOPs: 91.95 |
[default7]: iteration       61/  250000 | consumed samples:        11712 | consumed tokens:     23986176 | elapsed time per iteration (ms): 5685.7 | learning rate: 9.150E-06 | global batch size:   192 | lm loss: 7.054832E+00 | grad norm: 12.555 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.769 | TFLOPs: 91.78 |
[default7]: iteration       62/  250000 | consumed samples:        11904 | consumed tokens:     24379392 | elapsed time per iteration (ms): 8574.9 | learning rate: 9.300E-06 | global batch size:   192 | lm loss: 7.139392E+00 | grad norm: 10.022 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.391 | TFLOPs: 60.85 |
[default7]: iteration       63/  250000 | consumed samples:        12096 | consumed tokens:     24772608 | elapsed time per iteration (ms): 5678.6 | learning rate: 9.450E-06 | global batch size:   192 | lm loss: 6.893945E+00 | grad norm: 8.304 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.811 | TFLOPs: 91.89 |
[default7]: iteration       64/  250000 | consumed samples:        12288 | consumed tokens:     25165824 | elapsed time per iteration (ms): 8841.4 | learning rate: 9.600E-06 | global batch size:   192 | lm loss: 6.745643E+00 | grad norm: 11.326 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.716 | TFLOPs: 59.02 |
[default7]: iteration       65/  250000 | consumed samples:        12480 | consumed tokens:     25559040 | elapsed time per iteration (ms): 5673.2 | learning rate: 9.750E-06 | global batch size:   192 | lm loss: 6.792784E+00 | grad norm: 5.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.843 | TFLOPs: 91.98 |
[default7]: iteration       66/  250000 | consumed samples:        12672 | consumed tokens:     25952256 | elapsed time per iteration (ms): 5685.9 | learning rate: 9.900E-06 | global batch size:   192 | lm loss: 6.976478E+00 | grad norm: 30.850 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.768 | TFLOPs: 91.77 |
[default7]: iteration       67/  250000 | consumed samples:        12864 | consumed tokens:     26345472 | elapsed time per iteration (ms): 5681.4 | learning rate: 1.005E-05 | global batch size:   192 | lm loss: 6.649801E+00 | grad norm: 9.425 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.795 | TFLOPs: 91.85 |
[default7]: iteration       68/  250000 | consumed samples:        13056 | consumed tokens:     26738688 | elapsed time per iteration (ms): 5697.5 | learning rate: 1.020E-05 | global batch size:   192 | lm loss: 6.743084E+00 | grad norm: 7.129 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.699 | TFLOPs: 91.59 |
[default7]: iteration       69/  250000 | consumed samples:        13248 | consumed tokens:     27131904 | elapsed time per iteration (ms): 5691.4 | learning rate: 1.035E-05 | global batch size:   192 | lm loss: 6.655822E+00 | grad norm: 7.051 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.735 | TFLOPs: 91.69 |
[default7]: iteration       70/  250000 | consumed samples:        13440 | consumed tokens:     27525120 | elapsed time per iteration (ms): 5687.5 | learning rate: 1.050E-05 | global batch size:   192 | lm loss: 6.725087E+00 | grad norm: 6.300 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.758 | TFLOPs: 91.75 |
[default7]: iteration       71/  250000 | consumed samples:        13632 | consumed tokens:     27918336 | elapsed time per iteration (ms): 5688.7 | learning rate: 1.065E-05 | global batch size:   192 | lm loss: 6.670179E+00 | grad norm: 5.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.751 | TFLOPs: 91.73 |
[default7]: iteration       72/  250000 | consumed samples:        13824 | consumed tokens:     28311552 | elapsed time per iteration (ms): 5683.9 | learning rate: 1.080E-05 | global batch size:   192 | lm loss: 6.679178E+00 | grad norm: 4.869 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.780 | TFLOPs: 91.81 |
[default0]:[2024-03-28 23:43:23,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=49, skipped=0, lr=[7.349999999999999e-06, 7.349999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 49 loss: 6.9715 iter time (s): 5.677 samples/sec: 33.823
[default0]:[2024-03-28 23:43:29,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[7.499999999999999e-06, 7.499999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 50 loss: 6.9098 iter time (s): 5.674 samples/sec: 33.842
[default0]:[2024-03-28 23:44:47,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=51, skipped=0, lr=[7.65e-06, 7.65e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 51 loss: 6.9563 iter time (s): 5.673 samples/sec: 33.842
[default0]:[2024-03-28 23:44:53,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=52, skipped=0, lr=[7.8e-06, 7.8e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 52 loss: 6.7182 iter time (s): 5.681 samples/sec: 33.797
[default0]:[2024-03-28 23:44:59,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=53, skipped=0, lr=[7.949999999999998e-06, 7.949999999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 53 loss: 6.9930 iter time (s): 5.676 samples/sec: 33.825
[default0]:[2024-03-28 23:45:04,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=54, skipped=0, lr=[8.1e-06, 8.1e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 54 loss: 6.8722 iter time (s): 5.684 samples/sec: 33.781
[default0]:[2024-03-28 23:45:10,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=55, skipped=0, lr=[8.249999999999999e-06, 8.249999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 55 loss: 6.7886 iter time (s): 5.678 samples/sec: 33.815
[default0]:[2024-03-28 23:45:16,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=56, skipped=0, lr=[8.4e-06, 8.4e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 56 loss: 6.9750 iter time (s): 5.688 samples/sec: 33.755
[default0]:[2024-03-28 23:45:21,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=57, skipped=0, lr=[8.55e-06, 8.55e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 57 loss: 7.0625 iter time (s): 5.681 samples/sec: 33.796
[default0]:[2024-03-28 23:45:27,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=58, skipped=0, lr=[8.7e-06, 8.7e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 58 loss: 6.7751 iter time (s): 5.673 samples/sec: 33.842
[default0]:[2024-03-28 23:45:33,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=59, skipped=0, lr=[8.849999999999998e-06, 8.849999999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 59 loss: 7.0758 iter time (s): 5.672 samples/sec: 33.852
[default0]:[2024-03-28 23:45:39,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[8.999999999999999e-06, 8.999999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 60 loss: 6.9359 iter time (s): 5.671 samples/sec: 33.859
[default0]:[2024-03-28 23:45:44,703] [INFO] [logging.py:96:log_dist] [Rank 0] step=61, skipped=0, lr=[9.149999999999999e-06, 9.149999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 61 loss: 7.0548 iter time (s): 5.681 samples/sec: 33.799
[default0]:[2024-03-28 23:45:53,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=62, skipped=0, lr=[9.299999999999999e-06, 9.299999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 62 loss: 7.1394 iter time (s): 8.570 samples/sec: 22.404
[default0]:[2024-03-28 23:45:58,952] [INFO] [logging.py:96:log_dist] [Rank 0] step=63, skipped=0, lr=[9.45e-06, 9.45e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 63 loss: 6.8939 iter time (s): 5.673 samples/sec: 33.842
[default0]:[2024-03-28 23:46:07,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=64, skipped=0, lr=[9.6e-06, 9.6e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 64 loss: 6.7456 iter time (s): 8.837 samples/sec: 21.727
[default0]:[2024-03-28 23:46:13,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=65, skipped=0, lr=[9.75e-06, 9.75e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 65 loss: 6.7928 iter time (s): 5.665 samples/sec: 33.892
[default0]:[2024-03-28 23:46:19,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=66, skipped=0, lr=[9.899999999999998e-06, 9.899999999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 66 loss: 6.9765 iter time (s): 5.681 samples/sec: 33.796
[default0]:[2024-03-28 23:46:24,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=67, skipped=0, lr=[1.0049999999999999e-05, 1.0049999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 67 loss: 6.6498 iter time (s): 5.677 samples/sec: 33.822
[default0]:[2024-03-28 23:46:30,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=68, skipped=0, lr=[1.0199999999999999e-05, 1.0199999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 68 loss: 6.7431 iter time (s): 5.688 samples/sec: 33.753
[default0]:[2024-03-28 23:46:36,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=69, skipped=0, lr=[1.035e-05, 1.035e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 69 loss: 6.6558 iter time (s): 5.687 samples/sec: 33.763
[default0]:[2024-03-28 23:46:41,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.05e-05, 1.05e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 70 loss: 6.7251 iter time (s): 5.683 samples/sec: 33.786
[default0]:[2024-03-28 23:46:47,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=71, skipped=0, lr=[1.065e-05, 1.065e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 71 loss: 6.6702 iter time (s): 5.684 samples/sec: 33.780
[default0]:[2024-03-28 23:46:53,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=72, skipped=0, lr=[1.08e-05, 1.08e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 72 loss: 6.6792 iter time (s): 5.678 samples/sec: 33.815
[default0]:[2024-03-28 23:46:58,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=73, skipped=0, lr=[1.0949999999999998e-05, 1.0949999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 73 loss: 6.7810 iter time (s): 5.686 samples/sec: 33.770
[default0]:[2024-03-28 23:47:04,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=74, skipped=0, lr=[1.1099999999999999e-05, 1.1099999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 74 loss: 6.5282 iter time (s): 5.681 samples/sec: 33.800
[default0]:[2024-03-28 23:47:10,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=75, skipped=0, lr=[1.1249999999999999e-05, 1.1249999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 75 loss: 6.6183 iter time (s): 5.682 samples/sec: 33.792
[default0]:[2024-03-28 23:47:16,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=76, skipped=0, lr=[1.1399999999999998e-05, 1.1399999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 76 loss: 6.5726 iter time (s): 5.672 samples/sec: 33.848
[default0]:[2024-03-28 23:47:21,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=77, skipped=0, lr=[1.155e-05, 1.155e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 77 loss: 6.5938 iter time (s): 5.675 samples/sec: 33.830
[default0]:[2024-03-28 23:47:27,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=78, skipped=0, lr=[1.17e-05, 1.17e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 78 loss: 6.5381 iter time (s): 5.676 samples/sec: 33.826
[default0]:[2024-03-28 23:47:33,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=79, skipped=0, lr=[1.185e-05, 1.185e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 79 loss: 6.4474 iter time (s): 5.672 samples/sec: 33.853
[default0]:[2024-03-28 23:47:38,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.1999999999999999e-05, 1.1999999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 80 loss: 6.5157 iter time (s): 5.674 samples/sec: 33.839
[default0]:[2024-03-28 23:47:44,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=81, skipped=0, lr=[1.2149999999999999e-05, 1.2149999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 81 loss: 6.3954 iter time (s): 5.675 samples/sec: 33.831
[default7]: iteration       73/  250000 | consumed samples:        14016 | consumed tokens:     28704768 | elapsed time per iteration (ms): 5693.5 | learning rate: 1.095E-05 | global batch size:   192 | lm loss: 6.781015E+00 | grad norm: 7.244 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.723 | TFLOPs: 91.65 |
[default7]: iteration       74/  250000 | consumed samples:        14208 | consumed tokens:     29097984 | elapsed time per iteration (ms): 5685.1 | learning rate: 1.110E-05 | global batch size:   192 | lm loss: 6.528179E+00 | grad norm: 5.406 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.773 | TFLOPs: 91.79 |
[default7]: iteration       75/  250000 | consumed samples:        14400 | consumed tokens:     29491200 | elapsed time per iteration (ms): 5686.7 | learning rate: 1.125E-05 | global batch size:   192 | lm loss: 6.618286E+00 | grad norm: 4.552 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.763 | TFLOPs: 91.76 |
[default7]: iteration       76/  250000 | consumed samples:        14592 | consumed tokens:     29884416 | elapsed time per iteration (ms): 5678.5 | learning rate: 1.140E-05 | global batch size:   192 | lm loss: 6.572556E+00 | grad norm: 3.962 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.812 | TFLOPs: 91.89 |
[default7]: iteration       77/  250000 | consumed samples:        14784 | consumed tokens:     30277632 | elapsed time per iteration (ms): 5680.7 | learning rate: 1.155E-05 | global batch size:   192 | lm loss: 6.593817E+00 | grad norm: 4.870 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.799 | TFLOPs: 91.86 |
[default7]: iteration       78/  250000 | consumed samples:        14976 | consumed tokens:     30670848 | elapsed time per iteration (ms): 5681.6 | learning rate: 1.170E-05 | global batch size:   192 | lm loss: 6.538115E+00 | grad norm: 4.096 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.793 | TFLOPs: 91.84 |
[default7]: iteration       79/  250000 | consumed samples:        15168 | consumed tokens:     31064064 | elapsed time per iteration (ms): 5676.8 | learning rate: 1.185E-05 | global batch size:   192 | lm loss: 6.447375E+00 | grad norm: 4.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.822 | TFLOPs: 91.92 |
[default7]: iteration       80/  250000 | consumed samples:        15360 | consumed tokens:     31457280 | elapsed time per iteration (ms): 5679.3 | learning rate: 1.200E-05 | global batch size:   192 | lm loss: 6.515679E+00 | grad norm: 4.718 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.807 | TFLOPs: 91.88 |
[default7]: iteration       81/  250000 | consumed samples:        15552 | consumed tokens:     31850496 | elapsed time per iteration (ms): 5680.3 | learning rate: 1.215E-05 | global batch size:   192 | lm loss: 6.395402E+00 | grad norm: 3.636 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.801 | TFLOPs: 91.87 |
[default7]: iteration       82/  250000 | consumed samples:        15744 | consumed tokens:     32243712 | elapsed time per iteration (ms): 5689.8 | learning rate: 1.230E-05 | global batch size:   192 | lm loss: 6.359756E+00 | grad norm: 3.842 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.745 | TFLOPs: 91.71 |
[default7]: iteration       83/  250000 | consumed samples:        15936 | consumed tokens:     32636928 | elapsed time per iteration (ms): 5689.4 | learning rate: 1.245E-05 | global batch size:   192 | lm loss: 6.406954E+00 | grad norm: 3.539 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.747 | TFLOPs: 91.72 |
[default7]: iteration       84/  250000 | consumed samples:        16128 | consumed tokens:     33030144 | elapsed time per iteration (ms): 5690.0 | learning rate: 1.260E-05 | global batch size:   192 | lm loss: 6.410781E+00 | grad norm: 3.646 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.743 | TFLOPs: 91.71 |
[default7]: iteration       85/  250000 | consumed samples:        16320 | consumed tokens:     33423360 | elapsed time per iteration (ms): 5696.7 | learning rate: 1.275E-05 | global batch size:   192 | lm loss: 6.383521E+00 | grad norm: 3.879 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.704 | TFLOPs: 91.60 |
[default7]: iteration       86/  250000 | consumed samples:        16512 | consumed tokens:     33816576 | elapsed time per iteration (ms): 5688.0 | learning rate: 1.290E-05 | global batch size:   192 | lm loss: 6.336535E+00 | grad norm: 3.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.755 | TFLOPs: 91.74 |
[default7]: iteration       87/  250000 | consumed samples:        16704 | consumed tokens:     34209792 | elapsed time per iteration (ms): 5683.4 | learning rate: 1.305E-05 | global batch size:   192 | lm loss: 6.371501E+00 | grad norm: 4.060 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.782 | TFLOPs: 91.81 |
[default7]: iteration       88/  250000 | consumed samples:        16896 | consumed tokens:     34603008 | elapsed time per iteration (ms): 5691.0 | learning rate: 1.320E-05 | global batch size:   192 | lm loss: 6.379862E+00 | grad norm: 2.748 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.738 | TFLOPs: 91.69 |
[default7]: iteration       89/  250000 | consumed samples:        17088 | consumed tokens:     34996224 | elapsed time per iteration (ms): 5694.9 | learning rate: 1.335E-05 | global batch size:   192 | lm loss: 6.354602E+00 | grad norm: 3.328 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.714 | TFLOPs: 91.63 |
[default7]: iteration       90/  250000 | consumed samples:        17280 | consumed tokens:     35389440 | elapsed time per iteration (ms): 5692.9 | learning rate: 1.350E-05 | global batch size:   192 | lm loss: 6.238621E+00 | grad norm: 3.483 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.726 | TFLOPs: 91.66 |
[default7]: iteration       91/  250000 | consumed samples:        17472 | consumed tokens:     35782656 | elapsed time per iteration (ms): 5691.6 | learning rate: 1.365E-05 | global batch size:   192 | lm loss: 6.402470E+00 | grad norm: 4.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.734 | TFLOPs: 91.68 |
[default7]: iteration       92/  250000 | consumed samples:        17664 | consumed tokens:     36175872 | elapsed time per iteration (ms): 5693.1 | learning rate: 1.380E-05 | global batch size:   192 | lm loss: 6.248834E+00 | grad norm: 4.344 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.725 | TFLOPs: 91.66 |
[default7]: iteration       93/  250000 | consumed samples:        17856 | consumed tokens:     36569088 | elapsed time per iteration (ms): 5686.3 | learning rate: 1.395E-05 | global batch size:   192 | lm loss: 6.262490E+00 | grad norm: 3.656 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.765 | TFLOPs: 91.77 |
[default7]: iteration       94/  250000 | consumed samples:        18048 | consumed tokens:     36962304 | elapsed time per iteration (ms): 5679.9 | learning rate: 1.410E-05 | global batch size:   192 | lm loss: 6.333262E+00 | grad norm: 5.588 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.804 | TFLOPs: 91.87 |
[default7]: iteration       95/  250000 | consumed samples:        18240 | consumed tokens:     37355520 | elapsed time per iteration (ms): 5682.4 | learning rate: 1.425E-05 | global batch size:   192 | lm loss: 6.353318E+00 | grad norm: 6.891 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.788 | TFLOPs: 91.83 |
[default7]: iteration       96/  250000 | consumed samples:        18432 | consumed tokens:     37748736 | elapsed time per iteration (ms): 5684.4 | learning rate: 1.440E-05 | global batch size:   192 | lm loss: 6.242734E+00 | grad norm: 11.331 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.777 | TFLOPs: 91.80 |
[default7]: iteration       97/  250000 | consumed samples:        18624 | consumed tokens:     38141952 | elapsed time per iteration (ms): 5681.9 | learning rate: 1.455E-05 | global batch size:   192 | lm loss: 6.148460E+00 | grad norm: 7.737 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.792 | TFLOPs: 91.84 |
[default7]: iteration       98/  250000 | consumed samples:        18816 | consumed tokens:     38535168 | elapsed time per iteration (ms): 5690.6 | learning rate: 1.470E-05 | global batch size:   192 | lm loss: 6.323773E+00 | grad norm: 9.797 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.740 | TFLOPs: 91.70 |
[default7]: iteration       99/  250000 | consumed samples:        19008 | consumed tokens:     38928384 | elapsed time per iteration (ms): 5684.6 | learning rate: 1.485E-05 | global batch size:   192 | lm loss: 6.189894E+00 | grad norm: 5.381 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.775 | TFLOPs: 91.79 |
[default7]: iteration      100/  250000 | consumed samples:        19200 | consumed tokens:     39321600 | elapsed time per iteration (ms): 5686.0 | learning rate: 1.500E-05 | global batch size:   192 | lm loss: 6.157090E+00 | grad norm: 4.566 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.767 | TFLOPs: 91.77 |
[default7]:-----------------------------------------------------------------------------------------------
[default7]: validation loss at iteration 100 | lm loss value: 6.292620E+00 | lm loss PPL: 5.405676E+02 | 
[default7]:-----------------------------------------------------------------------------------------------
[default7]: iteration      101/  250000 | consumed samples:        19392 | consumed tokens:     39714816 | elapsed time per iteration (ms): 78553.5 | learning rate: 1.515E-05 | global batch size:   192 | lm loss: 6.267640E+00 | grad norm: 11.118 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.444 | TFLOPs: 6.64 |
[default7]: iteration      102/  250000 | consumed samples:        19584 | consumed tokens:     40108032 | elapsed time per iteration (ms): 5680.1 | learning rate: 1.530E-05 | global batch size:   192 | lm loss: 6.214202E+00 | grad norm: 8.578 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.802 | TFLOPs: 91.87 |
[default7]: iteration      103/  250000 | consumed samples:        19776 | consumed tokens:     40501248 | elapsed time per iteration (ms): 5683.1 | learning rate: 1.545E-05 | global batch size:   192 | lm loss: 6.167019E+00 | grad norm: 5.155 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.785 | TFLOPs: 91.82 |
[default7]: iteration      104/  250000 | consumed samples:        19968 | consumed tokens:     40894464 | elapsed time per iteration (ms): 5687.6 | learning rate: 1.560E-05 | global batch size:   192 | lm loss: 6.213768E+00 | grad norm: 6.660 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.757 | TFLOPs: 91.75 |
[default7]: iteration      105/  250000 | consumed samples:        20160 | consumed tokens:     41287680 | elapsed time per iteration (ms): 5680.0 | learning rate: 1.575E-05 | global batch size:   192 | lm loss: 6.267772E+00 | grad norm: 6.962 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.803 | TFLOPs: 91.87 |
[default7]: iteration      106/  250000 | consumed samples:        20352 | consumed tokens:     41680896 | elapsed time per iteration (ms): 5679.6 | learning rate: 1.590E-05 | global batch size:   192 | lm loss: 6.231410E+00 | grad norm: 5.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.805 | TFLOPs: 91.88 |
[default7]: iteration      107/  250000 | consumed samples:        20544 | consumed tokens:     42074112 | elapsed time per iteration (ms): 5679.0 | learning rate: 1.605E-05 | global batch size:   192 | lm loss: 6.292335E+00 | grad norm: 5.582 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.809 | TFLOPs: 91.89 |
[default7]: iteration      108/  250000 | consumed samples:        20736 | consumed tokens:     42467328 | elapsed time per iteration (ms): 5681.1 | learning rate: 1.620E-05 | global batch size:   192 | lm loss: 6.135859E+00 | grad norm: 4.602 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.796 | TFLOPs: 91.85 |
[default7]: iteration      109/  250000 | consumed samples:        20928 | consumed tokens:     42860544 | elapsed time per iteration (ms): 5686.2 | learning rate: 1.635E-05 | global batch size:   192 | lm loss: 6.157677E+00 | grad norm: 4.584 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.766 | TFLOPs: 91.77 |
[default7]: iteration      110/  250000 | consumed samples:        21120 | consumed tokens:     43253760 | elapsed time per iteration (ms): 5682.1 | learning rate: 1.650E-05 | global batch size:   192 | lm loss: 6.222160E+00 | grad norm: 5.362 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.790 | TFLOPs: 91.84 |
[default7]: iteration      111/  250000 | consumed samples:        21312 | consumed tokens:     43646976 | elapsed time per iteration (ms): 5685.7 | learning rate: 1.665E-05 | global batch size:   192 | lm loss: 6.141335E+00 | grad norm: 3.768 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.769 | TFLOPs: 91.78 |
[default0]:[2024-03-28 23:47:50,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=82, skipped=0, lr=[1.2299999999999999e-05, 1.2299999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 82 loss: 6.3598 iter time (s): 5.685 samples/sec: 33.774
[default0]:[2024-03-28 23:47:55,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=83, skipped=0, lr=[1.2449999999999998e-05, 1.2449999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 83 loss: 6.4070 iter time (s): 5.685 samples/sec: 33.775
[default0]:[2024-03-28 23:48:01,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=84, skipped=0, lr=[1.2599999999999998e-05, 1.2599999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 84 loss: 6.4108 iter time (s): 5.685 samples/sec: 33.773
[default0]:[2024-03-28 23:48:07,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=85, skipped=0, lr=[1.275e-05, 1.275e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 85 loss: 6.3835 iter time (s): 5.692 samples/sec: 33.733
[default0]:[2024-03-28 23:48:12,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=86, skipped=0, lr=[1.29e-05, 1.29e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 86 loss: 6.3365 iter time (s): 5.683 samples/sec: 33.784
[default0]:[2024-03-28 23:48:18,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=87, skipped=0, lr=[1.3049999999999999e-05, 1.3049999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 87 loss: 6.3715 iter time (s): 5.679 samples/sec: 33.811
[default0]:[2024-03-28 23:48:24,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=88, skipped=0, lr=[1.3199999999999999e-05, 1.3199999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 88 loss: 6.3799 iter time (s): 5.686 samples/sec: 33.766
[default0]:[2024-03-28 23:48:29,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=89, skipped=0, lr=[1.335e-05, 1.335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 89 loss: 6.3546 iter time (s): 5.690 samples/sec: 33.743
[default0]:[2024-03-28 23:48:35,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.3499999999999998e-05, 1.3499999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 90 loss: 6.2386 iter time (s): 5.688 samples/sec: 33.755
[default0]:[2024-03-28 23:48:41,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=91, skipped=0, lr=[1.3649999999999998e-05, 1.3649999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 91 loss: 6.4025 iter time (s): 5.687 samples/sec: 33.762
[default0]:[2024-03-28 23:48:47,032] [INFO] [logging.py:96:log_dist] [Rank 0] step=92, skipped=0, lr=[1.38e-05, 1.38e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 92 loss: 6.2488 iter time (s): 5.683 samples/sec: 33.784
[default0]:[2024-03-28 23:48:52,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=93, skipped=0, lr=[1.395e-05, 1.395e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 93 loss: 6.2625 iter time (s): 5.681 samples/sec: 33.794
[default0]:[2024-03-28 23:48:58,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=94, skipped=0, lr=[1.4099999999999999e-05, 1.4099999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 94 loss: 6.3333 iter time (s): 5.675 samples/sec: 33.831
[default0]:[2024-03-28 23:49:04,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=95, skipped=0, lr=[1.4249999999999999e-05, 1.4249999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 95 loss: 6.3533 iter time (s): 5.678 samples/sec: 33.816
[default0]:[2024-03-28 23:49:09,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=96, skipped=0, lr=[1.4399999999999998e-05, 1.4399999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 96 loss: 6.2427 iter time (s): 5.677 samples/sec: 33.819
[default0]:[2024-03-28 23:49:15,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=97, skipped=0, lr=[1.4549999999999998e-05, 1.4549999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 97 loss: 6.1485 iter time (s): 5.677 samples/sec: 33.822
[default0]:[2024-03-28 23:49:21,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=98, skipped=0, lr=[1.4699999999999998e-05, 1.4699999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 98 loss: 6.3238 iter time (s): 5.686 samples/sec: 33.769
[default0]:[2024-03-28 23:49:26,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=99, skipped=0, lr=[1.485e-05, 1.485e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 99 loss: 6.1899 iter time (s): 5.680 samples/sec: 33.805
[default0]:[2024-03-28 23:49:32,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1.4999999999999999e-05, 1.4999999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 100 loss: 6.1571 iter time (s): 5.681 samples/sec: 33.796
[default0]:[2024-03-28 23:50:51,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=101, skipped=0, lr=[1.5149999999999999e-05, 1.5149999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 101 loss: 6.2676 iter time (s): 5.680 samples/sec: 33.802
[default0]:[2024-03-28 23:50:56,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=102, skipped=0, lr=[1.53e-05, 1.53e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 102 loss: 6.2142 iter time (s): 5.676 samples/sec: 33.830
[default0]:[2024-03-28 23:51:02,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=103, skipped=0, lr=[1.545e-05, 1.545e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 103 loss: 6.1670 iter time (s): 5.678 samples/sec: 33.813
[default0]:[2024-03-28 23:51:08,120] [INFO] [logging.py:96:log_dist] [Rank 0] step=104, skipped=0, lr=[1.56e-05, 1.56e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 104 loss: 6.2138 iter time (s): 5.683 samples/sec: 33.785
[default0]:[2024-03-28 23:51:13,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=105, skipped=0, lr=[1.5749999999999997e-05, 1.5749999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 105 loss: 6.2678 iter time (s): 5.675 samples/sec: 33.835
[default0]:[2024-03-28 23:51:19,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=106, skipped=0, lr=[1.5899999999999997e-05, 1.5899999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 106 loss: 6.2314 iter time (s): 5.675 samples/sec: 33.833
[default0]:[2024-03-28 23:51:25,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=107, skipped=0, lr=[1.605e-05, 1.605e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 107 loss: 6.2923 iter time (s): 5.674 samples/sec: 33.838
[default0]:[2024-03-28 23:51:30,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=108, skipped=0, lr=[1.62e-05, 1.62e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 108 loss: 6.1359 iter time (s): 5.676 samples/sec: 33.825
[default0]:[2024-03-28 23:51:36,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=109, skipped=0, lr=[1.6349999999999998e-05, 1.6349999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 109 loss: 6.1577 iter time (s): 5.681 samples/sec: 33.795
[default0]:[2024-03-28 23:51:42,208] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1.6499999999999998e-05, 1.6499999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 110 loss: 6.2222 iter time (s): 5.677 samples/sec: 33.820
[default0]:[2024-03-28 23:51:47,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=111, skipped=0, lr=[1.6649999999999998e-05, 1.6649999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 111 loss: 6.1413 iter time (s): 5.681 samples/sec: 33.798
[default0]:[2024-03-28 23:51:53,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=112, skipped=0, lr=[1.68e-05, 1.68e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 112 loss: 6.0868 iter time (s): 5.683 samples/sec: 33.784
[default0]:[2024-03-28 23:51:59,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=113, skipped=0, lr=[1.695e-05, 1.695e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 113 loss: 6.1149 iter time (s): 5.684 samples/sec: 33.778
[default0]:[2024-03-28 23:52:04,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=114, skipped=0, lr=[1.71e-05, 1.71e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 114 loss: 5.8896 iter time (s): 5.682 samples/sec: 33.788
[default7]: iteration      112/  250000 | consumed samples:        21504 | consumed tokens:     44040192 | elapsed time per iteration (ms): 5687.8 | learning rate: 1.680E-05 | global batch size:   192 | lm loss: 6.086771E+00 | grad norm: 3.780 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.756 | TFLOPs: 91.74 |
[default7]: iteration      113/  250000 | consumed samples:        21696 | consumed tokens:     44433408 | elapsed time per iteration (ms): 5689.0 | learning rate: 1.695E-05 | global batch size:   192 | lm loss: 6.114865E+00 | grad norm: 3.781 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.750 | TFLOPs: 91.73 |
[default7]: iteration      114/  250000 | consumed samples:        21888 | consumed tokens:     44826624 | elapsed time per iteration (ms): 5687.5 | learning rate: 1.710E-05 | global batch size:   192 | lm loss: 5.889626E+00 | grad norm: 3.359 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.758 | TFLOPs: 91.75 |
[default7]: iteration      115/  250000 | consumed samples:        22080 | consumed tokens:     45219840 | elapsed time per iteration (ms): 5694.2 | learning rate: 1.725E-05 | global batch size:   192 | lm loss: 6.010123E+00 | grad norm: 4.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.718 | TFLOPs: 91.64 |
[default7]: iteration      116/  250000 | consumed samples:        22272 | consumed tokens:     45613056 | elapsed time per iteration (ms): 5688.0 | learning rate: 1.740E-05 | global batch size:   192 | lm loss: 5.964281E+00 | grad norm: 3.555 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.755 | TFLOPs: 91.74 |
[default7]: iteration      117/  250000 | consumed samples:        22464 | consumed tokens:     46006272 | elapsed time per iteration (ms): 5688.9 | learning rate: 1.755E-05 | global batch size:   192 | lm loss: 5.925764E+00 | grad norm: 3.793 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.750 | TFLOPs: 91.73 |
[default7]: iteration      118/  250000 | consumed samples:        22656 | consumed tokens:     46399488 | elapsed time per iteration (ms): 5686.4 | learning rate: 1.770E-05 | global batch size:   192 | lm loss: 5.879265E+00 | grad norm: 3.742 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.765 | TFLOPs: 91.77 |
[default7]: iteration      119/  250000 | consumed samples:        22848 | consumed tokens:     46792704 | elapsed time per iteration (ms): 5684.2 | learning rate: 1.785E-05 | global batch size:   192 | lm loss: 5.987779E+00 | grad norm: 5.781 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.778 | TFLOPs: 91.80 |
[default7]: iteration      120/  250000 | consumed samples:        23040 | consumed tokens:     47185920 | elapsed time per iteration (ms): 5681.6 | learning rate: 1.800E-05 | global batch size:   192 | lm loss: 6.035601E+00 | grad norm: 5.945 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.793 | TFLOPs: 91.84 |
[default7]: iteration      121/  250000 | consumed samples:        23232 | consumed tokens:     47579136 | elapsed time per iteration (ms): 5682.8 | learning rate: 1.815E-05 | global batch size:   192 | lm loss: 6.113987E+00 | grad norm: 4.468 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.786 | TFLOPs: 91.82 |
[default7]: iteration      122/  250000 | consumed samples:        23424 | consumed tokens:     47972352 | elapsed time per iteration (ms): 5684.1 | learning rate: 1.830E-05 | global batch size:   192 | lm loss: 5.856053E+00 | grad norm: 4.356 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.778 | TFLOPs: 91.80 |
[default7]: iteration      123/  250000 | consumed samples:        23616 | consumed tokens:     48365568 | elapsed time per iteration (ms): 5689.5 | learning rate: 1.845E-05 | global batch size:   192 | lm loss: 5.997756E+00 | grad norm: 5.016 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.746 | TFLOPs: 91.72 |
[default7]: iteration      124/  250000 | consumed samples:        23808 | consumed tokens:     48758784 | elapsed time per iteration (ms): 5683.2 | learning rate: 1.860E-05 | global batch size:   192 | lm loss: 5.924189E+00 | grad norm: 6.460 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.784 | TFLOPs: 91.82 |
[default7]: iteration      125/  250000 | consumed samples:        24000 | consumed tokens:     49152000 | elapsed time per iteration (ms): 5685.4 | learning rate: 1.875E-05 | global batch size:   192 | lm loss: 5.903030E+00 | grad norm: 6.925 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.771 | TFLOPs: 91.78 |
[default7]: iteration      126/  250000 | consumed samples:        24192 | consumed tokens:     49545216 | elapsed time per iteration (ms): 5684.4 | learning rate: 1.890E-05 | global batch size:   192 | lm loss: 5.724092E+00 | grad norm: 6.866 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.777 | TFLOPs: 91.80 |
[default7]: iteration      127/  250000 | consumed samples:        24384 | consumed tokens:     49938432 | elapsed time per iteration (ms): 5688.3 | learning rate: 1.905E-05 | global batch size:   192 | lm loss: 5.973657E+00 | grad norm: 6.618 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.753 | TFLOPs: 91.74 |
[default7]: iteration      128/  250000 | consumed samples:        24576 | consumed tokens:     50331648 | elapsed time per iteration (ms): 5686.9 | learning rate: 1.920E-05 | global batch size:   192 | lm loss: 6.019960E+00 | grad norm: 6.811 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.762 | TFLOPs: 91.76 |
[default7]: iteration      129/  250000 | consumed samples:        24768 | consumed tokens:     50724864 | elapsed time per iteration (ms): 5693.7 | learning rate: 1.935E-05 | global batch size:   192 | lm loss: 5.901422E+00 | grad norm: 8.086 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.722 | TFLOPs: 91.65 |
[default7]: iteration      130/  250000 | consumed samples:        24960 | consumed tokens:     51118080 | elapsed time per iteration (ms): 5698.4 | learning rate: 1.950E-05 | global batch size:   192 | lm loss: 5.872014E+00 | grad norm: 5.141 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.694 | TFLOPs: 91.57 |
[default7]: iteration      131/  250000 | consumed samples:        25152 | consumed tokens:     51511296 | elapsed time per iteration (ms): 5695.6 | learning rate: 1.965E-05 | global batch size:   192 | lm loss: 5.870321E+00 | grad norm: 4.471 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.710 | TFLOPs: 91.62 |
[default0]:[2024-03-28 23:52:10,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=115, skipped=0, lr=[1.725e-05, 1.725e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 115 loss: 6.0101 iter time (s): 5.689 samples/sec: 33.747
[default0]:[2024-03-28 23:52:16,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=116, skipped=0, lr=[1.74e-05, 1.74e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 116 loss: 5.9643 iter time (s): 5.683 samples/sec: 33.784
[default0]:[2024-03-28 23:52:22,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=117, skipped=0, lr=[1.755e-05, 1.755e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 117 loss: 5.9258 iter time (s): 5.684 samples/sec: 33.779
[default0]:[2024-03-28 23:52:27,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=118, skipped=0, lr=[1.7699999999999997e-05, 1.7699999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 118 loss: 5.8793 iter time (s): 5.682 samples/sec: 33.793
[default0]:[2024-03-28 23:52:33,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=119, skipped=0, lr=[1.7849999999999997e-05, 1.7849999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 119 loss: 5.9878 iter time (s): 5.679 samples/sec: 33.806
[default0]:[2024-03-28 23:52:39,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[1.7999999999999997e-05, 1.7999999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 120 loss: 6.0356 iter time (s): 5.677 samples/sec: 33.822
[default0]:[2024-03-28 23:52:44,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=121, skipped=0, lr=[1.815e-05, 1.815e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 121 loss: 6.1140 iter time (s): 5.678 samples/sec: 33.814
[default0]:[2024-03-28 23:52:50,451] [INFO] [logging.py:96:log_dist] [Rank 0] step=122, skipped=0, lr=[1.8299999999999998e-05, 1.8299999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 122 loss: 5.8561 iter time (s): 5.679 samples/sec: 33.808
[default0]:[2024-03-28 23:52:56,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=123, skipped=0, lr=[1.8449999999999998e-05, 1.8449999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 123 loss: 5.9978 iter time (s): 5.685 samples/sec: 33.774
[default0]:[2024-03-28 23:53:01,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=124, skipped=0, lr=[1.8599999999999998e-05, 1.8599999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 124 loss: 5.9242 iter time (s): 5.678 samples/sec: 33.813
[default0]:[2024-03-28 23:53:07,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=125, skipped=0, lr=[1.875e-05, 1.875e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 125 loss: 5.9030 iter time (s): 5.681 samples/sec: 33.800
[default0]:[2024-03-28 23:53:13,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=126, skipped=0, lr=[1.89e-05, 1.89e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 126 loss: 5.7241 iter time (s): 5.679 samples/sec: 33.806
[default0]:[2024-03-28 23:53:18,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=127, skipped=0, lr=[1.905e-05, 1.905e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 127 loss: 5.9737 iter time (s): 5.684 samples/sec: 33.781
[default0]:[2024-03-28 23:53:24,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=128, skipped=0, lr=[1.92e-05, 1.92e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 128 loss: 6.0200 iter time (s): 5.682 samples/sec: 33.790
[default0]:[2024-03-28 23:53:30,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=129, skipped=0, lr=[1.935e-05, 1.935e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 129 loss: 5.9014 iter time (s): 5.688 samples/sec: 33.755
[default0]:[2024-03-28 23:53:35,960] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[1.95e-05, 1.95e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 130 loss: 5.8720 iter time (s): 5.692 samples/sec: 33.733
[default0]:[2024-03-28 23:53:41,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=131, skipped=0, lr=[1.965e-05, 1.965e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 131 loss: 5.8703 iter time (s): 5.691 samples/sec: 33.737
[default0]:[2024-03-28 23:53:47,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=132, skipped=0, lr=[1.9799999999999997e-05, 1.9799999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 132 loss: 6.0026 iter time (s): 5.697 samples/sec: 33.702
[default0]:[2024-03-28 23:53:53,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=133, skipped=0, lr=[1.9949999999999997e-05, 1.9949999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 133 loss: 5.8720 iter time (s): 5.696 samples/sec: 33.709
[default0]:[2024-03-28 23:53:58,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=134, skipped=0, lr=[2.0099999999999997e-05, 2.0099999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 134 loss: 5.9035 iter time (s): 5.686 samples/sec: 33.766
[default0]:[2024-03-28 23:54:04,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=135, skipped=0, lr=[2.0249999999999998e-05, 2.0249999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 135 loss: 5.8737 iter time (s): 5.698 samples/sec: 33.696
[default0]:[2024-03-28 23:54:10,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=136, skipped=0, lr=[2.0399999999999998e-05, 2.0399999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 136 loss: 5.8536 iter time (s): 5.696 samples/sec: 33.708
[default0]:[2024-03-28 23:54:15,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=137, skipped=0, lr=[2.0549999999999998e-05, 2.0549999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 137 loss: 5.8234 iter time (s): 5.693 samples/sec: 33.724
[default0]:[2024-03-28 23:54:21,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=138, skipped=0, lr=[2.07e-05, 2.07e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 138 loss: 5.7771 iter time (s): 5.695 samples/sec: 33.716
[default0]:[2024-03-28 23:54:27,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=139, skipped=0, lr=[2.085e-05, 2.085e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 139 loss: 5.8353 iter time (s): 5.756 samples/sec: 33.356
[default0]:[2024-03-28 23:54:33,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[2.1e-05, 2.1e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 140 loss: 5.7675 iter time (s): 5.681 samples/sec: 33.796
[default0]:[2024-03-28 23:54:38,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=141, skipped=0, lr=[2.115e-05, 2.115e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 141 loss: 5.7652 iter time (s): 5.690 samples/sec: 33.741
[default0]:[2024-03-28 23:54:44,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=142, skipped=0, lr=[2.13e-05, 2.13e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 142 loss: 5.7254 iter time (s): 5.889 samples/sec: 32.603
[default0]:[2024-03-28 23:54:50,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=143, skipped=0, lr=[2.1449999999999996e-05, 2.1449999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 143 loss: 5.7087 iter time (s): 5.698 samples/sec: 33.698
[default0]:[2024-03-28 23:54:55,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=144, skipped=0, lr=[2.16e-05, 2.16e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 144 loss: 5.6341 iter time (s): 5.691 samples/sec: 33.738
[default0]:[2024-03-28 23:55:01,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=145, skipped=0, lr=[2.1749999999999997e-05, 2.1749999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 145 loss: 5.7765 iter time (s): 5.699 samples/sec: 33.687
[default0]:[2024-03-28 23:55:07,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=146, skipped=0, lr=[2.1899999999999997e-05, 2.1899999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 146 loss: 5.7289 iter time (s): 5.690 samples/sec: 33.745
[default0]:[2024-03-28 23:55:13,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=147, skipped=0, lr=[2.205e-05, 2.205e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 147 loss: 5.4448 iter time (s): 5.691 samples/sec: 33.735
[default7]: iteration      132/  250000 | consumed samples:        25344 | consumed tokens:     51904512 | elapsed time per iteration (ms): 5701.5 | learning rate: 1.980E-05 | global batch size:   192 | lm loss: 6.002617E+00 | grad norm: 8.067 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.675 | TFLOPs: 91.52 |
[default7]: iteration      133/  250000 | consumed samples:        25536 | consumed tokens:     52297728 | elapsed time per iteration (ms): 5700.4 | learning rate: 1.995E-05 | global batch size:   192 | lm loss: 5.871951E+00 | grad norm: 6.603 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.682 | TFLOPs: 91.54 |
[default7]: iteration      134/  250000 | consumed samples:        25728 | consumed tokens:     52690944 | elapsed time per iteration (ms): 5690.9 | learning rate: 2.010E-05 | global batch size:   192 | lm loss: 5.903485E+00 | grad norm: 4.867 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.738 | TFLOPs: 91.69 |
[default7]: iteration      135/  250000 | consumed samples:        25920 | consumed tokens:     53084160 | elapsed time per iteration (ms): 5704.6 | learning rate: 2.025E-05 | global batch size:   192 | lm loss: 5.873748E+00 | grad norm: 5.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.657 | TFLOPs: 91.47 |
[default7]: iteration      136/  250000 | consumed samples:        26112 | consumed tokens:     53477376 | elapsed time per iteration (ms): 5700.6 | learning rate: 2.040E-05 | global batch size:   192 | lm loss: 5.853569E+00 | grad norm: 4.825 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.680 | TFLOPs: 91.54 |
[default7]: iteration      137/  250000 | consumed samples:        26304 | consumed tokens:     53870592 | elapsed time per iteration (ms): 5697.9 | learning rate: 2.055E-05 | global batch size:   192 | lm loss: 5.823386E+00 | grad norm: 5.148 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.696 | TFLOPs: 91.58 |
[default7]: iteration      138/  250000 | consumed samples:        26496 | consumed tokens:     54263808 | elapsed time per iteration (ms): 5699.3 | learning rate: 2.070E-05 | global batch size:   192 | lm loss: 5.777148E+00 | grad norm: 4.723 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.689 | TFLOPs: 91.56 |
[default7]: iteration      139/  250000 | consumed samples:        26688 | consumed tokens:     54657024 | elapsed time per iteration (ms): 5760.8 | learning rate: 2.085E-05 | global batch size:   192 | lm loss: 5.835307E+00 | grad norm: 3.863 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.329 | TFLOPs: 90.58 |
[default7]: iteration      140/  250000 | consumed samples:        26880 | consumed tokens:     55050240 | elapsed time per iteration (ms): 5686.0 | learning rate: 2.100E-05 | global batch size:   192 | lm loss: 5.767503E+00 | grad norm: 3.315 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.767 | TFLOPs: 91.77 |
[default7]: iteration      141/  250000 | consumed samples:        27072 | consumed tokens:     55443456 | elapsed time per iteration (ms): 5695.3 | learning rate: 2.115E-05 | global batch size:   192 | lm loss: 5.765229E+00 | grad norm: 3.727 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.712 | TFLOPs: 91.62 |
[default7]: iteration      142/  250000 | consumed samples:        27264 | consumed tokens:     55836672 | elapsed time per iteration (ms): 5894.1 | learning rate: 2.130E-05 | global batch size:   192 | lm loss: 5.725355E+00 | grad norm: 4.544 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.575 | TFLOPs: 88.53 |
[default7]: iteration      143/  250000 | consumed samples:        27456 | consumed tokens:     56229888 | elapsed time per iteration (ms): 5702.6 | learning rate: 2.145E-05 | global batch size:   192 | lm loss: 5.708687E+00 | grad norm: 5.010 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.669 | TFLOPs: 91.51 |
[default7]: iteration      144/  250000 | consumed samples:        27648 | consumed tokens:     56623104 | elapsed time per iteration (ms): 5695.7 | learning rate: 2.160E-05 | global batch size:   192 | lm loss: 5.634146E+00 | grad norm: 4.154 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.710 | TFLOPs: 91.62 |
[default7]: iteration      145/  250000 | consumed samples:        27840 | consumed tokens:     57016320 | elapsed time per iteration (ms): 5704.5 | learning rate: 2.175E-05 | global batch size:   192 | lm loss: 5.776478E+00 | grad norm: 3.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.658 | TFLOPs: 91.48 |
[default7]: iteration      146/  250000 | consumed samples:        28032 | consumed tokens:     57409536 | elapsed time per iteration (ms): 5699.3 | learning rate: 2.190E-05 | global batch size:   192 | lm loss: 5.728938E+00 | grad norm: 3.341 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.688 | TFLOPs: 91.56 |
[default7]: iteration      147/  250000 | consumed samples:        28224 | consumed tokens:     57802752 | elapsed time per iteration (ms): 5696.2 | learning rate: 2.205E-05 | global batch size:   192 | lm loss: 5.444786E+00 | grad norm: 4.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.707 | TFLOPs: 91.61 |
[default7]: iteration      148/  250000 | consumed samples:        28416 | consumed tokens:     58195968 | elapsed time per iteration (ms): 5687.8 | learning rate: 2.220E-05 | global batch size:   192 | lm loss: 5.635953E+00 | grad norm: 3.677 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.757 | TFLOPs: 91.74 |
[default7]: iteration      149/  250000 | consumed samples:        28608 | consumed tokens:     58589184 | elapsed time per iteration (ms): 5694.2 | learning rate: 2.235E-05 | global batch size:   192 | lm loss: 5.533233E+00 | grad norm: 4.433 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.718 | TFLOPs: 91.64 |
[default7]: iteration      150/  250000 | consumed samples:        28800 | consumed tokens:     58982400 | elapsed time per iteration (ms): 5697.2 | learning rate: 2.250E-05 | global batch size:   192 | lm loss: 5.541862E+00 | grad norm: 4.751 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.700 | TFLOPs: 91.59 |
[default7]:-----------------------------------------------------------------------------------------------
[default7]: validation loss at iteration 150 | lm loss value: 5.649342E+00 | lm loss PPL: 2.841045E+02 | 
[default7]:-----------------------------------------------------------------------------------------------
[default7]: iteration      151/  250000 | consumed samples:        28992 | consumed tokens:     59375616 | elapsed time per iteration (ms): 78777.2 | learning rate: 2.265E-05 | global batch size:   192 | lm loss: 5.564361E+00 | grad norm: 4.146 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.437 | TFLOPs: 6.62 |
[default7]: iteration      152/  250000 | consumed samples:        29184 | consumed tokens:     59768832 | elapsed time per iteration (ms): 5691.1 | learning rate: 2.280E-05 | global batch size:   192 | lm loss: 5.641956E+00 | grad norm: 3.838 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.737 | TFLOPs: 91.69 |
[default7]: iteration      153/  250000 | consumed samples:        29376 | consumed tokens:     60162048 | elapsed time per iteration (ms): 5683.0 | learning rate: 2.295E-05 | global batch size:   192 | lm loss: 5.538945E+00 | grad norm: 3.659 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.785 | TFLOPs: 91.82 |
[default7]: iteration      154/  250000 | consumed samples:        29568 | consumed tokens:     60555264 | elapsed time per iteration (ms): 5787.4 | learning rate: 2.310E-05 | global batch size:   192 | lm loss: 5.570643E+00 | grad norm: 4.681 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.175 | TFLOPs: 90.16 |
[default7]: iteration      155/  250000 | consumed samples:        29760 | consumed tokens:     60948480 | elapsed time per iteration (ms): 5694.3 | learning rate: 2.325E-05 | global batch size:   192 | lm loss: 5.521452E+00 | grad norm: 3.598 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.718 | TFLOPs: 91.64 |
[default7]: iteration      156/  250000 | consumed samples:        29952 | consumed tokens:     61341696 | elapsed time per iteration (ms): 5699.2 | learning rate: 2.340E-05 | global batch size:   192 | lm loss: 5.455865E+00 | grad norm: 2.889 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.689 | TFLOPs: 91.56 |
[default7]: iteration      157/  250000 | consumed samples:        30144 | consumed tokens:     61734912 | elapsed time per iteration (ms): 5696.5 | learning rate: 2.355E-05 | global batch size:   192 | lm loss: 5.407000E+00 | grad norm: 3.648 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.705 | TFLOPs: 91.60 |
[default7]: iteration      158/  250000 | consumed samples:        30336 | consumed tokens:     62128128 | elapsed time per iteration (ms): 5701.1 | learning rate: 2.370E-05 | global batch size:   192 | lm loss: 5.459304E+00 | grad norm: 3.127 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.678 | TFLOPs: 91.53 |
[default7]: iteration      159/  250000 | consumed samples:        30528 | consumed tokens:     62521344 | elapsed time per iteration (ms): 5705.2 | learning rate: 2.385E-05 | global batch size:   192 | lm loss: 5.533158E+00 | grad norm: 3.014 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.654 | TFLOPs: 91.46 |
[default7]: iteration      160/  250000 | consumed samples:        30720 | consumed tokens:     62914560 | elapsed time per iteration (ms): 5696.8 | learning rate: 2.400E-05 | global batch size:   192 | lm loss: 5.404231E+00 | grad norm: 3.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.703 | TFLOPs: 91.60 |
[default7]: iteration      161/  250000 | consumed samples:        30912 | consumed tokens:     63307776 | elapsed time per iteration (ms): 5691.9 | learning rate: 2.415E-05 | global batch size:   192 | lm loss: 5.522488E+00 | grad norm: 3.438 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.732 | TFLOPs: 91.68 |
[default7]: iteration      162/  250000 | consumed samples:        31104 | consumed tokens:     63700992 | elapsed time per iteration (ms): 5694.5 | learning rate: 2.430E-05 | global batch size:   192 | lm loss: 5.514556E+00 | grad norm: 3.410 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.717 | TFLOPs: 91.64 |
[default7]: iteration      163/  250000 | consumed samples:        31296 | consumed tokens:     64094208 | elapsed time per iteration (ms): 5693.6 | learning rate: 2.445E-05 | global batch size:   192 | lm loss: 5.341336E+00 | grad norm: 3.963 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.722 | TFLOPs: 91.65 |
[default7]: iteration      164/  250000 | consumed samples:        31488 | consumed tokens:     64487424 | elapsed time per iteration (ms): 5693.2 | learning rate: 2.460E-05 | global batch size:   192 | lm loss: 5.517552E+00 | grad norm: 4.731 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.725 | TFLOPs: 91.66 |
[default7]: iteration      165/  250000 | consumed samples:        31680 | consumed tokens:     64880640 | elapsed time per iteration (ms): 5690.3 | learning rate: 2.475E-05 | global batch size:   192 | lm loss: 5.317833E+00 | grad norm: 3.728 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.742 | TFLOPs: 91.70 |
[default7]: iteration      166/  250000 | consumed samples:        31872 | consumed tokens:     65273856 | elapsed time per iteration (ms): 5689.3 | learning rate: 2.490E-05 | global batch size:   192 | lm loss: 5.489504E+00 | grad norm: 5.551 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.748 | TFLOPs: 91.72 |
[default7]: iteration      167/  250000 | consumed samples:        32064 | consumed tokens:     65667072 | elapsed time per iteration (ms): 5687.4 | learning rate: 2.505E-05 | global batch size:   192 | lm loss: 5.437291E+00 | grad norm: 4.979 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.759 | TFLOPs: 91.75 |
[default7]: iteration      168/  250000 | consumed samples:        32256 | consumed tokens:     66060288 | elapsed time per iteration (ms): 5690.4 | learning rate: 2.520E-05 | global batch size:   192 | lm loss: 5.483199E+00 | grad norm: 5.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.741 | TFLOPs: 91.70 |
[default7]: iteration      169/  250000 | consumed samples:        32448 | consumed tokens:     66453504 | elapsed time per iteration (ms): 5687.0 | learning rate: 2.535E-05 | global batch size:   192 | lm loss: 5.429967E+00 | grad norm: 4.362 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.761 | TFLOPs: 91.76 |
[default7]: iteration      170/  250000 | consumed samples:        32640 | consumed tokens:     66846720 | elapsed time per iteration (ms): 5702.8 | learning rate: 2.550E-05 | global batch size:   192 | lm loss: 5.265066E+00 | grad norm: 3.733 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.668 | TFLOPs: 91.50 |
[default0]:[2024-03-28 23:55:18,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=148, skipped=0, lr=[2.2199999999999998e-05, 2.2199999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 148 loss: 5.6360 iter time (s): 5.683 samples/sec: 33.786
[default0]:[2024-03-28 23:55:24,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=149, skipped=0, lr=[2.235e-05, 2.235e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 149 loss: 5.5332 iter time (s): 5.689 samples/sec: 33.748
[default0]:[2024-03-28 23:55:30,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[2.2499999999999998e-05, 2.2499999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 150 loss: 5.5419 iter time (s): 5.692 samples/sec: 33.730
[default0]:[2024-03-28 23:56:48,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=151, skipped=0, lr=[2.2649999999999998e-05, 2.2649999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 151 loss: 5.5644 iter time (s): 5.686 samples/sec: 33.768
[default0]:[2024-03-28 23:56:54,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=152, skipped=0, lr=[2.2799999999999995e-05, 2.2799999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 152 loss: 5.6420 iter time (s): 5.683 samples/sec: 33.783
[default0]:[2024-03-28 23:57:00,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=153, skipped=0, lr=[2.295e-05, 2.295e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 153 loss: 5.5389 iter time (s): 5.678 samples/sec: 33.813
[default0]:[2024-03-28 23:57:06,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=154, skipped=0, lr=[2.31e-05, 2.31e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 154 loss: 5.5706 iter time (s): 5.783 samples/sec: 33.202
[default0]:[2024-03-28 23:57:11,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=155, skipped=0, lr=[2.3249999999999996e-05, 2.3249999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 155 loss: 5.5215 iter time (s): 5.689 samples/sec: 33.747
[default0]:[2024-03-28 23:57:17,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=156, skipped=0, lr=[2.34e-05, 2.34e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 156 loss: 5.4559 iter time (s): 5.694 samples/sec: 33.718
[default0]:[2024-03-28 23:57:23,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=157, skipped=0, lr=[2.3549999999999996e-05, 2.3549999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 157 loss: 5.4070 iter time (s): 5.691 samples/sec: 33.737
[default0]:[2024-03-28 23:57:28,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=158, skipped=0, lr=[2.37e-05, 2.37e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 158 loss: 5.4593 iter time (s): 5.696 samples/sec: 33.707
[default0]:[2024-03-28 23:57:34,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=159, skipped=0, lr=[2.3849999999999997e-05, 2.3849999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 159 loss: 5.5332 iter time (s): 5.700 samples/sec: 33.685
[default0]:[2024-03-28 23:57:40,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[2.3999999999999997e-05, 2.3999999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 160 loss: 5.4042 iter time (s): 5.692 samples/sec: 33.732
[default0]:[2024-03-28 23:57:45,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=161, skipped=0, lr=[2.415e-05, 2.415e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 161 loss: 5.5225 iter time (s): 5.685 samples/sec: 33.770
[default0]:[2024-03-28 23:57:51,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=162, skipped=0, lr=[2.4299999999999998e-05, 2.4299999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 162 loss: 5.5146 iter time (s): 5.687 samples/sec: 33.763
[default0]:[2024-03-28 23:57:57,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=163, skipped=0, lr=[2.4449999999999998e-05, 2.4449999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 163 loss: 5.3413 iter time (s): 5.689 samples/sec: 33.751
[default0]:[2024-03-28 23:58:03,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=164, skipped=0, lr=[2.4599999999999998e-05, 2.4599999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 164 loss: 5.5176 iter time (s): 5.689 samples/sec: 33.752
[default0]:[2024-03-28 23:58:08,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=165, skipped=0, lr=[2.475e-05, 2.475e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 165 loss: 5.3178 iter time (s): 5.686 samples/sec: 33.770
[default0]:[2024-03-28 23:58:14,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=166, skipped=0, lr=[2.4899999999999995e-05, 2.4899999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 166 loss: 5.4895 iter time (s): 5.685 samples/sec: 33.775
[default0]:[2024-03-28 23:58:20,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=167, skipped=0, lr=[2.505e-05, 2.505e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 167 loss: 5.4373 iter time (s): 5.683 samples/sec: 33.788
[default0]:[2024-03-28 23:58:25,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=168, skipped=0, lr=[2.5199999999999996e-05, 2.5199999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 168 loss: 5.4832 iter time (s): 5.682 samples/sec: 33.792
[default0]:[2024-03-28 23:58:31,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=169, skipped=0, lr=[2.5349999999999996e-05, 2.5349999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 169 loss: 5.4300 iter time (s): 5.682 samples/sec: 33.789
[default0]:[2024-03-28 23:58:37,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[2.55e-05, 2.55e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 170 loss: 5.2651 iter time (s): 5.698 samples/sec: 33.697
[default0]:[2024-03-28 23:58:42,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=171, skipped=0, lr=[2.5649999999999997e-05, 2.5649999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 171 loss: 5.3996 iter time (s): 5.684 samples/sec: 33.777
[default0]:[2024-03-28 23:58:48,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=172, skipped=0, lr=[2.58e-05, 2.58e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 172 loss: 5.4910 iter time (s): 5.691 samples/sec: 33.739
[default0]:[2024-03-28 23:58:54,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=173, skipped=0, lr=[2.5949999999999997e-05, 2.5949999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 173 loss: 5.4762 iter time (s): 5.687 samples/sec: 33.763
[default0]:[2024-03-28 23:58:59,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=174, skipped=0, lr=[2.6099999999999997e-05, 2.6099999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 174 loss: 5.3957 iter time (s): 5.685 samples/sec: 33.776
[default0]:[2024-03-28 23:59:05,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=175, skipped=0, lr=[2.6249999999999994e-05, 2.6249999999999994e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 175 loss: 5.4288 iter time (s): 5.705 samples/sec: 33.652
[default0]:[2024-03-28 23:59:11,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=176, skipped=0, lr=[2.6399999999999998e-05, 2.6399999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 176 loss: 5.4581 iter time (s): 5.692 samples/sec: 33.734
[default0]:[2024-03-28 23:59:17,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=177, skipped=0, lr=[2.6549999999999998e-05, 2.6549999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 177 loss: 5.3050 iter time (s): 5.688 samples/sec: 33.754
[default0]:[2024-03-28 23:59:22,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=178, skipped=0, lr=[2.67e-05, 2.67e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 178 loss: 5.3915 iter time (s): 5.692 samples/sec: 33.732
[default0]:[2024-03-28 23:59:28,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=179, skipped=0, lr=[2.685e-05, 2.685e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 179 loss: 5.3845 iter time (s): 5.686 samples/sec: 33.765
[default0]:[2024-03-28 23:59:34,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[2.6999999999999996e-05, 2.6999999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default7]: iteration      171/  250000 | consumed samples:        32832 | consumed tokens:     67239936 | elapsed time per iteration (ms): 5689.7 | learning rate: 2.565E-05 | global batch size:   192 | lm loss: 5.399624E+00 | grad norm: 3.672 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.745 | TFLOPs: 91.71 |
[default7]: iteration      172/  250000 | consumed samples:        33024 | consumed tokens:     67633152 | elapsed time per iteration (ms): 5695.8 | learning rate: 2.580E-05 | global batch size:   192 | lm loss: 5.490979E+00 | grad norm: 9.485 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.709 | TFLOPs: 91.62 |
[default7]: iteration      173/  250000 | consumed samples:        33216 | consumed tokens:     68026368 | elapsed time per iteration (ms): 5691.5 | learning rate: 2.595E-05 | global batch size:   192 | lm loss: 5.476170E+00 | grad norm: 5.776 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.734 | TFLOPs: 91.68 |
[default7]: iteration      174/  250000 | consumed samples:        33408 | consumed tokens:     68419584 | elapsed time per iteration (ms): 5689.6 | learning rate: 2.610E-05 | global batch size:   192 | lm loss: 5.395711E+00 | grad norm: 3.626 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.746 | TFLOPs: 91.71 |
[default7]: iteration      175/  250000 | consumed samples:        33600 | consumed tokens:     68812800 | elapsed time per iteration (ms): 5710.4 | learning rate: 2.625E-05 | global batch size:   192 | lm loss: 5.428818E+00 | grad norm: 4.340 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.623 | TFLOPs: 91.38 |
[default7]: iteration      176/  250000 | consumed samples:        33792 | consumed tokens:     69206016 | elapsed time per iteration (ms): 5696.3 | learning rate: 2.640E-05 | global batch size:   192 | lm loss: 5.458052E+00 | grad norm: 3.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.706 | TFLOPs: 91.61 |
[default7]: iteration      177/  250000 | consumed samples:        33984 | consumed tokens:     69599232 | elapsed time per iteration (ms): 5693.0 | learning rate: 2.655E-05 | global batch size:   192 | lm loss: 5.304991E+00 | grad norm: 4.276 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.726 | TFLOPs: 91.66 |
[default7]: iteration      178/  250000 | consumed samples:        34176 | consumed tokens:     69992448 | elapsed time per iteration (ms): 5696.8 | learning rate: 2.670E-05 | global batch size:   192 | lm loss: 5.391511E+00 | grad norm: 4.405 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.703 | TFLOPs: 91.60 |
[default7]: iteration      179/  250000 | consumed samples:        34368 | consumed tokens:     70385664 | elapsed time per iteration (ms): 5691.8 | learning rate: 2.685E-05 | global batch size:   192 | lm loss: 5.384504E+00 | grad norm: 4.045 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.733 | TFLOPs: 91.68 |
[default7]: iteration      180/  250000 | consumed samples:        34560 | consumed tokens:     70778880 | elapsed time per iteration (ms): 5690.6 | learning rate: 2.700E-05 | global batch size:   192 | lm loss: 5.312858E+00 | grad norm: 3.433 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.740 | TFLOPs: 91.70 |
[default7]: iteration      181/  250000 | consumed samples:        34752 | consumed tokens:     71172096 | elapsed time per iteration (ms): 5694.1 | learning rate: 2.715E-05 | global batch size:   192 | lm loss: 5.297894E+00 | grad norm: 3.341 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.719 | TFLOPs: 91.64 |
[default7]: iteration      182/  250000 | consumed samples:        34944 | consumed tokens:     71565312 | elapsed time per iteration (ms): 5688.8 | learning rate: 2.730E-05 | global batch size:   192 | lm loss: 5.174265E+00 | grad norm: 3.573 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.750 | TFLOPs: 91.73 |
[default7]: iteration      183/  250000 | consumed samples:        35136 | consumed tokens:     71958528 | elapsed time per iteration (ms): 5701.8 | learning rate: 2.745E-05 | global batch size:   192 | lm loss: 5.169555E+00 | grad norm: 4.512 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.673 | TFLOPs: 91.52 |
[default7]: iteration      184/  250000 | consumed samples:        35328 | consumed tokens:     72351744 | elapsed time per iteration (ms): 5693.2 | learning rate: 2.760E-05 | global batch size:   192 | lm loss: 5.146359E+00 | grad norm: 4.629 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.724 | TFLOPs: 91.66 |
[default7]: iteration      185/  250000 | consumed samples:        35520 | consumed tokens:     72744960 | elapsed time per iteration (ms): 5696.2 | learning rate: 2.775E-05 | global batch size:   192 | lm loss: 5.134186E+00 | grad norm: 3.037 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.706 | TFLOPs: 91.61 |
[default7]: iteration      186/  250000 | consumed samples:        35712 | consumed tokens:     73138176 | elapsed time per iteration (ms): 5692.8 | learning rate: 2.790E-05 | global batch size:   192 | lm loss: 5.218320E+00 | grad norm: 4.394 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.727 | TFLOPs: 91.66 |
[default7]: iteration      187/  250000 | consumed samples:        35904 | consumed tokens:     73531392 | elapsed time per iteration (ms): 5691.9 | learning rate: 2.805E-05 | global batch size:   192 | lm loss: 5.151023E+00 | grad norm: 4.399 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.732 | TFLOPs: 91.68 |
[default7]: iteration      188/  250000 | consumed samples:        36096 | consumed tokens:     73924608 | elapsed time per iteration (ms): 5697.1 | learning rate: 2.820E-05 | global batch size:   192 | lm loss: 5.315638E+00 | grad norm: 3.120 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.701 | TFLOPs: 91.59 |
[default7]: iteration      189/  250000 | consumed samples:        36288 | consumed tokens:     74317824 | elapsed time per iteration (ms): 5695.3 | learning rate: 2.835E-05 | global batch size:   192 | lm loss: 5.099562E+00 | grad norm: 2.800 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.712 | TFLOPs: 91.62 |
[default7]: iteration      190/  250000 | consumed samples:        36480 | consumed tokens:     74711040 | elapsed time per iteration (ms): 5696.4 | learning rate: 2.850E-05 | global batch size:   192 | lm loss: 5.146226E+00 | grad norm: 3.063 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.706 | TFLOPs: 91.61 |
[default7]: iteration      191/  250000 | consumed samples:        36672 | consumed tokens:     75104256 | elapsed time per iteration (ms): 5737.6 | learning rate: 2.865E-05 | global batch size:   192 | lm loss: 5.096418E+00 | grad norm: 2.785 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.464 | TFLOPs: 90.95 |
[default7]: iteration      192/  250000 | consumed samples:        36864 | consumed tokens:     75497472 | elapsed time per iteration (ms): 5704.5 | learning rate: 2.880E-05 | global batch size:   192 | lm loss: 5.142348E+00 | grad norm: 2.868 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.658 | TFLOPs: 91.48 |
[default7]: iteration      193/  250000 | consumed samples:        37056 | consumed tokens:     75890688 | elapsed time per iteration (ms): 5696.5 | learning rate: 2.895E-05 | global batch size:   192 | lm loss: 5.144218E+00 | grad norm: 2.815 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.705 | TFLOPs: 91.60 |
[default7]: iteration      194/  250000 | consumed samples:        37248 | consumed tokens:     76283904 | elapsed time per iteration (ms): 5691.5 | learning rate: 2.910E-05 | global batch size:   192 | lm loss: 5.130310E+00 | grad norm: 2.947 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.734 | TFLOPs: 91.68 |
[default7]: iteration      195/  250000 | consumed samples:        37440 | consumed tokens:     76677120 | elapsed time per iteration (ms): 5686.4 | learning rate: 2.925E-05 | global batch size:   192 | lm loss: 4.982610E+00 | grad norm: 2.861 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.765 | TFLOPs: 91.77 |
[default7]: iteration      196/  250000 | consumed samples:        37632 | consumed tokens:     77070336 | elapsed time per iteration (ms): 5693.1 | learning rate: 2.940E-05 | global batch size:   192 | lm loss: 4.987804E+00 | grad norm: 4.130 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.725 | TFLOPs: 91.66 |
[default7]: iteration      197/  250000 | consumed samples:        37824 | consumed tokens:     77463552 | elapsed time per iteration (ms): 5699.8 | learning rate: 2.955E-05 | global batch size:   192 | lm loss: 5.097157E+00 | grad norm: 2.800 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.685 | TFLOPs: 91.55 |
[default7]: iteration      198/  250000 | consumed samples:        38016 | consumed tokens:     77856768 | elapsed time per iteration (ms): 5689.3 | learning rate: 2.970E-05 | global batch size:   192 | lm loss: 4.993086E+00 | grad norm: 3.540 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.748 | TFLOPs: 91.72 |
[default7]: iteration      199/  250000 | consumed samples:        38208 | consumed tokens:     78249984 | elapsed time per iteration (ms): 5689.0 | learning rate: 2.985E-05 | global batch size:   192 | lm loss: 4.993749E+00 | grad norm: 3.549 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.749 | TFLOPs: 91.72 |
[default7]: iteration      200/  250000 | consumed samples:        38400 | consumed tokens:     78643200 | elapsed time per iteration (ms): 5693.9 | learning rate: 3.000E-05 | global batch size:   192 | lm loss: 4.922544E+00 | grad norm: 3.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.720 | TFLOPs: 91.65 |
[default7]:-----------------------------------------------------------------------------------------------
[default7]: validation loss at iteration 200 | lm loss value: 5.109439E+00 | lm loss PPL: 1.655775E+02 | 
[default7]:-----------------------------------------------------------------------------------------------
[default7]: iteration      201/  250000 | consumed samples:        38592 | consumed tokens:     79036416 | elapsed time per iteration (ms): 79028.1 | learning rate: 3.015E-05 | global batch size:   192 | lm loss: 4.937846E+00 | grad norm: 2.934 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.430 | TFLOPs: 6.60 |
[default7]: iteration      202/  250000 | consumed samples:        38784 | consumed tokens:     79429632 | elapsed time per iteration (ms): 5693.2 | learning rate: 3.030E-05 | global batch size:   192 | lm loss: 4.915497E+00 | grad norm: 5.100 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.724 | TFLOPs: 91.66 |
[default7]: iteration      203/  250000 | consumed samples:        38976 | consumed tokens:     79822848 | elapsed time per iteration (ms): 5693.1 | learning rate: 3.045E-05 | global batch size:   192 | lm loss: 4.935485E+00 | grad norm: 3.074 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.725 | TFLOPs: 91.66 |
[default7]: iteration      204/  250000 | consumed samples:        39168 | consumed tokens:     80216064 | elapsed time per iteration (ms): 5689.4 | learning rate: 3.060E-05 | global batch size:   192 | lm loss: 5.005785E+00 | grad norm: 3.420 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.747 | TFLOPs: 91.72 |
[default7]: iteration      205/  250000 | consumed samples:        39360 | consumed tokens:     80609280 | elapsed time per iteration (ms): 5696.6 | learning rate: 3.075E-05 | global batch size:   192 | lm loss: 4.980792E+00 | grad norm: 3.522 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.704 | TFLOPs: 91.60 |
[default7]: iteration      206/  250000 | consumed samples:        39552 | consumed tokens:     81002496 | elapsed time per iteration (ms): 5694.8 | learning rate: 3.090E-05 | global batch size:   192 | lm loss: 4.854958E+00 | grad norm: 3.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.715 | TFLOPs: 91.63 |
[default7]: iteration      207/  250000 | consumed samples:        39744 | consumed tokens:     81395712 | elapsed time per iteration (ms): 5695.8 | learning rate: 3.105E-05 | global batch size:   192 | lm loss: 4.900203E+00 | grad norm: 3.434 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.709 | TFLOPs: 91.61 |
[default7]: iteration      208/  250000 | consumed samples:        39936 | consumed tokens:     81788928 | elapsed time per iteration (ms): 5698.0 | learning rate: 3.120E-05 | global batch size:   192 | lm loss: 4.923148E+00 | grad norm: 3.888 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.696 | TFLOPs: 91.58 |
[default7]: iteration      209/  250000 | consumed samples:        40128 | consumed tokens:     82182144 | elapsed time per iteration (ms): 5693.8 | learning rate: 3.135E-05 | global batch size:   192 | lm loss: 4.879353E+00 | grad norm: 4.652 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.721 | TFLOPs: 91.65 |
[default0]:steps: 180 loss: 5.3129 iter time (s): 5.686 samples/sec: 33.769
[default0]:[2024-03-28 23:59:39,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=181, skipped=0, lr=[2.715e-05, 2.715e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 181 loss: 5.2979 iter time (s): 5.689 samples/sec: 33.748
[default0]:[2024-03-28 23:59:45,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=182, skipped=0, lr=[2.7299999999999996e-05, 2.7299999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 182 loss: 5.1743 iter time (s): 5.684 samples/sec: 33.782
[default0]:[2024-03-28 23:59:51,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=183, skipped=0, lr=[2.7449999999999996e-05, 2.7449999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 183 loss: 5.1696 iter time (s): 5.697 samples/sec: 33.703
[default0]:[2024-03-28 23:59:56,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=184, skipped=0, lr=[2.76e-05, 2.76e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 184 loss: 5.1464 iter time (s): 5.689 samples/sec: 33.752
[default0]:[2024-03-29 00:00:02,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=185, skipped=0, lr=[2.7749999999999997e-05, 2.7749999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 185 loss: 5.1342 iter time (s): 5.691 samples/sec: 33.736
[default0]:[2024-03-29 00:00:08,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=186, skipped=0, lr=[2.79e-05, 2.79e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 186 loss: 5.2183 iter time (s): 5.688 samples/sec: 33.756
[default0]:[2024-03-29 00:00:14,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=187, skipped=0, lr=[2.8049999999999997e-05, 2.8049999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 187 loss: 5.1510 iter time (s): 5.687 samples/sec: 33.760
[default0]:[2024-03-29 00:00:19,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=188, skipped=0, lr=[2.8199999999999998e-05, 2.8199999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 188 loss: 5.3156 iter time (s): 5.692 samples/sec: 33.730
[default0]:[2024-03-29 00:00:25,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=189, skipped=0, lr=[2.8349999999999995e-05, 2.8349999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 189 loss: 5.0996 iter time (s): 5.691 samples/sec: 33.740
[default0]:[2024-03-29 00:00:31,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[2.8499999999999998e-05, 2.8499999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 190 loss: 5.1462 iter time (s): 5.692 samples/sec: 33.734
[default0]:[2024-03-29 00:00:36,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=191, skipped=0, lr=[2.865e-05, 2.865e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 191 loss: 5.0964 iter time (s): 5.733 samples/sec: 33.492
[default0]:[2024-03-29 00:00:42,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=192, skipped=0, lr=[2.8799999999999995e-05, 2.8799999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 192 loss: 5.1423 iter time (s): 5.697 samples/sec: 33.700
[default0]:[2024-03-29 00:00:48,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=193, skipped=0, lr=[2.895e-05, 2.895e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 193 loss: 5.1442 iter time (s): 5.692 samples/sec: 33.734
[default0]:[2024-03-29 00:00:53,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=194, skipped=0, lr=[2.9099999999999996e-05, 2.9099999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 194 loss: 5.1303 iter time (s): 5.687 samples/sec: 33.764
[default0]:[2024-03-29 00:00:59,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=195, skipped=0, lr=[2.925e-05, 2.925e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 195 loss: 4.9826 iter time (s): 5.682 samples/sec: 33.794
[default0]:[2024-03-29 00:01:05,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=196, skipped=0, lr=[2.9399999999999996e-05, 2.9399999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 196 loss: 4.9878 iter time (s): 5.688 samples/sec: 33.754
[default0]:[2024-03-29 00:01:11,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=197, skipped=0, lr=[2.9549999999999997e-05, 2.9549999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 197 loss: 5.0972 iter time (s): 5.695 samples/sec: 33.715
[default0]:[2024-03-29 00:01:16,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=198, skipped=0, lr=[2.97e-05, 2.97e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 198 loss: 4.9931 iter time (s): 5.684 samples/sec: 33.781
[default0]:[2024-03-29 00:01:22,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=199, skipped=0, lr=[2.9849999999999997e-05, 2.9849999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 199 loss: 4.9937 iter time (s): 5.684 samples/sec: 33.780
[default0]:[2024-03-29 00:01:28,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 200 loss: 4.9225 iter time (s): 5.689 samples/sec: 33.752
[default0]:[2024-03-29 00:02:47,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=201, skipped=0, lr=[3.0149999999999998e-05, 3.0149999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 201 loss: 4.9378 iter time (s): 5.685 samples/sec: 33.775
[default0]:[2024-03-29 00:02:52,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=202, skipped=0, lr=[3.0299999999999998e-05, 3.0299999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 202 loss: 4.9155 iter time (s): 5.688 samples/sec: 33.755
[default0]:[2024-03-29 00:02:58,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=203, skipped=0, lr=[3.0449999999999995e-05, 3.0449999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 203 loss: 4.9355 iter time (s): 5.688 samples/sec: 33.755
[default0]:[2024-03-29 00:03:04,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=204, skipped=0, lr=[3.06e-05, 3.06e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 204 loss: 5.0058 iter time (s): 5.684 samples/sec: 33.776
[default0]:[2024-03-29 00:03:09,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=205, skipped=0, lr=[3.075e-05, 3.075e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 205 loss: 4.9808 iter time (s): 5.691 samples/sec: 33.735
[default0]:[2024-03-29 00:03:15,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=206, skipped=0, lr=[3.09e-05, 3.09e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 206 loss: 4.8550 iter time (s): 5.690 samples/sec: 33.744
[default0]:[2024-03-29 00:03:21,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=207, skipped=0, lr=[3.1049999999999996e-05, 3.1049999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 207 loss: 4.9002 iter time (s): 5.691 samples/sec: 33.739
[default0]:[2024-03-29 00:03:26,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=208, skipped=0, lr=[3.12e-05, 3.12e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 208 loss: 4.9231 iter time (s): 5.693 samples/sec: 33.725
[default0]:[2024-03-29 00:03:32,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=209, skipped=0, lr=[3.1349999999999996e-05, 3.1349999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 209 loss: 4.8794 iter time (s): 5.689 samples/sec: 33.751
[default0]:[2024-03-29 00:03:38,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[3.149999999999999e-05, 3.149999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 210 loss: 4.9137 iter time (s): 5.689 samples/sec: 33.748
[default0]:[2024-03-29 00:03:44,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=211, skipped=0, lr=[3.165e-05, 3.165e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 211 loss: 4.7057 iter time (s): 5.697 samples/sec: 33.704
[default0]:[2024-03-29 00:03:49,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=212, skipped=0, lr=[3.1799999999999994e-05, 3.1799999999999994e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 212 loss: 4.9494 iter time (s): 5.687 samples/sec: 33.763
[default7]: iteration      210/  250000 | consumed samples:        40320 | consumed tokens:     82575360 | elapsed time per iteration (ms): 5694.1 | learning rate: 3.150E-05 | global batch size:   192 | lm loss: 4.913681E+00 | grad norm: 3.929 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.719 | TFLOPs: 91.64 |
[default7]: iteration      211/  250000 | consumed samples:        40512 | consumed tokens:     82968576 | elapsed time per iteration (ms): 5701.4 | learning rate: 3.165E-05 | global batch size:   192 | lm loss: 4.705688E+00 | grad norm: 4.749 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.676 | TFLOPs: 91.52 |
[default7]: iteration      212/  250000 | consumed samples:        40704 | consumed tokens:     83361792 | elapsed time per iteration (ms): 5691.6 | learning rate: 3.180E-05 | global batch size:   192 | lm loss: 4.949391E+00 | grad norm: 4.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.734 | TFLOPs: 91.68 |
[default7]: iteration      213/  250000 | consumed samples:        40896 | consumed tokens:     83755008 | elapsed time per iteration (ms): 5695.0 | learning rate: 3.195E-05 | global batch size:   192 | lm loss: 4.904572E+00 | grad norm: 3.540 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.714 | TFLOPs: 91.63 |
[default7]: iteration      214/  250000 | consumed samples:        41088 | consumed tokens:     84148224 | elapsed time per iteration (ms): 5687.6 | learning rate: 3.210E-05 | global batch size:   192 | lm loss: 4.993629E+00 | grad norm: 5.563 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.758 | TFLOPs: 91.75 |
[default7]: iteration      215/  250000 | consumed samples:        41280 | consumed tokens:     84541440 | elapsed time per iteration (ms): 5692.5 | learning rate: 3.225E-05 | global batch size:   192 | lm loss: 4.893494E+00 | grad norm: 4.436 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.729 | TFLOPs: 91.67 |
[default7]: iteration      216/  250000 | consumed samples:        41472 | consumed tokens:     84934656 | elapsed time per iteration (ms): 5691.8 | learning rate: 3.240E-05 | global batch size:   192 | lm loss: 4.826881E+00 | grad norm: 4.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.733 | TFLOPs: 91.68 |
[default7]: iteration      217/  250000 | consumed samples:        41664 | consumed tokens:     85327872 | elapsed time per iteration (ms): 5691.1 | learning rate: 3.255E-05 | global batch size:   192 | lm loss: 4.876530E+00 | grad norm: 3.352 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.737 | TFLOPs: 91.69 |
[default7]: iteration      218/  250000 | consumed samples:        41856 | consumed tokens:     85721088 | elapsed time per iteration (ms): 5697.0 | learning rate: 3.270E-05 | global batch size:   192 | lm loss: 4.719400E+00 | grad norm: 2.581 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.702 | TFLOPs: 91.59 |
[default7]: iteration      219/  250000 | consumed samples:        42048 | consumed tokens:     86114304 | elapsed time per iteration (ms): 5693.2 | learning rate: 3.285E-05 | global batch size:   192 | lm loss: 4.684650E+00 | grad norm: 2.524 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.724 | TFLOPs: 91.66 |
[default7]: iteration      220/  250000 | consumed samples:        42240 | consumed tokens:     86507520 | elapsed time per iteration (ms): 5699.9 | learning rate: 3.300E-05 | global batch size:   192 | lm loss: 4.604788E+00 | grad norm: 2.588 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.685 | TFLOPs: 91.55 |
[default7]: iteration      221/  250000 | consumed samples:        42432 | consumed tokens:     86900736 | elapsed time per iteration (ms): 5699.8 | learning rate: 3.315E-05 | global batch size:   192 | lm loss: 4.676252E+00 | grad norm: 2.432 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.685 | TFLOPs: 91.55 |
[default7]: iteration      222/  250000 | consumed samples:        42624 | consumed tokens:     87293952 | elapsed time per iteration (ms): 5702.6 | learning rate: 3.330E-05 | global batch size:   192 | lm loss: 4.654998E+00 | grad norm: 2.887 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.669 | TFLOPs: 91.51 |
[default7]: iteration      223/  250000 | consumed samples:        42816 | consumed tokens:     87687168 | elapsed time per iteration (ms): 5702.7 | learning rate: 3.345E-05 | global batch size:   192 | lm loss: 4.724995E+00 | grad norm: 2.641 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.668 | TFLOPs: 91.50 |
[default7]: iteration      224/  250000 | consumed samples:        43008 | consumed tokens:     88080384 | elapsed time per iteration (ms): 5700.7 | learning rate: 3.360E-05 | global batch size:   192 | lm loss: 4.640641E+00 | grad norm: 2.310 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.680 | TFLOPs: 91.54 |
[default7]: iteration      225/  250000 | consumed samples:        43200 | consumed tokens:     88473600 | elapsed time per iteration (ms): 5704.4 | learning rate: 3.375E-05 | global batch size:   192 | lm loss: 4.663522E+00 | grad norm: 2.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.659 | TFLOPs: 91.48 |
[default7]: iteration      226/  250000 | consumed samples:        43392 | consumed tokens:     88866816 | elapsed time per iteration (ms): 5706.1 | learning rate: 3.390E-05 | global batch size:   192 | lm loss: 4.506720E+00 | grad norm: 3.537 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.648 | TFLOPs: 91.45 |
[default7]: iteration      227/  250000 | consumed samples:        43584 | consumed tokens:     89260032 | elapsed time per iteration (ms): 5698.0 | learning rate: 3.405E-05 | global batch size:   192 | lm loss: 4.602952E+00 | grad norm: 2.618 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.696 | TFLOPs: 91.58 |
[default7]: iteration      228/  250000 | consumed samples:        43776 | consumed tokens:     89653248 | elapsed time per iteration (ms): 5707.2 | learning rate: 3.420E-05 | global batch size:   192 | lm loss: 4.628720E+00 | grad norm: 2.360 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.642 | TFLOPs: 91.43 |
[default7]: iteration      229/  250000 | consumed samples:        43968 | consumed tokens:     90046464 | elapsed time per iteration (ms): 5703.0 | learning rate: 3.435E-05 | global batch size:   192 | lm loss: 4.641730E+00 | grad norm: 2.414 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.667 | TFLOPs: 91.50 |
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
slurmstepd: error: *** JOB 1299552 ON jean-zay-iam04 CANCELLED AT 2024-03-29T00:07:00 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 1299552.0 ON jean-zay-iam04 CANCELLED AT 2024-03-29T00:07:00 DUE TO TIME LIMIT ***
[2024-03-29 00:07:00,756] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-03-29 00:07:00,755] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-03-29 00:07:00,756] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305984 closing signal SIGTERM
[2024-03-29 00:07:00,756] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531022 closing signal SIGTERM
[2024-03-29 00:07:00,756] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-03-29 00:07:00,756] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311503 closing signal SIGTERM
[2024-03-29 00:07:00,756] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-03-29 00:07:00,756] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343001 closing signal SIGTERM
[2024-03-29 00:07:00,756] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305985 closing signal SIGTERM
[2024-03-29 00:07:00,757] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305986 closing signal SIGTERM
[2024-03-29 00:07:00,757] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311504 closing signal SIGTERM
[2024-03-29 00:07:00,757] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343002 closing signal SIGTERM
[2024-03-29 00:07:00,757] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531023 closing signal SIGTERM
[2024-03-29 00:07:00,757] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305987 closing signal SIGTERM
[2024-03-29 00:07:00,757] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531024 closing signal SIGTERM
[2024-03-29 00:07:00,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343003 closing signal SIGTERM
[2024-03-29 00:07:00,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311505 closing signal SIGTERM
[2024-03-29 00:07:00,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305988 closing signal SIGTERM
[2024-03-29 00:07:00,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343004 closing signal SIGTERM
[2024-03-29 00:07:00,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531025 closing signal SIGTERM
[2024-03-29 00:07:00,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311506 closing signal SIGTERM
[2024-03-29 00:07:00,759] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305989 closing signal SIGTERM
[2024-03-29 00:07:00,758] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343005 closing signal SIGTERM
[2024-03-29 00:07:00,759] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531026 closing signal SIGTERM
[2024-03-29 00:07:00,759] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343006 closing signal SIGTERM
[2024-03-29 00:07:00,759] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311507 closing signal SIGTERM
[2024-03-29 00:07:00,759] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343007 closing signal SIGTERM
[2024-03-29 00:07:00,760] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531027 closing signal SIGTERM
[2024-03-29 00:07:00,760] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305984 closing signal SIGTERM
[2024-03-29 00:07:00,760] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311508 closing signal SIGTERM
[2024-03-29 00:07:00,760] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531028 closing signal SIGTERM
[2024-03-29 00:07:00,760] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531029 closing signal SIGTERM
[2024-03-29 00:07:00,760] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311509 closing signal SIGTERM
[2024-03-29 00:07:00,760] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305985 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343001 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311503 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305986 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343002 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305987 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305988 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305989 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305990 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 305991 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311504 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343003 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311505 closing signal SIGTERM
[2024-03-29 00:07:00,761] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343004 closing signal SIGTERM
[2024-03-29 00:07:00,762] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311506 closing signal SIGTERM
[2024-03-29 00:07:00,762] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343005 closing signal SIGTERM
[2024-03-29 00:07:00,762] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311507 closing signal SIGTERM
[2024-03-29 00:07:00,762] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311508 closing signal SIGTERM
[2024-03-29 00:07:00,762] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343006 closing signal SIGTERM
[2024-03-29 00:07:00,762] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311509 closing signal SIGTERM
[2024-03-29 00:07:00,762] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343007 closing signal SIGTERM
[2024-03-29 00:07:00,762] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 311510 closing signal SIGTERM
[2024-03-29 00:07:00,762] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1343008 closing signal SIGTERM
[2024-03-29 00:07:00,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531022 closing signal SIGTERM
[2024-03-29 00:07:00,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531023 closing signal SIGTERM
[2024-03-29 00:07:00,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531024 closing signal SIGTERM
[2024-03-29 00:07:00,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531025 closing signal SIGTERM
[2024-03-29 00:07:00,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531026 closing signal SIGTERM
[2024-03-29 00:07:00,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531027 closing signal SIGTERM
[2024-03-29 00:07:00,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531028 closing signal SIGTERM
[2024-03-29 00:07:00,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 531029 closing signal SIGTERM
