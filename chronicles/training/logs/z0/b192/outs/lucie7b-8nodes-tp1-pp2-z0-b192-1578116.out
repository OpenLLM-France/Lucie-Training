+ CHECKPOINT_PATH=/gpfswork/rech/qgz/urc37ho/checkpoints12/
+ LOGS_PATH=/gpfswork/rech/qgz/urc37ho/lucie-logs
+ MEGATRON_DEEPSPEED_REPO=/gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya
+ cd /gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya
+ DATASET=/gpfsscratch/rech/qgz/commun/preprocessed_data/Lucie/lucie_tokens_2.4-space_prefix_all/Wikipedia--fr--026_text_document
+ TOKENIZER_PATH=OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all
++ head -n 1
++ scontrol show hostnames 'jean-zay-iam[03,13,24,26,31,33,36,38]'
+ MASTER_ADDR=jean-zay-iam03
+ MASTER_PORT=6000
+ GPUS_PER_NODE=8
+ NNODES=8
+ TP=1
+ PP=2
+ MICRO_BATCH_SIZE=3
+ GLOBAL_BATCH_SIZE=192
+ HIDDEN_SIZE=4096
+ FFN_HIDDEN_SIZE=11008
+ NUM_LAYERS=32
+ NUM_HEADS=32
+ SEQ_LENGTH=2048
+ NUM_KV_HEADS=32
+ TRAIN_STEPS=250000
+ LR=3e-4
+ MIN_LR=3e-5
+ LR_WARMUP_STEPS=2000
+ WEIGHT_DECAY=0.1
+ GRAD_CLIP=1
+ SAVE_INTERVAL=100
+ ZERO_STAGE=0
+ DS_CONFIG=./ds_config.1578116.json
+ activation_checkpoint=false
+ cat
+ ds_args=
+ ds_args=' --deepspeed '
+ ds_args=' --deepspeed_config=./ds_config.1578116.json  --deepspeed '
+ ds_args=' --zero-stage=0  --deepspeed_config=./ds_config.1578116.json  --deepspeed '
+ '[' false = true ']'
+ export CUDA_LAUNCH_BLOCKING=1
+ CUDA_LAUNCH_BLOCKING=1
+ export TORCHELASTIC_ERROR_FILE=/tmp/torch-elastic-error.json
+ TORCHELASTIC_ERROR_FILE=/tmp/torch-elastic-error.json
+ module purge
+ unset _mlshdbg
+ '[' 0 = 1 ']'
+ unset _mlre _mlIFS
+ '[' -n x ']'
+ _mlIFS=' 	
'
+ IFS=' '
+ for _mlv in ${MODULES_RUN_QUARANTINE:-}
+ '[' LD_LIBRARY_PATH = LD_LIBRARY_PATH -a LD_LIBRARY_PATH = LD_LIBRARY_PATH ']'
++ eval 'echo ${LD_LIBRARY_PATH+x}'
+++ echo x
+ '[' -n x ']'
++ eval 'echo ${LD_LIBRARY_PATH}'
+++ echo /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' '
+ _mlrv=MODULES_RUNENV_LD_LIBRARY_PATH
++ eval 'echo ${MODULES_RUNENV_LD_LIBRARY_PATH:-}'
+++ echo
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' '
+ '[' -n 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' ' ']'
++ eval 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\''' 'LD_LIBRARY_PATH='\'''\''' /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash '"$@"'
+++ LD_LIBRARY_PATH_modquar=/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+++ LD_LIBRARY_PATH=
+++ /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash purge
+ eval
+ _mlstatus=0
+ '[' -n x ']'
+ IFS=' 	
'
+ unset _mlre _mlv _mlrv _mlIFS
+ '[' -n '' ']'
+ unset _mlshdbg
+ return 0
+ module load cpuarch/amd
+ unset _mlshdbg
+ '[' 0 = 1 ']'
+ unset _mlre _mlIFS
+ '[' -n x ']'
+ _mlIFS=' 	
'
+ IFS=' '
+ for _mlv in ${MODULES_RUN_QUARANTINE:-}
+ '[' LD_LIBRARY_PATH = LD_LIBRARY_PATH -a LD_LIBRARY_PATH = LD_LIBRARY_PATH ']'
++ eval 'echo ${LD_LIBRARY_PATH+x}'
+++ echo x
+ '[' -n x ']'
++ eval 'echo ${LD_LIBRARY_PATH}'
+++ echo /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' '
+ _mlrv=MODULES_RUNENV_LD_LIBRARY_PATH
++ eval 'echo ${MODULES_RUNENV_LD_LIBRARY_PATH:-}'
+++ echo
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' '
+ '[' -n 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' ' ']'
++ eval 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\''' 'LD_LIBRARY_PATH='\'''\''' /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash '"$@"'
+++ LD_LIBRARY_PATH_modquar=/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+++ LD_LIBRARY_PATH=
+++ /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash load cpuarch/amd
+ eval '_LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1;' export '_LMFILES__modshare;
LOADEDMODULES_modshare=cpuarch/amd:1;' export 'LOADEDMODULES_modshare;
unset' 'MODULEPATH_modshare;
_LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd;' export '_LMFILES_;
LOADEDMODULES=cpuarch/amd;' export 'LOADEDMODULES;
MODULEPATH=/gpfslocalsup/pub/module-rh/modulefiles:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64;' export 'MODULEPATH;
test' '0;'
++ _LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1
++ export _LMFILES__modshare
++ LOADEDMODULES_modshare=cpuarch/amd:1
++ export LOADEDMODULES_modshare
++ unset MODULEPATH_modshare
++ _LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd
++ export _LMFILES_
++ LOADEDMODULES=cpuarch/amd
++ export LOADEDMODULES
++ MODULEPATH=/gpfslocalsup/pub/module-rh/modulefiles:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64
++ export MODULEPATH
++ test 0
+ _mlstatus=0
+ '[' -n x ']'
+ IFS=' 	
'
+ unset _mlre _mlv _mlrv _mlIFS
+ '[' -n '' ']'
+ unset _mlshdbg
+ return 0
+ module load anaconda-py3/2023.09
+ unset _mlshdbg
+ '[' 0 = 1 ']'
+ unset _mlre _mlIFS
+ '[' -n x ']'
+ _mlIFS=' 	
'
+ IFS=' '
+ for _mlv in ${MODULES_RUN_QUARANTINE:-}
+ '[' LD_LIBRARY_PATH = LD_LIBRARY_PATH -a LD_LIBRARY_PATH = LD_LIBRARY_PATH ']'
++ eval 'echo ${LD_LIBRARY_PATH+x}'
+++ echo x
+ '[' -n x ']'
++ eval 'echo ${LD_LIBRARY_PATH}'
+++ echo /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' '
+ _mlrv=MODULES_RUNENV_LD_LIBRARY_PATH
++ eval 'echo ${MODULES_RUNENV_LD_LIBRARY_PATH:-}'
+++ echo
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' '
+ '[' -n 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' ' ']'
++ eval 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\''' 'LD_LIBRARY_PATH='\'''\''' /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash '"$@"'
+++ LD_LIBRARY_PATH_modquar=/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+++ LD_LIBRARY_PATH=
+++ /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash load anaconda-py3/2023.09
+ eval '_LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:1;' export '_LMFILES__modshare;
LOADEDMODULES_modshare=anaconda-py3/2023.09:1:cpuarch/amd:1;' export 'LOADEDMODULES_modshare;
MODULES_LMCONFLICT_modshare=anaconda-py3/2023.09\&anaconda-py3\&anaconda-py2:1;' export 'MODULES_LMCONFLICT_modshare;
_LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09;' export '_LMFILES_;
LOADEDMODULES=cpuarch/amd:anaconda-py3/2023.09;' export 'LOADEDMODULES;
MODULES_LMCONFLICT=anaconda-py3/2023.09\&anaconda-py3\&anaconda-py2;' export 'MODULES_LMCONFLICT;
.' '/gpfslocalsup/pub/anaconda-py3/2023.09/etc/profile.d/conda.sh;
conda' activate 'base;
test' '0;'
++ _LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:1
++ export _LMFILES__modshare
++ LOADEDMODULES_modshare=anaconda-py3/2023.09:1:cpuarch/amd:1
++ export LOADEDMODULES_modshare
++ MODULES_LMCONFLICT_modshare='anaconda-py3/2023.09&anaconda-py3&anaconda-py2:1'
++ export MODULES_LMCONFLICT_modshare
++ _LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09
++ export _LMFILES_
++ LOADEDMODULES=cpuarch/amd:anaconda-py3/2023.09
++ export LOADEDMODULES
++ MODULES_LMCONFLICT='anaconda-py3/2023.09&anaconda-py3&anaconda-py2'
++ export MODULES_LMCONFLICT
++ . /gpfslocalsup/pub/anaconda-py3/2023.09/etc/profile.d/conda.sh
+++ export CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
+++ CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
+++ CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
++++ dirname /gpfslocalsup/pub/anaconda-py3/2023.09/bin
+++ PATH=/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate base
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate base
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate base
+++ /gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda shell.posix activate base
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin'\''
export CONDA_PREFIX='\''/gpfslocalsup/pub/anaconda-py3/2023.09'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin'\''
export CONDA_PREFIX='\''/gpfslocalsup/pub/anaconda-py3/2023.09'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/gpfslocalsup/pub/anaconda-py3/2023.09/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
+++ PATH=/gpfslocalsup/pub/anaconda-py3/2023.09/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
+++ export CONDA_PREFIX=/gpfslocalsup/pub/anaconda-py3/2023.09
+++ CONDA_PREFIX=/gpfslocalsup/pub/anaconda-py3/2023.09
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
+++ CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
+++ CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
++ test 0
+ _mlstatus=0
+ '[' -n x ']'
+ IFS=' 	
'
+ unset _mlre _mlv _mlrv _mlIFS
+ '[' -n '' ']'
+ unset _mlshdbg
+ return 0
+ conda activate lucie-torch211
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate lucie-torch211
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate lucie-torch211
++ /gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda shell.posix activate lucie-torch211
+ ask_conda='PS1='\''(lucie-torch211) '\''
export PATH='\''/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin'\''
export CONDA_PREFIX='\''/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''lucie-torch211'\''
export CONDA_PROMPT_MODIFIER='\''(lucie-torch211) '\''
export CONDA_PREFIX_1='\''/gpfslocalsup/pub/anaconda-py3/2023.09'\''
export CONDA_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python'\''
. "/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libblas_mkl_activate.sh"
. "/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libxml2_activate.sh"'
+ eval 'PS1='\''(lucie-torch211) '\''
export PATH='\''/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin'\''
export CONDA_PREFIX='\''/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''lucie-torch211'\''
export CONDA_PROMPT_MODIFIER='\''(lucie-torch211) '\''
export CONDA_PREFIX_1='\''/gpfslocalsup/pub/anaconda-py3/2023.09'\''
export CONDA_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python'\''
. "/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libblas_mkl_activate.sh"
. "/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libxml2_activate.sh"'
++ PS1='(lucie-torch211) '
++ export PATH=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
++ PATH=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
++ export CONDA_PREFIX=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211
++ CONDA_PREFIX=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=lucie-torch211
++ CONDA_DEFAULT_ENV=lucie-torch211
++ export 'CONDA_PROMPT_MODIFIER=(lucie-torch211) '
++ CONDA_PROMPT_MODIFIER='(lucie-torch211) '
++ export CONDA_PREFIX_1=/gpfslocalsup/pub/anaconda-py3/2023.09
++ CONDA_PREFIX_1=/gpfslocalsup/pub/anaconda-py3/2023.09
++ export CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
++ CONDA_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
++ CONDA_PYTHON_EXE=/gpfslocalsup/pub/anaconda-py3/2023.09/bin/python
++ . /linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libblas_mkl_activate.sh
+++ export CONDA_MKL_INTERFACE_LAYER_BACKUP=
+++ CONDA_MKL_INTERFACE_LAYER_BACKUP=
+++ export MKL_INTERFACE_LAYER=LP64,GNU
+++ MKL_INTERFACE_LAYER=LP64,GNU
++ . /linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/conda/activate.d/libxml2_activate.sh
+++ test -n ''
+++ xml_catalog_files_libxml2=
+++ XML_CATALOG_FILES=
+++ conda_catalog_files=
+++ ifs_libxml2=' 	
'
+++ IFS=' '
+++ rem=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211
+++ for pre in ${rem}
+++ test '' = /linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211
+++ conda_catalog_files=/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211
+++ rem=
+++ IFS=' 	
'
+++ conda_catalog_files='file:///linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/xml/catalog file:///etc/xml/catalog'
+++ export 'XML_CATALOG_FILES=file:///linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/xml/catalog file:///etc/xml/catalog'
+++ XML_CATALOG_FILES='file:///linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/etc/xml/catalog file:///etc/xml/catalog'
+++ unset conda_catalog_files ifs_libxml2 rem
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ module load cuda/12.1.0
+ unset _mlshdbg
+ '[' 0 = 1 ']'
+ unset _mlre _mlIFS
+ '[' -n x ']'
+ _mlIFS=' 	
'
+ IFS=' '
+ for _mlv in ${MODULES_RUN_QUARANTINE:-}
+ '[' LD_LIBRARY_PATH = LD_LIBRARY_PATH -a LD_LIBRARY_PATH = LD_LIBRARY_PATH ']'
++ eval 'echo ${LD_LIBRARY_PATH+x}'
+++ echo x
+ '[' -n x ']'
++ eval 'echo ${LD_LIBRARY_PATH}'
+++ echo /gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' '
+ _mlrv=MODULES_RUNENV_LD_LIBRARY_PATH
++ eval 'echo ${MODULES_RUNENV_LD_LIBRARY_PATH:-}'
+++ echo
+ _mlre='LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' '
+ '[' -n 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\'' LD_LIBRARY_PATH='\'''\'' ' ']'
++ eval 'LD_LIBRARY_PATH_modquar='\''/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib'\''' 'LD_LIBRARY_PATH='\'''\''' /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash '"$@"'
+++ LD_LIBRARY_PATH_modquar=/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
+++ LD_LIBRARY_PATH=
+++ /gpfslocalsup/spack_soft/tcl/8.6.8/gcc-4.8.5-5nqkfcnctewdheju62zvqbsonnzszr6m/bin/tclsh /gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/libexec/modulecmd.tcl bash load cuda/12.1.0
+ eval 'LD_LIBRARY_PATH=/gpfslocalsys/cuda/12.1.0/nvvm/lib64:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:/gpfslocalsys/cuda/12.1.0/lib64:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib;' export 'LD_LIBRARY_PATH;
MANPATH=/gpfslocalsys/cuda/12.1.0/doc/man::/opt/c3/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man:/gpfslocalsys/slurm/current/share/man:/usr/share/catman:/usr/share/man:/usr/catman:/usr/man;' export 'MANPATH;
LIBRARY_PATH=/gpfslocalsys/cuda/12.1.0/lib64/stubs:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:/gpfslocalsys/cuda/12.1.0/lib64:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib;' export 'LIBRARY_PATH;
_LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:/gpfslocalsup/pub/module-rh/modulefiles/cuda/12.1.0;' export '_LMFILES_;
LOADEDMODULES=cpuarch/amd:anaconda-py3/2023.09:cuda/12.1.0;' export 'LOADEDMODULES;
MODULES_LMCONFLICT=anaconda-py3/2023.09\&anaconda-py3\&anaconda-py2:cuda/12.1.0\&cuda;' export 'MODULES_LMCONFLICT;
MANPATH_modshare=:1:/opt/sgi/share/man:1:/gpfslocalsys/slurm/current/share/man:1:/opt/c3/man:1:/opt/clmgr/lib/cm-cli/man:1:/opt/clmgr/share/man:1:/usr/man:1:/usr/catman:1:/opt/clmgr/man:1:/gpfslocalsys/cuda/12.1.0/doc/man:1:/usr/share/man:1:/usr/share/catman:1;' export 'MANPATH_modshare;
NVHPC_CUDA_HOME=/gpfslocalsys/cuda/12.1.0;' export 'NVHPC_CUDA_HOME;
LIBRARY_PATH_modshare=/gpfslocalsys/cuda/12.1.0/lib64/stubs:1:/gpfslocalsys/cuda/12.1.0/lib64:1:/gpfslocalsys/slurm/current/lib/slurm:1:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:1:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:1:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:1:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:1:/gpfslocalsys/slurm/current/lib:1;' export 'LIBRARY_PATH_modshare;
MODULES_LMCONFLICT_modshare=anaconda-py3/2023.09\&anaconda-py3\&anaconda-py2:1:cuda/12.1.0\&cuda:1;' export 'MODULES_LMCONFLICT_modshare;
CPLUS_INCLUDE_PATH=/gpfslocalsys/cuda/12.1.0/include;' export 'CPLUS_INCLUDE_PATH;
CUDA_INSTALL_PATH=/gpfslocalsys/cuda/12.1.0;' export 'CUDA_INSTALL_PATH;
CUDA_ROOT=/gpfslocalsys/cuda/12.1.0;' export 'CUDA_ROOT;
CUDA_PATH=/gpfslocalsys/cuda/12.1.0;' export 'CUDA_PATH;
LD_LIBRARY_PATH_modshare=/gpfslocalsys/cuda/12.1.0/lib64:1:/gpfslocalsys/slurm/current/lib/slurm:1:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:1:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:1:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:1:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:1:/gpfslocalsys/slurm/current/lib:1;' export 'LD_LIBRARY_PATH_modshare;
C_INCLUDE_PATH=/gpfslocalsys/cuda/12.1.0/include;' export 'C_INCLUDE_PATH;
_LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1:/gpfslocalsup/pub/module-rh/modulefiles/cuda/12.1.0:1:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:1;' export '_LMFILES__modshare;
LOADEDMODULES_modshare=cuda/12.1.0:1:cpuarch/amd:1:anaconda-py3/2023.09:1;' export 'LOADEDMODULES_modshare;
PATH=/gpfslocalsys/cuda/12.1.0/samples:/gpfslocalsys/cuda/12.1.0/nvvm/bin:/gpfslocalsys/cuda/12.1.0/bin:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin;' export 'PATH;
CUDA_HOME=/gpfslocalsys/cuda/12.1.0;' export 'CUDA_HOME;
PATH_modshare=/usr/bin:1:/gpfslocalsup/bin:1:/usr/local/bin:1:/gpfslocalsys/cuda/12.1.0/bin:1:/opt/sgi/bin:1:/gpfslocalsys/slurm/current/bin:1:/opt/clmgr/bin:1:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:1:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:1:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:1:/opt/sgi/sbin:1:/gpfslocalsys/cuda/12.1.0/samples:1:/bin:1:/opt/clmgr/sbin:1:/gpfslocalsys/bin:1:/gpfslocalsys/cuda/12.1.0/nvvm/bin:1:/sbin:1:/usr/sbin:1:/usr/local/sbin:1:/usr/lpp/mmfs/bin:1:/opt/c3/bin:1;' export 'PATH_modshare;
test' '0;'
++ LD_LIBRARY_PATH=/gpfslocalsys/cuda/12.1.0/nvvm/lib64:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:/gpfslocalsys/cuda/12.1.0/lib64:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
++ export LD_LIBRARY_PATH
++ MANPATH=/gpfslocalsys/cuda/12.1.0/doc/man::/opt/c3/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man:/gpfslocalsys/slurm/current/share/man:/usr/share/catman:/usr/share/man:/usr/catman:/usr/man
++ export MANPATH
++ LIBRARY_PATH=/gpfslocalsys/cuda/12.1.0/lib64/stubs:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:/gpfslocalsys/cuda/12.1.0/lib64:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:/gpfslocalsys/slurm/current/lib/slurm:/gpfslocalsys/slurm/current/lib
++ export LIBRARY_PATH
++ _LMFILES_=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:/gpfslocalsup/pub/module-rh/modulefiles/cuda/12.1.0
++ export _LMFILES_
++ LOADEDMODULES=cpuarch/amd:anaconda-py3/2023.09:cuda/12.1.0
++ export LOADEDMODULES
++ MODULES_LMCONFLICT='anaconda-py3/2023.09&anaconda-py3&anaconda-py2:cuda/12.1.0&cuda'
++ export MODULES_LMCONFLICT
++ MANPATH_modshare=:1:/opt/sgi/share/man:1:/gpfslocalsys/slurm/current/share/man:1:/opt/c3/man:1:/opt/clmgr/lib/cm-cli/man:1:/opt/clmgr/share/man:1:/usr/man:1:/usr/catman:1:/opt/clmgr/man:1:/gpfslocalsys/cuda/12.1.0/doc/man:1:/usr/share/man:1:/usr/share/catman:1
++ export MANPATH_modshare
++ NVHPC_CUDA_HOME=/gpfslocalsys/cuda/12.1.0
++ export NVHPC_CUDA_HOME
++ LIBRARY_PATH_modshare=/gpfslocalsys/cuda/12.1.0/lib64/stubs:1:/gpfslocalsys/cuda/12.1.0/lib64:1:/gpfslocalsys/slurm/current/lib/slurm:1:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:1:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:1:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:1:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:1:/gpfslocalsys/slurm/current/lib:1
++ export LIBRARY_PATH_modshare
++ MODULES_LMCONFLICT_modshare='anaconda-py3/2023.09&anaconda-py3&anaconda-py2:1:cuda/12.1.0&cuda:1'
++ export MODULES_LMCONFLICT_modshare
++ CPLUS_INCLUDE_PATH=/gpfslocalsys/cuda/12.1.0/include
++ export CPLUS_INCLUDE_PATH
++ CUDA_INSTALL_PATH=/gpfslocalsys/cuda/12.1.0
++ export CUDA_INSTALL_PATH
++ CUDA_ROOT=/gpfslocalsys/cuda/12.1.0
++ export CUDA_ROOT
++ CUDA_PATH=/gpfslocalsys/cuda/12.1.0
++ export CUDA_PATH
++ LD_LIBRARY_PATH_modshare=/gpfslocalsys/cuda/12.1.0/lib64:1:/gpfslocalsys/slurm/current/lib/slurm:1:/gpfslocalsys/cuda/12.1.0/nvvm/lib64:1:/gpfslocalsys/cuda/12.1.0/extras/CUPTI/lib64:1:/gpfslocalsys/cuda/12.1.0/targets/x86_64-linux/lib:1:/gpfslocalsys/cuda/12.1.0/samples/common/lib/linux/x86_64:1:/gpfslocalsys/slurm/current/lib:1
++ export LD_LIBRARY_PATH_modshare
++ C_INCLUDE_PATH=/gpfslocalsys/cuda/12.1.0/include
++ export C_INCLUDE_PATH
++ _LMFILES__modshare=/gpfslocalsup/pub/module-rh/modulefiles/cpuarch/amd:1:/gpfslocalsup/pub/module-rh/modulefiles/cuda/12.1.0:1:/gpfslocalsup/pub/modules-idris-env4/modulefiles/linux-rhel8-x86_64/anaconda-py3/2023.09:1
++ export _LMFILES__modshare
++ LOADEDMODULES_modshare=cuda/12.1.0:1:cpuarch/amd:1:anaconda-py3/2023.09:1
++ export LOADEDMODULES_modshare
++ PATH=/gpfslocalsys/cuda/12.1.0/samples:/gpfslocalsys/cuda/12.1.0/nvvm/bin:/gpfslocalsys/cuda/12.1.0/bin:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/usr/lpp/mmfs/bin:/sbin:/bin:/gpfslocalsys/slurm/current/bin:/gpfslocalsup/bin:/gpfslocalsys/bin
++ export PATH
++ CUDA_HOME=/gpfslocalsys/cuda/12.1.0
++ export CUDA_HOME
++ PATH_modshare=/usr/bin:1:/gpfslocalsup/bin:1:/usr/local/bin:1:/gpfslocalsys/cuda/12.1.0/bin:1:/opt/sgi/bin:1:/gpfslocalsys/slurm/current/bin:1:/opt/clmgr/bin:1:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/bin:1:/gpfslocalsup/pub/anaconda-py3/2023.09/condabin:1:/gpfslocalsup/spack_soft/environment-modules/4.3.1/gcc-4.8.5-ism7cdy4xverxywj27jvjstqwk5oxe2v/bin:1:/opt/sgi/sbin:1:/gpfslocalsys/cuda/12.1.0/samples:1:/bin:1:/opt/clmgr/sbin:1:/gpfslocalsys/bin:1:/gpfslocalsys/cuda/12.1.0/nvvm/bin:1:/sbin:1:/usr/sbin:1:/usr/local/sbin:1:/usr/lpp/mmfs/bin:1:/opt/c3/bin:1
++ export PATH_modshare
++ test 0
+ _mlstatus=0
+ '[' -n x ']'
+ IFS=' 	
'
+ unset _mlre _mlv _mlrv _mlIFS
+ '[' -n '' ']'
+ unset _mlshdbg
+ return 0
+ DISTRIBUTED_ARGS='--nproc_per_node 8 --nnodes 8 --node_rank $SLURM_PROCID --rdzv_endpoint jean-zay-iam03:6000 --rdzv_backend c10d --max_restarts 0 --tee 3'
++ pwd
+ export 'RUN=torchrun --nproc_per_node 8 --nnodes 8 --node_rank $SLURM_PROCID --rdzv_endpoint jean-zay-iam03:6000 --rdzv_backend c10d --max_restarts 0 --tee 3        /gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/pretrain_gpt.py        --data-cache-path /linkhome/rech/genlor01/urc37ho/.cache        --tensor-model-parallel-size 1        --pipeline-model-parallel-size 2        --num-layers 32        --hidden-size 4096        --ffn-hidden-size 11008        --num-attention-heads 32        --micro-batch-size 3        --global-batch-size 192        --seq-length 2048        --max-position-embeddings 2048        --train-iters 250000        --save /gpfswork/rech/qgz/urc37ho/checkpoints12/        --load /gpfswork/rech/qgz/urc37ho/checkpoints12/        --data-path /gpfsscratch/rech/qgz/commun/preprocessed_data/Lucie/lucie_tokens_2.4-space_prefix_all/Wikipedia--fr--026_text_document        --data-impl mmap        --tokenizer-type PretrainedFromHF         --tokenizer-name-or-path OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all        --distributed-backend nccl        --lr 3e-4        --lr-decay-style cosine        --min-lr 3e-5        --weight-decay 0.1        --clip-grad 1        --lr-warmup-iters 2000        --optimizer adam        --adam-beta1 0.9        --adam-beta2 0.95        --log-interval 1        --save-interval 100        --eval-interval 100        --eval-iters 1        --bf16        --use-flash-attn-v2        --no-query-key-layer-scaling        --attention-dropout 0        --hidden-dropout 0        --use-rotary-position-embeddings        --untie-embeddings-and-output-weights        --swiglu        --normalization rmsnorm        --disable-bias-linear        --num-key-value-heads 32         --zero-stage=0  --deepspeed_config=./ds_config.1578116.json  --deepspeed                 '
+ RUN='torchrun --nproc_per_node 8 --nnodes 8 --node_rank $SLURM_PROCID --rdzv_endpoint jean-zay-iam03:6000 --rdzv_backend c10d --max_restarts 0 --tee 3        /gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/pretrain_gpt.py        --data-cache-path /linkhome/rech/genlor01/urc37ho/.cache        --tensor-model-parallel-size 1        --pipeline-model-parallel-size 2        --num-layers 32        --hidden-size 4096        --ffn-hidden-size 11008        --num-attention-heads 32        --micro-batch-size 3        --global-batch-size 192        --seq-length 2048        --max-position-embeddings 2048        --train-iters 250000        --save /gpfswork/rech/qgz/urc37ho/checkpoints12/        --load /gpfswork/rech/qgz/urc37ho/checkpoints12/        --data-path /gpfsscratch/rech/qgz/commun/preprocessed_data/Lucie/lucie_tokens_2.4-space_prefix_all/Wikipedia--fr--026_text_document        --data-impl mmap        --tokenizer-type PretrainedFromHF         --tokenizer-name-or-path OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all        --distributed-backend nccl        --lr 3e-4        --lr-decay-style cosine        --min-lr 3e-5        --weight-decay 0.1        --clip-grad 1        --lr-warmup-iters 2000        --optimizer adam        --adam-beta1 0.9        --adam-beta2 0.95        --log-interval 1        --save-interval 100        --eval-interval 100        --eval-iters 1        --bf16        --use-flash-attn-v2        --no-query-key-layer-scaling        --attention-dropout 0        --hidden-dropout 0        --use-rotary-position-embeddings        --untie-embeddings-and-output-weights        --swiglu        --normalization rmsnorm        --disable-bias-linear        --num-key-value-heads 32         --zero-stage=0  --deepspeed_config=./ds_config.1578116.json  --deepspeed                 '
+ srun --jobid 1578116 bash -c 'torchrun --nproc_per_node 8 --nnodes 8 --node_rank $SLURM_PROCID --rdzv_endpoint jean-zay-iam03:6000 --rdzv_backend c10d --max_restarts 0 --tee 3        /gpfswork/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/pretrain_gpt.py        --data-cache-path /linkhome/rech/genlor01/urc37ho/.cache        --tensor-model-parallel-size 1        --pipeline-model-parallel-size 2        --num-layers 32        --hidden-size 4096        --ffn-hidden-size 11008        --num-attention-heads 32        --micro-batch-size 3        --global-batch-size 192        --seq-length 2048        --max-position-embeddings 2048        --train-iters 250000        --save /gpfswork/rech/qgz/urc37ho/checkpoints12/        --load /gpfswork/rech/qgz/urc37ho/checkpoints12/        --data-path /gpfsscratch/rech/qgz/commun/preprocessed_data/Lucie/lucie_tokens_2.4-space_prefix_all/Wikipedia--fr--026_text_document        --data-impl mmap        --tokenizer-type PretrainedFromHF         --tokenizer-name-or-path OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all        --distributed-backend nccl        --lr 3e-4        --lr-decay-style cosine        --min-lr 3e-5        --weight-decay 0.1        --clip-grad 1        --lr-warmup-iters 2000        --optimizer adam        --adam-beta1 0.9        --adam-beta2 0.95        --log-interval 1        --save-interval 100        --eval-interval 100        --eval-iters 1        --bf16        --use-flash-attn-v2        --no-query-key-layer-scaling        --attention-dropout 0        --hidden-dropout 0        --use-rotary-position-embeddings        --untie-embeddings-and-output-weights        --swiglu        --normalization rmsnorm        --disable-bias-linear        --num-key-value-heads 32         --zero-stage=0  --deepspeed_config=./ds_config.1578116.json  --deepspeed                 '
+ tee -a /gpfswork/rech/qgz/urc37ho/lucie-logs/main_log.txt
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] 
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] 
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] 
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] 
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-21 10:47:03,289] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] 
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] 
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] 
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] 
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] *****************************************
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-21 10:47:03,290] torch.distributed.run: [WARNING] *****************************************
[default7]:[2024-04-21 10:47:10,287] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2024-04-21 10:47:10,394] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-04-21 10:47:10,389] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2024-04-21 10:47:10,394] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-04-21 10:47:10,393] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:[2024-04-21 10:47:10,367] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-04-21 10:47:10,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:[2024-04-21 10:47:10,393] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default1]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[2024-04-21 10:47:10,310] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2024-04-21 10:47:10,287] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-04-21 10:47:10,319] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default7]:[2024-04-21 10:47:10,395] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-04-21 10:47:10,395] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:[2024-04-21 10:47:10,394] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:[2024-04-21 10:47:10,396] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-04-21 10:47:10,373] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default5]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default5]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default5]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default5]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:/bin/sh: line 0: type: git: not found
[default1]:/bin/sh: line 0: type: git: not found
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default1]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default5]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default5]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default5]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default5]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default3]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:/bin/sh: line 0: type: git: not found
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default1]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default1]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 2.2.1
[default1]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.12.6, unknown, unknown
[default1]:torch cuda version ............... 12.1
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 12.1
[default1]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default1]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default7]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default7]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default7]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:DeepSpeed general environment info:
[default7]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default7]:torch version .................... 2.2.1
[default7]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default7]:deepspeed info ................... 0.12.6, unknown, unknown
[default7]:torch cuda version ............... 12.1
[default7]:torch hip version ................ None
[default7]:nvcc version ..................... 12.1
[default7]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default7]:shared memory (/dev/shm) size .... 251.60 GB
[default7]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default2]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default4]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default4]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default4]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:DeepSpeed general environment info:
[default7]:/bin/sh: line 0: type: git: not found
[default4]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default3]:/bin/sh: line 0: type: git: not found
[default2]:/bin/sh: line 0: type: git: not found
[default0]:/bin/sh: line 0: type: git: not found
[default6]:/bin/sh: line 0: type: git: not found
[default1]:/bin/sh: line 0: type: git: not found
[default7]:/bin/sh: line 0: type: git: not found
[default3]:/bin/sh: line 0: type: git: not found
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 2.2.1
[default3]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.12.6, unknown, unknown
[default3]:torch cuda version ............... 12.1
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 12.1
[default3]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default3]:shared memory (/dev/shm) size .... 251.60 GB
[default3]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default7]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default7]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default7]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:DeepSpeed general environment info:
[default7]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default7]:torch version .................... 2.2.1
[default7]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default7]:deepspeed info ................... 0.12.6, unknown, unknown
[default7]:torch cuda version ............... 12.1
[default7]:torch hip version ................ None
[default7]:nvcc version ..................... 12.1
[default7]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default7]:shared memory (/dev/shm) size .... 251.60 GB
[default7]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default2]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:using world size: 64, data-parallel-size: 32, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 2 
[default0]:accumulate and all-reduce gradients in fp32 for bfloat16 data type.
[default0]:using torch.bfloat16 for parameters ...
[default0]:------------------------ arguments ------------------------
[default0]:  accumulate_allreduce_grads_in_fp32 .............. True
[default0]:  adam_beta1 ...................................... 0.9
[default0]:  adam_beta2 ...................................... 0.95
[default0]:  adam_eps ........................................ 1e-08
[default0]:  add_bias_linear ................................. False
[default0]:  add_position_embedding .......................... False
[default0]:  adlr_autoresume ................................. False
[default0]:  adlr_autoresume_interval ........................ 1000
[default0]:  aml_data_download_path .......................... None
[default0]:  apply_layernorm_1p .............................. False
[default0]:  apply_query_key_layer_scaling ................... False
[default0]:  apply_residual_connection_post_layernorm ........ False
[default0]:  async_tensor_model_parallel_allreduce ........... False
[default0]:  attention_dropout ............................... 0.0
[default0]:  attention_softmax_in_fp32 ....................... False
[default0]:  barrier_with_L1_time ............................ True
[default0]:  bert_binary_head ................................ True
[default0]:  bert_embedder_type .............................. megatron
[default0]:  bert_load ....................................... None
[default0]:  bf16 ............................................ True
[default0]:  bias_dropout_fusion ............................. True
[default0]:  bias_gelu_fusion ................................ False
[default0]:  biencoder_projection_dim ........................ 0
[default0]:  biencoder_shared_query_context_model ............ False
[default0]:  block_data_path ................................. None
[default0]:  checkpoint_activations .......................... False
[default0]:  checkpoint_in_cpu ............................... False
[default0]:  checkpoint_num_layers ........................... 1
[default0]:  classes_fraction ................................ 1.0
[default0]:  clip_grad ....................................... 1.0
[default0]:  compression_training ............................ False
[default0]:  consumed_train_samples .......................... 0
[default0]:  consumed_train_tokens ........................... 0
[default0]:  consumed_valid_samples .......................... 0
[default0]:  contigious_checkpointing ........................ False
[default0]:  cpu_optimizer ................................... False
[default0]:  cpu_torch_adam .................................. False
[default0]:  create_moe_param_group .......................... False
[default0]:  curriculum_learning_legacy ...................... False
[default0]:  data_cache_path ................................. /linkhome/rech/genlor01/urc37ho/.cache
[default0]:  data_efficiency_curriculum_learning ............. False
[default0]:  data_impl ....................................... mmap
[default0]:  data_parallel_random_init ....................... False
[default0]:  data_parallel_size .............................. 32
[default0]:  data_path ....................................... ['/gpfsscratch/rech/qgz/commun/preprocessed_data/Lucie/lucie_tokens_2.4-space_prefix_all/Wikipedia--fr--026_text_document']
[default0]:  data_per_class_fraction ......................... 1.0
[default0]:  data_sharding ................................... True
[default0]:  dataloader_type ................................. single
[default0]:  DDP_impl ........................................ local
[default0]:  decoder_num_layers .............................. None
[default0]:  decoder_seq_length .............................. None
[default0]:  deepscale ....................................... False
[default0]:  deepscale_config ................................ None
[default0]:  deepspeed ....................................... True
[default0]:  deepspeed_activation_checkpointing .............. False
[default0]:  deepspeed_config ................................ ./ds_config.1578116.json
[default0]:  deepspeed_mpi ................................... False
[default0]:  dino_bottleneck_size ............................ 256
[default0]:  dino_freeze_last_layer .......................... 1
[default0]:  dino_head_hidden_size ........................... 2048
[default0]:  dino_local_crops_number ......................... 10
[default0]:  dino_local_img_size ............................. 96
[default0]:  dino_norm_last_layer ............................ False
[default0]:  dino_teacher_temp ............................... 0.07
[default0]:  dino_warmup_teacher_temp ........................ 0.04
[default0]:  dino_warmup_teacher_temp_epochs ................. 30
[default0]:  disable_mem_efficient_ln ........................ True
[default0]:  distribute_checkpointed_activations ............. False
[default0]:  distribute_saved_activations .................... False
[default0]:  distributed_backend ............................. nccl
[default0]:  distributed_timeout_minutes ..................... 10
[default0]:  ds_inference .................................... False
[default0]:  ds_pipeline_enabled ............................. True
[default0]:  ds_sequence_parallel_size ....................... 1
[default0]:  embedding_path .................................. None
[default0]:  embedding_weights_in_fp32 ....................... False
[default0]:  empty_unused_memory_level ....................... 0
[default0]:  enable_expert_tensor_parallelism ................ False
[default0]:  encoder_num_layers .............................. 32
[default0]:  encoder_seq_length .............................. 2048
[default0]:  end_weight_decay ................................ 0.1
[default0]:  eod_mask_loss ................................... False
[default0]:  eval_interval ................................... 100
[default0]:  eval_iters ...................................... 1
[default0]:  evidence_data_path .............................. None
[default0]:  exit_duration_in_mins ........................... None
[default0]:  exit_interval ................................... None
[default0]:  exit_on_missing_checkpoint ...................... False
[default0]:  exit_signal_handler ............................. False
[default0]:  expert_interval ................................. 2
[default0]:  ffn_hidden_size ................................. 11008
[default0]:  finetune ........................................ False
[default0]:  force_ds_sequence_parallel ...................... False
[default0]:  fp16 ............................................ False
[default0]:  fp16_lm_cross_entropy ........................... False
[default0]:  fp32_residual_connection ........................ False
[default0]:  fp8_amax_compute_algo ........................... most_recent
[default0]:  fp8_amax_history_len ............................ 1
[default0]:  fp8_e4m3 ........................................ False
[default0]:  fp8_hybrid ...................................... False
[default0]:  fp8_interval .................................... 1
[default0]:  fp8_margin ...................................... 0
[default0]:  fp8_wgrad ....................................... True
[default0]:  global_batch_size ............................... 192
[default0]:  gradient_accumulation_fusion .................... True
[default0]:  head_lr_mult .................................... 1.0
[default0]:  hidden_dropout .................................. 0.0
[default0]:  hidden_size ..................................... 4096
[default0]:  hidden_size_teacher ............................. None
[default0]:  hysteresis ...................................... 2
[default0]:  ict_head_size ................................... None
[default0]:  ict_load ........................................ None
[default0]:  img_h ........................................... 224
[default0]:  img_w ........................................... 224
[default0]:  indexer_batch_size .............................. 128
[default0]:  indexer_log_interval ............................ 1000
[default0]:  inference ....................................... False
[default0]:  inference_batch_times_seqlen_threshold .......... 512
[default0]:  init_method_std ................................. 0.02
[default0]:  init_method_xavier_uniform ...................... False
[default0]:  initial_loss_scale .............................. 4294967296
[default0]:  iter_per_epoch .................................. 1250
[default0]:  kd .............................................. False
[default0]:  kd_alpha_ce ..................................... 1
[default0]:  kd_beta_ce ...................................... 1
[default0]:  kd_temp ......................................... 1.0
[default0]:  kv_channels ..................................... 128
[default0]:  layernorm_epsilon ............................... 1e-05
[default0]:  lazy_mpu_init ................................... None
[default0]:  load ............................................ /gpfswork/rech/qgz/urc37ho/checkpoints12/
[default0]:  load_iteration .................................. None
[default0]:  load_teacher .................................... None
[default0]:  local_rank ...................................... None
[default0]:  log_batch_size_to_tensorboard ................... False
[default0]:  log_interval .................................... 1
[default0]:  log_learning_rate_to_tensorboard ................ True
[default0]:  log_loss_scale_to_tensorboard ................... True
[default0]:  log_memory_to_tensorboard ....................... False
[default0]:  log_num_zeros_in_grad ........................... False
[default0]:  log_optimizer_states_to_tensorboard ............. False
[default0]:  log_params_norm ................................. False
[default0]:  log_timers_to_tensorboard ....................... False
[default0]:  log_validation_ppl_to_tensorboard ............... False
[default0]:  log_world_size_to_tensorboard ................... False
[default0]:  loss_scale ...................................... None
[default0]:  loss_scale_window ............................... 1000
[default0]:  lr .............................................. 0.0003
[default0]:  lr_decay_iters .................................. None
[default0]:  lr_decay_samples ................................ None
[default0]:  lr_decay_style .................................. cosine
[default0]:  lr_decay_tokens ................................. None
[default0]:  lr_warmup_fraction .............................. None
[default0]:  lr_warmup_iters ................................. 2000
[default0]:  lr_warmup_samples ............................... 0
[default0]:  lr_warmup_tokens ................................ None
[default0]:  make_vocab_size_divisible_by .................... 128
[default0]:  mask_factor ..................................... 1.0
[default0]:  mask_prob ....................................... 0.15
[default0]:  mask_type ....................................... random
[default0]:  masked_softmax_fusion ........................... True
[default0]:  max_position_embeddings ......................... 2048
[default0]:  max_tokens_to_oom ............................... 12000
[default0]:  memory_centric_tiled_linear ..................... False
[default0]:  merge_file ...................................... None
[default0]:  micro_batch_size ................................ 3
[default0]:  min_loss_scale .................................. 1.0
[default0]:  min_lr .......................................... 3e-05
[default0]:  mlp_type ........................................ standard
[default0]:  mmap_warmup ..................................... False
[default0]:  moe_eval_capacity_factor ........................ 1.0
[default0]:  moe_expert_parallel_size ........................ 1
[default0]:  moe_loss_coeff .................................. 0.1
[default0]:  moe_min_capacity ................................ 4
[default0]:  moe_token_dropping .............................. True
[default0]:  moe_train_capacity_factor ....................... 1.0
[default0]:  mos ............................................. False
[default0]:  multiple_valid_sets ............................. False
[default0]:  no_load_lr_state ................................ False
[default0]:  no_load_optim ................................... None
[default0]:  no_load_rng ..................................... None
[default0]:  no_persist_layer_norm ........................... False
[default0]:  no_pipeline_parallel ............................ False
[default0]:  no_save_optim ................................... None
[default0]:  no_save_rng ..................................... None
[default0]:  normalization ................................... rmsnorm
[default0]:  num_attention_heads ............................. 32
[default0]:  num_attention_heads_teacher ..................... None
[default0]:  num_channels .................................... 3
[default0]:  num_classes ..................................... 1000
[default0]:  num_experts ..................................... [1]
[default0]:  num_experts_switch .............................. None
[default0]:  num_experts_teacher ............................. [1]
[default0]:  num_key_value_heads ............................. 32
[default0]:  num_layers ...................................... 32
[default0]:  num_layers_per_virtual_pipeline_stage ........... None
[default0]:  num_layers_teacher .............................. None
[default0]:  num_workers ..................................... 2
[default0]:  onnx_safe ....................................... None
[default0]:  openai_gelu ..................................... False
[default0]:  optimizer ....................................... adam
[default0]:  output_bert_embeddings .......................... False
[default0]:  overlap_p2p_comm ................................ False
[default0]:  override_opt_param_scheduler .................... False
[default0]:  params_dtype .................................... torch.bfloat16
[default0]:  partition_activations ........................... False
[default0]:  patch_dim ....................................... 16
[default0]:  perform_initialization .......................... True
[default0]:  pipeline_model_parallel_size .................... 2
[default0]:  pipeline_model_parallel_split_rank .............. None
[default0]:  profile_backward ................................ False
[default0]:  query_in_block_prob ............................. 0.1
[default0]:  rampup_batch_size ............................... None
[default0]:  random_ltd ...................................... False
[default0]:  rank ............................................ 0
[default0]:  recompute_granularity ........................... None
[default0]:  recompute_method ................................ None
[default0]:  recompute_num_layers ............................ 1
[default0]:  remote_device ................................... none
[default0]:  reset_attention_mask ............................ False
[default0]:  reset_iteration ................................. False
[default0]:  reset_position_ids .............................. False
[default0]:  retriever_report_topk_accuracies ................ []
[default0]:  retriever_score_scaling ......................... False
[default0]:  retriever_seq_length ............................ 256
[default0]:  retro_add_retriever ............................. False
[default0]:  retro_cyclic_train_iters ........................ None
[default0]:  retro_encoder_attention_dropout ................. 0.1
[default0]:  retro_encoder_hidden_dropout .................... 0.1
[default0]:  retro_encoder_layers ............................ 2
[default0]:  retro_num_neighbors ............................. 2
[default0]:  retro_num_retrieved_chunks ...................... 2
[default0]:  retro_return_doc_ids ............................ False
[default0]:  retro_workdir ................................... None
[default0]:  return_data_index ............................... False
[default0]:  rotary_percent .................................. 1.0
[default0]:  sample_rate ..................................... 1.0
[default0]:  save ............................................ /gpfswork/rech/qgz/urc37ho/checkpoints12/
[default0]:  save_interval ................................... 100
[default0]:  scatter_gather_tensors_in_pipeline .............. True
[default0]:  scattered_embeddings ............................ False
[default0]:  seed ............................................ 1234
[default0]:  seq_length ...................................... 2048
[default0]:  sequence_parallel ............................... False
[default0]:  sgd_momentum .................................... 0.9
[default0]:  short_seq_prob .................................. 0.1
[default0]:  skip_train ...................................... False
[default0]:  split ........................................... 969, 30, 1
[default0]:  split_transformers .............................. False
[default0]:  squared_relu .................................... False
[default0]:  standalone_embedding_stage ...................... False
[default5]:/bin/sh: line 0: type: git: not found
[default0]:  start_weight_decay .............................. 0.1
[default0]:  swiglu .......................................... True
[default0]:  swin_backbone_type .............................. tiny
[default0]:  synchronize_each_layer .......................... False
[default0]:  tensor_model_parallel_size ...................... 1
[default0]:  tensorboard_dir ................................. None
[default0]:  tensorboard_log_interval ........................ 1
[default0]:  tensorboard_queue_size .......................... 1000
[default0]:  test_data_path .................................. None
[default0]:  tile_factor ..................................... 1
[default0]:  timing_log_level ................................ 0
[default0]:  timing_log_option ............................... minmax
[default0]:  titles_data_path ................................ None
[default0]:  tokenizer_model ................................. None
[default0]:  tokenizer_name_or_path .......................... OpenLLM-France/Lucie-tokenizer-v2.4-space_prefix_all
[default0]:  tokenizer_type .................................. PretrainedFromHF
[default0]:  topk ............................................ 1
[default0]:  train_data_exact_num_epochs ..................... None
[default0]:  train_data_path ................................. None
[default0]:  train_desc_path ................................. None
[default0]:  train_doc_idx_path .............................. None
[default0]:  train_idx_path .................................. None
[default0]:  train_iters ..................................... 250000
[default0]:  train_sample_idx_path ........................... None
[default0]:  train_samples ................................... None
[default0]:  train_shuffle_idx_path .......................... None
[default0]:  train_tokens .................................... None
[default0]:  transformer_impl ................................ local
[default0]:  transformer_pipeline_model_parallel_size ........ 2
[default0]:  universal_checkpoint ............................ False
[default0]:  untie_embeddings_and_output_weights ............. True
[default0]:  use_checkpoint_args ............................. False
[default0]:  use_checkpoint_opt_param_scheduler .............. False
[default0]:  use_contiguous_buffers_in_local_ddp ............. True
[default0]:  use_cpu_initialization .......................... None
[default0]:  use_dataset_only ................................ False
[default0]:  use_distributed_optimizer ....................... False
[default0]:  use_flash_attn .................................. True
[default0]:  use_flash_attn_triton ........................... False
[default0]:  use_flash_attn_v1 ............................... False
[default0]:  use_flash_attn_v2 ............................... True
[default0]:  use_one_sent_docs ............................... False
[default0]:  use_pin_memory .................................. False
[default0]:  use_ring_exchange_p2p ........................... False
[default0]:  use_rotary_position_embeddings .................. True
[default0]:  use_tutel ....................................... False
[default0]:  valid_data_path ................................. None
[default0]:  variable_seq_lengths ............................ False
[default0]:  virtual_pipeline_model_parallel_size ............ None
[default0]:  vision_backbone_type ............................ vit
[default0]:  vision_pretraining .............................. False
[default0]:  vision_pretraining_type ......................... classify
[default0]:  vocab_extra_ids ................................. 0
[default0]:  vocab_file ...................................... None
[default0]:  vocab_size ...................................... None
[default0]:  wandb_api_key ................................... None
[default0]:  wandb_entity .................................... None
[default0]:  wandb_id ........................................ None
[default0]:  wandb_logger .................................... False
[default0]:  wandb_project ................................... megatron-ds-training
[default0]:  wandb_resume .................................... None
[default0]:  wandb_run_name .................................. None
[default0]:  weight_decay .................................... 0.1
[default0]:  weight_decay_incr_style ......................... constant
[default0]:  world_size ...................................... 64
[default0]:  zero_allgather_bucket_size ...................... 0.0
[default0]:  zero_contigious_gradients ....................... False
[default0]:  zero_reduce_bucket_size ......................... 0.0
[default0]:  zero_reduce_scatter ............................. False
[default0]:  zero_stage ...................................... 0
[default0]:-------------------- end of arguments ---------------------
[default0]:setting number of micro-batches to constant 2
[default0]:> building PretrainedFromHF tokenizer ...
[default0]: vocab file is un-used. loading tokenizer from pre-trained model
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default4]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default4]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default4]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:DeepSpeed general environment info:
[default4]:/bin/sh: line 0: type: git: not found
[default2]:/bin/sh: line 0: type: git: not found
[default6]:/bin/sh: line 0: type: git: not found
[default5]:/bin/sh: line 0: type: git: not found
[default1]:[2024-04-21 10:47:10,287] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2024-04-21 10:47:10,396] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:[2024-04-21 10:47:10,394] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-04-21 10:47:10,395] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-04-21 10:47:10,390] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:[2024-04-21 10:47:10,395] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default7]:[2024-04-21 10:47:10,397] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-04-21 10:47:10,396] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:[2024-04-21 10:47:10,285] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-04-21 10:47:10,391] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2024-04-21 10:47:10,388] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-04-21 10:47:10,391] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:[2024-04-21 10:47:10,384] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2024-04-21 10:47:10,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-04-21 10:47:10,390] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:[2024-04-21 10:47:10,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:[2024-04-21 10:47:10,283] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:[2024-04-21 10:47:10,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-04-21 10:47:10,393] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2024-04-21 10:47:10,387] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-04-21 10:47:10,391] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-04-21 10:47:10,361] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:[2024-04-21 10:47:10,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2024-04-21 10:47:10,391] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:loading file tokenizer.model from cache at None
[default0]:loading file tokenizer.json from cache at /linkhome/rech/genlor01/urc37ho/.cache/huggingface/hub/models--OpenLLM-France--Lucie-tokenizer-v2.4-space_prefix_all/snapshots/edf1b3d40698a2eec64a63aa8731c510e5810ae6/tokenizer.json
[default0]:loading file added_tokens.json from cache at None
[default0]:loading file special_tokens_map.json from cache at /linkhome/rech/genlor01/urc37ho/.cache/huggingface/hub/models--OpenLLM-France--Lucie-tokenizer-v2.4-space_prefix_all/snapshots/edf1b3d40698a2eec64a63aa8731c510e5810ae6/special_tokens_map.json
[default0]:loading file tokenizer_config.json from cache at /linkhome/rech/genlor01/urc37ho/.cache/huggingface/hub/models--OpenLLM-France--Lucie-tokenizer-v2.4-space_prefix_all/snapshots/edf1b3d40698a2eec64a63aa8731c510e5810ae6/tokenizer_config.json
[default4]:[2024-04-21 10:47:10,283] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2024-04-21 10:47:10,389] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-04-21 10:47:10,385] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-04-21 10:47:10,383] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:[2024-04-21 10:47:10,368] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2024-04-21 10:47:10,394] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default7]:[2024-04-21 10:47:10,368] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-04-21 10:47:10,395] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default1]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default5]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default5]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default5]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default7]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default7]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:[2024-04-21 10:47:10,298] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:[2024-04-21 10:47:10,298] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-04-21 10:47:10,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:[2024-04-21 10:47:10,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-04-21 10:47:10,389] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-04-21 10:47:10,393] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2024-04-21 10:47:10,363] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default7]:[2024-04-21 10:47:10,393] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default1]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:/bin/sh: line 0: type: git: not found
[default4]:/bin/sh: line 0: type: git: not found
[default3]:/bin/sh: line 0: type: git: not found
[default0]:/bin/sh: line 0: type: git: not found
[default5]:/bin/sh: line 0: type: git: not found
[default6]:/bin/sh: line 0: type: git: not found
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:DeepSpeed general environment info:
[default5]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default5]:torch version .................... 2.2.1
[default5]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default5]:deepspeed info ................... 0.12.6, unknown, unknown
[default5]:torch cuda version ............... 12.1
[default5]:torch hip version ................ None
[default5]:nvcc version ..................... 12.1
[default5]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default5]:shared memory (/dev/shm) size .... 251.60 GB
[default5]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default1]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default7]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default7]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:DeepSpeed general environment info:
[default7]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default7]:torch version .................... 2.2.1
[default7]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default7]:deepspeed info ................... 0.12.6, unknown, unknown
[default7]:torch cuda version ............... 12.1
[default7]:torch hip version ................ None
[default7]:nvcc version ..................... 12.1
[default7]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default7]:shared memory (/dev/shm) size .... 251.60 GB
[default7]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default3]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default3]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 2.2.1
[default3]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.12.6, unknown, unknown
[default3]:torch cuda version ............... 12.1
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 12.1
[default3]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default3]:shared memory (/dev/shm) size .... 251.60 GB
[default3]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default4]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default4]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default4]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:/bin/sh: line 0: type: git: not found
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default5]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default5]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default5]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default5]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default5]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default5]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default1]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:/bin/sh: line 0: type: git: not found
[default7]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:--------------------------------------------------
[default2]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default4]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default4]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:DeepSpeed general environment info:
[default4]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default4]:torch version .................... 2.2.1
[default4]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default4]:deepspeed info ................... 0.12.6, unknown, unknown
[default4]:torch cuda version ............... 12.1
[default4]:torch hip version ................ None
[default4]:nvcc version ..................... 12.1
[default4]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default4]:shared memory (/dev/shm) size .... 251.60 GB
[default4]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:[2024-04-21 10:47:34,614] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default3]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 2.2.1
[default3]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.12.6, unknown, unknown
[default3]:torch cuda version ............... 12.1
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 12.1
[default3]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default3]:shared memory (/dev/shm) size .... 251.60 GB
[default3]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default1]:[2024-04-21 10:47:10,285] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default5]:[2024-04-21 10:47:10,393] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2024-04-21 10:47:10,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default6]:[2024-04-21 10:47:10,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2024-04-21 10:47:10,354] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default7]:[2024-04-21 10:47:10,391] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2024-04-21 10:47:10,389] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default4]:[2024-04-21 10:47:10,393] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default4]:--------------------------------------------------
[default4]:DeepSpeed C++/CUDA extension op report
[default4]:--------------------------------------------------
[default4]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default4]:      runtime if needed. Op compatibility means that your system
[default4]:      meet the required dependencies to JIT install the op.
[default4]:--------------------------------------------------
[default4]:JIT compiled ops requires ninja
[default4]:ninja .................. [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:op name ................ installed .. compatible
[default4]:--------------------------------------------------
[default5]:--------------------------------------------------
[default5]:DeepSpeed C++/CUDA extension op report
[default5]:--------------------------------------------------
[default5]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default5]:      runtime if needed. Op compatibility means that your system
[default5]:      meet the required dependencies to JIT install the op.
[default5]:--------------------------------------------------
[default5]:JIT compiled ops requires ninja
[default5]:ninja .................. [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:op name ................ installed .. compatible
[default5]:--------------------------------------------------
[default7]:--------------------------------------------------
[default7]:DeepSpeed C++/CUDA extension op report
[default7]:--------------------------------------------------
[default7]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default7]:      runtime if needed. Op compatibility means that your system
[default7]:      meet the required dependencies to JIT install the op.
[default7]:--------------------------------------------------
[default7]:JIT compiled ops requires ninja
[default7]:ninja .................. [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:op name ................ installed .. compatible
[default7]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default6]:--------------------------------------------------
[default6]:DeepSpeed C++/CUDA extension op report
[default6]:--------------------------------------------------
[default6]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default6]:      runtime if needed. Op compatibility means that your system
[default6]:      meet the required dependencies to JIT install the op.
[default6]:--------------------------------------------------
[default6]:JIT compiled ops requires ninja
[default6]:ninja .................. [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:op name ................ installed .. compatible
[default6]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default4]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default5]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default7]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default6]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:/bin/sh: line 0: type: git: not found
[default6]:/bin/sh: line 0: type: git: not found
[default1]:/bin/sh: line 0: type: git: not found
[default2]:/bin/sh: line 0: type: git: not found
[default4]:/bin/sh: line 0: type: git: not found
[default6]:/bin/sh: line 0: type: git: not found
[default5]:/bin/sh: line 0: type: git: not found
[default3]:/bin/sh: line 0: type: git: not found
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:--------------------------------------------------
[default2]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:[2024-04-21 10:47:34,643] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default3]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 2.2.1
[default3]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.12.6, unknown, unknown
[default3]:torch cuda version ............... 12.1
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 12.1
[default3]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default3]:shared memory (/dev/shm) size .... 251.60 GB
[default3]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default4]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [default2]:/bin/sh: line 0: type: git: not found
[default1]:/bin/sh: line 0: type: git: not found
[default0]:/bin/sh: line 0: type: git: not found
[default3]:/bin/sh: line 0: type: git: not found
[default7]:/bin/sh: line 0: type: git: not found
[default0]:/bin/sh: line 0: type: git: not found
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default2]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default3]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default3]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 2.2.1
[default3]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.12.6, unknown, unknown
[default3]:torch cuda version ............... 12.1
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 12.1
[default3]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default3]:shared memory (/dev/shm) size .... 251.60 GB
[default3]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default4]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default4]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default4]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:DeepSpeed general environment info:
[default4]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default4]:torch version .................... 2.2.1
[default4]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default4]:deepspeed info ................... 0.12.6, unknown, unknown
[default4]:torch cuda version ............... 12.1
[default4]:torch hip version ................ None
[default4]:nvcc version ..................... 12.1
[default4]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default4]:shared memory (/dev/shm) size .... 251.60 GB
[default4]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default7]:/bin/sh: line 0: type: git: not found
[default3]:/bin/sh: line 0: type: git: not found
[default2]:/bin/sh: line 0: type: git: not found
[default6]:/bin/sh: line 0: type: git: not found
[default1]:/bin/sh: line 0: type: git: not found
[default2]:/bin/sh: line 0: type: git: not found
[default5]:/bin/sh: line 0: type: git: not found
[default4]:/bin/sh: line 0: type: git: not found
[default5]:/bin/sh: line 0: type: git: not found
[default4]:/bin/sh: line 0: type: git: not found
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default5]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default5]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default5]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default1]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default3]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:/bin/sh: line 0: type: git: not found
[default3]:/bin/sh: line 0: type: git: not found
[default5]:/bin/sh: line 0: type: git: not found
[default4]:/bin/sh: line 0: type: git: not found
[default6]:/bin/sh: line 0: type: git: not found
[default1]:/bin/sh: line 0: type: git: not found
[default7]:/bin/sh: line 0: type: git: not found
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 2.2.1
[default3]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.12.6, unknown, unknown
[default3]:torch cuda version ............... 12.1
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 12.1
[default3]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default3]:shared memory (/dev/shm) size .... 251.60 GB
[default3]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:DeepSpeed general environment info:
[default5]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default5]:torch version .................... 2.2.1
[default5]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default5]:deepspeed info ................... 0.12.6, unknown, unknown
[default5]:torch cuda version ............... 12.1
[default5]:torch hip version ................ None
[default5]:nvcc version ..................... 12.1
[default5]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default5]:shared memory (/dev/shm) size .... 251.60 GB
[default5]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default7]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default7]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default7]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:DeepSpeed general environment info:
[default7]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default7]:torch version .................... 2.2.1
[default2]:/bin/sh: line 0: type: git: not found
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default0]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default5]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default5]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default5]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default5]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default1]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default2]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default7]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default7]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default7]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default7]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default6]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default6]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default6]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default6]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:/bin/sh: line 0: type: git: not found
[default0]:/bin/sh: line 0: type: git: not found
[default5]:/bin/sh: line 0: type: git: not found
[default1]:/bin/sh: line 0: type: git: not found
[default7]:/bin/sh: line 0: type: git: not found
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:[2024-04-21 10:47:34,833] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default4]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default4]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:DeepSpeed general environment info:
[default4]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default4]:torch version .................... 2.2.1
[default4]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default4]:deepspeed info ................... 0.12.6, unknown, unknown
[default4]:torch cuda version ............... 12.1
[default4]:torch hip version ................ None
[default4]:nvcc version ..................... 12.1
[default4]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default4]:shared memory (/dev/shm) size .... 251.60 GB
[default4]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default1]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default1]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 2.2.1
[default1]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.12.6, unknown, unknown
[default1]:torch cuda version ............... 12.1
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 12.1
[default1]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default1]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default2]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:DeepSpeed general environment info:
[default5]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default4]:/bin/sh: line 0: type: git: not found
[default6]:/bin/sh: line 0: type: git: not found
[default3]:/bin/sh: line 0: type: git: not found
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:Loading extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:Loading extension module scaled_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:Loading extension module scaled_softmax_cuda...
[default0]:[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank59]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank61]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:[rank56]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank62]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank57]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank63]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank58]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank60]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank43]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank42]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:[rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank47]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank46]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:[rank40]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank41]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank44]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank22]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank23]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:[rank48]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank20]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank21]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank38]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank45]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank27]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank26]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank29]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank28]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:[rank24]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank25]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank53]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank14]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:[rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank51]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank39]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank15]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank34]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default3]:[rank35]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank33]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank30]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:[rank32]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank37]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank52]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default2]:[rank50]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default6]:[rank54]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default1]:[rank49]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank31]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default5]:[rank13]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank12]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default4]:[rank36]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default7]:[rank55]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default3]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default3]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default0]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default0]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default2]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default2]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default6]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default6]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default1]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default1]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default5]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default5]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default4]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default7]:/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/training.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/tensor/python_tensor.cpp:83.)
[default7]:  start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
[default4]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default4]:torch version .................... 2.2.1
[default4]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default4]:deepspeed info ................... 0.12.6, unknown, unknown
[default4]:torch cuda version ............... 12.1
[default4]:torch hip version ................ None
[default4]:nvcc version ..................... 12.1
[default4]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default4]:shared memory (/dev/shm) size .... 251.60 GB
[default4]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default1]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default1]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 2.2.1
[default1]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.12.6, unknown, unknown
[default1]:torch cuda version ............... 12.1
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 12.1
[default1]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default1]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:DeepSpeed general environment info:
[default5]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default5]:torch version .................... 2.2.1
[default5]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default5]:deepspeed info ................... 0.12.6, unknown, unknown
[default5]:torch cuda version ............... 12.1
[default5]:torch hip version ................ None
[default5]:nvcc version ..................... 12.1
[default5]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default5]:shared memory (/dev/shm) size .... 251.60 GB
[default5]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]: > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
[default0]:> initializing torch distributed ...
[default0]:[2024-04-21 10:47:34,394] [INFO] [comm.py:637:init_distributed] cdb=None
[default0]:[2024-04-21 10:47:34,394] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default5]:[2024-04-21 10:47:35,237] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-04-21 10:47:35,265] [INFO] [comm.py:637:init_distributed] cdb=None
[default7]:[2024-04-21 10:47:35,260] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-04-21 10:47:35,280] [INFO] [comm.py:637:init_distributed] cdb=None
[default6]:[2024-04-21 10:47:35,266] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:[2024-04-21 10:47:35,280] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-04-21 10:47:35,264] [INFO] [comm.py:637:init_distributed] cdb=None
[default0]:> initialized tensor model parallel with size 1
[default0]:> initialized pipeline model parallel with size 2
[default0]:> setting random seeds to 1234 ...
[default0]:> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[default0]:> compiling dataset index builder ...
[default0]:make: Entering directory '/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/data'
[default0]:make: Nothing to be done for 'default'.
[default0]:make: Leaving directory '/gpfsdswork/projects/rech/qgz/urc37ho/Megatron-DeepSpeed-yaya/megatron/data'
[default0]:>>> done with dataset index builder. Compilation time: 0.065 seconds
[default0]:> compiling and loading fused kernels ...
[default0]:ninja: no work to do.
[default0]:ninja: no work to do.
[default0]:ninja: no work to do.
[default0]:>>> done with compiling and loading fused kernels. Compilation time: 30.261 seconds
[default0]:time to initialize megatron (seconds): 42.662
[default0]:[after megatron is initialized] datetime: 2024-04-21 10:48:06 
[default0]:building GPT model ...
[default0]:[2024-04-21 10:48:06,732] [INFO] [utils.py:791:see_memory_usage] Before Building Model
[default0]:[2024-04-21 10:48:06,732] [INFO] [utils.py:792:see_memory_usage] MA 0.0 GB         Max_MA 2.16 GB         CA 0.0 GB         Max_CA 2 GB 
[default0]:[2024-04-21 10:48:06,732] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 29.2 GB, percent = 5.8%
[default0]:SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
[default0]:Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=0, data=4, model=0): 4, ProcessCoord(pipe=0, data=5, model=0): 5, ProcessCoord(pipe=0, data=6, model=0): 6, ProcessCoord(pipe=0, data=7, model=0): 7, ProcessCoord(pipe=0, data=8, model=0): 8, ProcessCoord(pipe=0, data=9, model=0): 9, ProcessCoord(pipe=0, data=10, model=0): 10, ProcessCoord(pipe=0, data=11, model=0): 11, ProcessCoord(pipe=0, data=12, model=0): 12, ProcessCoord(pipe=0, data=13, model=0): 13, ProcessCoord(pipe=0, data=14, model=0): 14, ProcessCoord(pipe=0, data=15, model=0): 15, ProcessCoord(pipe=0, data=16, model=0): 16, ProcessCoord(pipe=0, data=17, model=0): 17, ProcessCoord(pipe=0, data=18, model=0): 18, ProcessCoord(pipe=0, data=19, model=0): 19, ProcessCoord(pipe=0, data=20, model=0): 20, ProcessCoord(pipe=0, data=21, model=0): 21, ProcessCoord(pipe=0, data=22, model=0): 22, ProcessCoord(pipe=0, data=23, model=0): 23, ProcessCoord(pipe=0, data=24, model=0): 24, ProcessCoord(pipe=0, data=25, model=0): 25, ProcessCoord(pipe=0, data=26, model=0): 26, ProcessCoord(pipe=0, data=27, model=0): 27, ProcessCoord(pipe=0, data=28, model=0): 28, ProcessCoord(pipe=0, data=29, model=0): 29, ProcessCoord(pipe=0, data=30, model=0): 30, ProcessCoord(pipe=0, data=31, model=0): 31, ProcessCoord(pipe=1, data=0, model=0): 32, ProcessCoord(pipe=1, data=1, model=0): 33, ProcessCoord(pipe=1, data=2, model=0): 34, ProcessCoord(pipe=1, data=3, model=0): 35, ProcessCoord(pipe=1, data=4, model=0): 36, ProcessCoord(pipe=1, data=5, model=0): 37, ProcessCoord(pipe=1, data=6, model=0): 38, ProcessCoord(pipe=1, data=7, model=0): 39, ProcessCoord(pipe=1, data=8, model=0): 40, ProcessCoord(pipe=1, data=9, model=0): 41, ProcessCoord(pipe=1, data=10, model=0): 42, ProcessCoord(pipe=1, data=11, model=0): 43, ProcessCoord(pipe=1, data=12, model=0): 44, ProcessCoord(pipe=1, data=13, model=0): 45, ProcessCoord(pipe=1, data=14, model=0): 46, ProcessCoord(pipe=1, data=15, model=0): 47, ProcessCoord(pipe=1, data=16, model=0): 48, ProcessCoord(pipe=1, data=17, model=0): 49, ProcessCoord(pipe=1, data=18, model=0): 50, ProcessCoord(pipe=1, data=19, model=0): 51, ProcessCoord(pipe=1, data=20, model=0): 52, ProcessCoord(pipe=1, data=21, model=0): 53, ProcessCoord(pipe=1, data=22, model=0): 54, ProcessCoord(pipe=1, data=23, model=0): 55, ProcessCoord(pipe=1, data=24, model=0): 56, ProcessCoord(pipe=1, data=25, model=0): 57, ProcessCoord(pipe=1, data=26, model=0): 58, ProcessCoord(pipe=1, data=27, model=0): 59, ProcessCoord(pipe=1, data=28, model=0): 60, ProcessCoord(pipe=1, data=29, model=0): 61, ProcessCoord(pipe=1, data=30, model=0): 62, ProcessCoord(pipe=1, data=31, model=0): 63}
[default0]:[2024-04-21 10:48:06,738] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
[default0]:stage=0 layers=18
[default0]:     0: _to_float16
[default0]:     1: EmbeddingPipe
[default0]:     2: ParallelTransformerLayerPipe
[default0]:     3: ParallelTransformerLayerPipe
[default0]:     4: ParallelTransformerLayerPipe
[default0]:     5: ParallelTransformerLayerPipe
[default0]:     6: ParallelTransformerLayerPipe
[default0]:     7: ParallelTransformerLayerPipe
[default0]:     8: ParallelTransformerLayerPipe
[default0]:     9: ParallelTransformerLayerPipe
[default0]:    10: ParallelTransformerLayerPipe
[default0]:    11: ParallelTransformerLayerPipe
[default0]:    12: ParallelTransformerLayerPipe
[default0]:    13: ParallelTransformerLayerPipe
[default0]:    14: ParallelTransformerLayerPipe
[default0]:    15: ParallelTransformerLayerPipe
[default0]:    16: ParallelTransformerLayerPipe
[default0]:    17: ParallelTransformerLayerPipe
[default0]:stage=1 layers=19
[default0]:    18: ParallelTransformerLayerPipe
[default0]:    19: ParallelTransformerLayerPipe
[default0]:    20: ParallelTransformerLayerPipe
[default0]:    21: ParallelTransformerLayerPipe
[default0]:    22: ParallelTransformerLayerPipe
[default0]:    23: ParallelTransformerLayerPipe
[default0]:    24: ParallelTransformerLayerPipe
[default0]:    25: ParallelTransformerLayerPipe
[default0]:    26: ParallelTransformerLayerPipe
[default0]:    27: ParallelTransformerLayerPipe
[default0]:    28: ParallelTransformerLayerPipe
[default0]:    29: ParallelTransformerLayerPipe
[default0]:    30: ParallelTransformerLayerPipe
[default0]:    31: ParallelTransformerLayerPipe
[default0]:    32: ParallelTransformerLayerPipe
[default0]:    33: ParallelTransformerLayerPipe
[default0]:    34: MixedFusedRMSNorm
[default0]:    35: LMHeadPipe
[default0]:    36: float16_to_fp32
[default0]:  loss: CrossEntropy
[default0]:[2024-04-21 10:48:06,980] [INFO] [utils.py:791:see_memory_usage] After Building Model
[default0]:[2024-04-21 10:48:06,980] [INFO] [utils.py:792:see_memory_usage] MA 6.29 GB         Max_MA 6.31 GB         CA 6.31 GB         Max_CA 6 GB 
[default0]:[2024-04-21 10:48:06,980] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 29.46 GB, percent = 5.9%
[default0]: > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3369205760
[default0]: > total number of parameters in model: 3369205760
[default0]:> learning rate decay style: cosine
[default0]:DeepSpeed is enabled.
[default0]:[2024-04-21 10:48:06,982] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown
[default3]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default5]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default0]:[2024-04-21 10:48:10,487] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[default0]:[2024-04-21 10:48:10,487] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[default0]:[2024-04-21 10:48:10,487] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[default0]:[2024-04-21 10:48:10,489] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[default0]:[2024-04-21 10:48:10,489] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[default0]:[2024-04-21 10:48:10,564] [INFO] [utils.py:791:see_memory_usage] begin bf16_optimizer
[default0]:[2024-04-21 10:48:10,565] [INFO] [utils.py:792:see_memory_usage] MA 6.28 GB         Max_MA 6.29 GB         CA 6.31 GB         Max_CA 6 GB 
[default0]:[2024-04-21 10:48:10,565] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 30.02 GB, percent = 6.0%
[default6]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default4]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default2]:[2024-04-21 10:48:10,562] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default1]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default7]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default0]:[2024-04-21 10:48:10,626] [INFO] [utils.py:791:see_memory_usage] before initializing group 0
[default0]:[2024-04-21 10:48:10,627] [INFO] [utils.py:792:see_memory_usage] MA 6.28 GB         Max_MA 6.28 GB         CA 6.31 GB         Max_CA 6 GB 
[default0]:[2024-04-21 10:48:10,627] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 30.02 GB, percent = 6.0%
[default0]:[2024-04-21 10:48:10,760] [INFO] [utils.py:791:see_memory_usage] after initializing group 0
[default0]:[2024-04-21 10:48:10,761] [INFO] [utils.py:792:see_memory_usage] MA 19.22 GB         Max_MA 19.22 GB         CA 25.53 GB         Max_CA 26 GB 
[default0]:[2024-04-21 10:48:10,761] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 30.1 GB, percent = 6.0%
[default0]:[2024-04-21 10:48:10,848] [INFO] [utils.py:791:see_memory_usage] before initializing group 1
[default0]:[2024-04-21 10:48:10,849] [INFO] [utils.py:792:see_memory_usage] MA 19.22 GB         Max_MA 19.22 GB         CA 25.53 GB         Max_CA 26 GB 
[default0]:[2024-04-21 10:48:10,849] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 30.69 GB, percent = 6.1%
[default0]:[2024-04-21 10:48:10,929] [INFO] [utils.py:791:see_memory_usage] after initializing group 1
[default0]:[2024-04-21 10:48:10,930] [INFO] [utils.py:792:see_memory_usage] MA 19.22 GB         Max_MA 19.22 GB         CA 25.53 GB         Max_CA 26 GB 
[default0]:[2024-04-21 10:48:10,930] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 30.71 GB, percent = 6.1%
[default0]:[2024-04-21 10:48:10,989] [INFO] [utils.py:791:see_memory_usage] before initialize_optimizer
[default0]:[2024-04-21 10:48:10,990] [INFO] [utils.py:792:see_memory_usage] MA 19.22 GB         Max_MA 19.22 GB         CA 25.53 GB         Max_CA 26 GB 
[default0]:[2024-04-21 10:48:10,990] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 30.72 GB, percent = 6.1%
[default0]:[2024-04-21 10:48:11,105] [INFO] [utils.py:791:see_memory_usage] end initialize_optimizer
[default0]:[2024-04-21 10:48:11,105] [INFO] [utils.py:792:see_memory_usage] MA 20.01 GB         Max_MA 20.01 GB         CA 26.31 GB         Max_CA 26 GB 
[default0]:[2024-04-21 10:48:11,105] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 31.39 GB, percent = 6.2%
[default0]:[2024-04-21 10:48:11,175] [INFO] [utils.py:791:see_memory_usage] end bf16_optimizer
[default0]:[2024-04-21 10:48:11,176] [INFO] [utils.py:792:see_memory_usage] MA 20.01 GB         Max_MA 20.01 GB         CA 26.31 GB         Max_CA 26 GB 
[default0]:[2024-04-21 10:48:11,176] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 31.4 GB, percent = 6.2%
[default0]:[2024-04-21 10:48:11,176] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[default0]:[2024-04-21 10:48:11,176] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[default0]:[2024-04-21 10:48:11,176] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x146af09241f0>
[default0]:[2024-04-21 10:48:11,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2024-04-21 10:48:11,176] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   activation_checkpointing_config  {
[default0]:    "partition_activations": false, 
[default0]:    "contiguous_memory_optimization": false, 
[default0]:    "cpu_checkpointing": false, 
[default0]:    "number_checkpoints": null, 
[default0]:    "synchronize_checkpoint_boundary": false, 
[default0]:    "profile": false
[default0]:}
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   amp_enabled .................. False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   amp_params ................... False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   autotuning_config ............ {
[default0]:    "enabled": false, 
[default0]:    "start_step": null, 
[default0]:    "end_step": null, 
[default0]:    "metric_path": null, 
[default0]:    "arg_mappings": null, 
[default0]:    "metric": "throughput", 
[default0]:    "model_info": null, 
[default0]:    "results_dir": "autotuning_results", 
[default0]:    "exps_dir": "autotuning_exps", 
[default0]:    "overwrite": true, 
[default0]:    "fast": true, 
[default0]:    "start_profile_step": 3, 
[default0]:    "end_profile_step": 5, 
[default0]:    "tuner_type": "gridsearch", 
[default0]:    "tuner_early_stopping": 5, 
[default0]:    "tuner_num_trials": 50, 
[default0]:    "model_info_path": null, 
[default0]:    "mp_size": 1, 
[default0]:    "max_train_batch_size": null, 
[default0]:    "min_train_batch_size": 1, 
[default0]:    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[default0]:    "min_train_micro_batch_size_per_gpu": 1, 
[default0]:    "num_tuning_micro_batch_sizes": 3
[default0]:}
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   bfloat16_enabled ............. True
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x146af0925ae0>
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   communication_data_type ...... None
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   disable_allgather ............ False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   dump_state ................... False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [default4]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default4]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default4]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default4]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default4]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:DeepSpeed general environment info:
[default4]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default4]:torch version .................... 2.2.1
[default4]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default4]:deepspeed info ................... 0.12.6, unknown, unknown
[default4]:torch cuda version ............... 12.1
[default4]:torch hip version ................ None
[default4]:nvcc version ..................... 12.1
[default4]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default4]:shared memory (/dev/shm) size .... 251.60 GB
[default4]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default7]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default7]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default7]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:DeepSpeed general environment info:
[default7]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default7]:torch version .................... 2.2.1
[default7]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default7]:deepspeed info ................... 0.12.6, unknown, unknown
[default7]:torch cuda version ............... 12.1
[default7]:torch hip version ................ None
[default7]:nvcc version ..................... 12.1
[default7]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default7]:shared memory (/dev/shm) size .... 251.60 GB
[default7]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default5]:DeepSpeed general environment info:
[default5]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default5]:torch version .................... 2.2.1
[default5]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default5]:deepspeed info ................... 0.12.6, unknown, unknown
[default5]:torch cuda version ............... 12.1
[default5]:torch hip version ................ None
[default5]:nvcc version ..................... 12.1
[default5]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default5]:shared memory (/dev/shm) size .... 251.60 GB
[default5]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default7]:[2024-04-21 10:47:35,630] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-04-21 10:47:35,653] [INFO] [comm.py:637:init_distributed] cdb=None
[default6]:[2024-04-21 10:47:35,625] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-04-21 10:47:35,657] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-04-21 10:47:35,660] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:[2024-04-21 10:47:35,668] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-04-21 10:47:35,668] [INFO] [comm.py:637:init_distributed] cdb=None
[default0]: > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3369209856
[default0]: > total number of parameters in model: 3369209856
[default0]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default4]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default2]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default3]:[2024-04-21 10:48:10,560] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default7]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default1]:[2024-04-21 10:48:10,560] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default5]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default6]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default7]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default7]:deepspeed info ................... 0.12.6, unknown, unknown
[default7]:torch cuda version ............... 12.1
[default7]:torch hip version ................ None
[default7]:nvcc version ..................... 12.1
[default7]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default7]:shared memory (/dev/shm) size .... 251.60 GB
[default7]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default2]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:[2024-04-21 10:47:34,761] [INFO] [comm.py:637:init_distributed] cdb=None
[default6]:[2024-04-21 10:47:35,741] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-04-21 10:47:35,743] [INFO] [comm.py:637:init_distributed] cdb=None
[default7]:[2024-04-21 10:47:35,748] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-04-21 10:47:35,763] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-04-21 10:47:35,741] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:[2024-04-21 10:47:35,738] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-04-21 10:47:35,769] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-04-21 10:48:10,556] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default6]:[2024-04-21 10:48:10,557] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default0]:[2024-04-21 10:48:10,556] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default5]:[2024-04-21 10:48:10,556] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default7]:[2024-04-21 10:48:10,557] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default1]:[2024-04-21 10:48:10,556] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default4]:[2024-04-21 10:48:10,557] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default2]:[2024-04-21 10:48:10,557] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            !                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:[2024-04-21 10:47:34,633] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:DeepSpeed general environment info:
[default5]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default5]:torch version .................... 2.2.1
[default5]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default5]:deepspeed info ................... 0.12.6, unknown, unknown
[default5]:torch cuda version ............... 12.1
[default5]:torch hip version ................ None
[default5]:nvcc version ..................... 12.1
[default5]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default5]:shared memory (/dev/shm) size .... 251.60 GB
[default5]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default1]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default1]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 2.2.1
[default1]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.12.6, unknown, unknown
[default1]:torch cuda version ............... 12.1
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 12.1
[default1]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default1]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default7]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default7]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default7]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default7]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default7]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default7]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default7]:--------------------------------------------------
[default7]:DeepSpeed general environment info:
[default7]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default7]:torch version .................... 2.2.1
[default7]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default7]:deepspeed info ................... 0.12.6, unknown, unknown
[default7]:torch cuda version ............... 12.1
[default7]:torch hip version ................ None
[default7]:nvcc version ..................... 12.1
[default7]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default7]:shared memory (/dev/shm) size .... 251.60 GB
[default7]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default3]:[2024-04-21 10:47:35,647] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:[2024-04-21 10:47:35,651] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-04-21 10:47:35,646] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-04-21 10:47:35,663] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-04-21 10:47:35,649] [INFO] [comm.py:637:init_distributed] cdb=None
[default7]:[2024-04-21 10:47:35,672] [INFO] [comm.py:637:init_distributed] cdb=None
[default6]:[2024-04-21 10:47:35,673] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-04-21 10:48:10,557] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default0]:[2024-04-21 10:48:10,557] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default4]:[2024-04-21 10:48:10,557] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default7]:[2024-04-21 10:48:10,556] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default6]:[2024-04-21 10:48:10,556] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default5]:[2024-04-21 10:48:10,556] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default1]:[2024-04-21 10:48:10,557] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default3]:[2024-04-21 10:48:10,557] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
                                                                                                                                                                                                                                                                                                        [default4]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:--------------------------------------------------
[default4]:DeepSpeed general environment info:
[default4]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default4]:torch version .................... 2.2.1
[default4]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default4]:deepspeed info ................... 0.12.6, unknown, unknown
[default4]:torch cuda version ............... 12.1
[default4]:torch hip version ................ None
[default4]:nvcc version ..................... 12.1
[default4]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default4]:shared memory (/dev/shm) size .... 251.60 GB
[default4]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default0]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 2.2.1
[default0]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.12.6, unknown, unknown
[default0]:torch cuda version ............... 12.1
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 12.1
[default0]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default0]:shared memory (/dev/shm) size .... 251.60 GB
[default0]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default1]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default1]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 2.2.1
[default1]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.12.6, unknown, unknown
[default1]:torch cuda version ............... 12.1
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 12.1
[default1]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default1]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:[2024-04-21 10:47:34,581] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-04-21 10:47:35,610] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-04-21 10:47:35,600] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-04-21 10:47:35,610] [INFO] [comm.py:637:init_distributed] cdb=None
[default7]:[2024-04-21 10:47:35,603] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-04-21 10:47:35,582] [INFO] [comm.py:637:init_distributed] cdb=None
[default6]:[2024-04-21 10:47:35,599] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:[2024-04-21 10:47:35,611] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default6]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default3]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default4]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default7]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default0]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default1]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default5]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default4]:torch version .................... 2.2.1
[default4]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default4]:deepspeed info ................... 0.12.6, unknown, unknown
[default4]:torch cuda version ............... 12.1
[default4]:torch hip version ................ None
[default4]:nvcc version ..................... 12.1
[default4]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default4]:shared memory (/dev/shm) size .... 251.60 GB
[default4]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:DeepSpeed general environment info:
[default5]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default5]:torch version .................... 2.2.1
[default5]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default5]:deepspeed info ................... 0.12.6, unknown, unknown
[default5]:torch cuda version ............... 12.1
[default5]:torch hip version ................ None
[default5]:nvcc version ..................... 12.1
[default5]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default5]:shared memory (/dev/shm) size .... 251.60 GB
[default5]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default0]:[2024-04-21 10:47:34,395] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-04-21 10:47:35,387] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-04-21 10:47:35,388] [INFO] [comm.py:637:init_distributed] cdb=None
[default7]:[2024-04-21 10:47:35,387] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:[2024-04-21 10:47:35,395] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-04-21 10:47:35,394] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-04-21 10:47:35,388] [INFO] [comm.py:637:init_distributed] cdb=None
[default6]:[2024-04-21 10:47:35,367] [INFO] [comm.py:637:init_distributed] cdb=None
[default0]:[2024-04-21 10:48:10,559] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default1]:[2024-04-21 10:48:10,563] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default3]:[2024-04-21 10:48:10,560] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default6]:[2024-04-21 10:48:10,560] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default5]:[2024-04-21 10:48:10,561] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default2]:[2024-04-21 10:48:10,559] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default4]:[2024-04-21 10:48:10,562] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default7]:[2024-04-21 10:48:10,560] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
4]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default4]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default4]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default4]:[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default4]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
[default4]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default4]:fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adagrad ............ [93 [NO][0a     . [92m[OKAY][0m
[default3]:cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WA NING]      se specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[default3]:evoformer_attn ......... [93m[NO][0m ....... [93m[NO] 0m
[defA     used_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default7]:DeepSpeed general environment info:
[default7]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default7]:torch version .................... 2.2.1
[default7]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default7]:deepspeed info ................... 0.12.6, unknown, unknown
[default7]:torch cuda version ............... 12.1
[default7]:torch hip version ................ None
[default7]:nvcc version ..................... 12.1
[default7]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default7]:shared memory (/dev/shm) size .... 251.60 GB
[default7]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default1]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default1]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 2.2.1
[default1]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.12.6, unknown, unknown
[default1]:torch cuda version ............... 12.1
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 12.1
[default1]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default1]:shared memory (/dev/shm) size .... 251.60 GB
[default1]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default2]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default2]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 2.2.1
[default2]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.12.6, unknown, unknown
[default2]:torch cuda version ............... 12.1
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 12.1
[default2]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default2]:shared memory (/dev/shm) size .... 251.60 GB
[default2]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default5]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default5]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default5]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default5]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default5]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default5]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default5]:--------------------------------------------------
[default5]:DeepSpeed general environment info:
[default5]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default5]:torch version .................... 2.2.1
[default5]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default5]:deepspeed info ................... 0.12.6, unknown, unknown
[default5]:torch cuda version ............... 12.1
[default5]:torch hip version ................ None
[default5]:nvcc version ..................... 12.1
[default5]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default5]:shared memory (/dev/shm) size .... 251.60 GB
[default5]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default4]:[2024-04-21 10:47:35,628] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-04-21 10:47:35,631] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-04-21 10:47:35,626] [INFO] [comm.py:637:init_distributed] cdb=None
[default6]:[2024-04-21 10:47:35,632] [INFO] [comm.py:637:init_distributed] cdb=None
[default7]:[2024-04-21 10:47:35,627] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-04-21 10:47:35,631] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-04-21 10:47:35,635] [INFO] [comm.py:637:init_distributed] cdb=None
[default0]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default4]:[2024-04-21 10:48:10,583] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default5]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default1]:[2024-04-21 10:48:10,583] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default6]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default7]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default2]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default3]:[2024-04-21 10:48:10,584] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default5]:torch version .................... 2.2.1
[default5]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default5]:deepspeed info ................... 0.12.6, unknown, unknown
[default5]:torch cuda version ............... 12.1
[default5]:torch hip version ................ None
[default5]:nvcc version ..................... 12.1
[default5]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default5]:shared memory (/dev/shm) size .... 251.60 GB
[default5]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default6]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default6]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default6]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default6]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default6]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default6]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default6]:--------------------------------------------------
[default6]:DeepSpeed general environment info:
[default6]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default6]:torch version .................... 2.2.1
[default6]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default6]:deepspeed info ................... 0.12.6, unknown, unknown
[default6]:torch cuda version ............... 12.1
[default6]:torch hip version ................ None
[default6]:nvcc version ..................... 12.1
[default6]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default6]:shared memory (/dev/shm) size .... 251.60 GB
[default6]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default3]:inference_core_ops ..... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cutlass_ops ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_device_ops ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:ragged_ops ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[default3]:[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 2.2.1
[default3]:deepspeed install path ........... ['/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.12.6, unknown, unknown
[default3]:torch cuda version ............... 12.1
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 12.1
[default3]:deepspeed wheel compiled w. ...... torch 2.2, cuda 12.1
[default3]:shared memory (/dev/shm) size .... 251.60 GB
[default3]:**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[default6]:[2024-04-21 10:47:35,817] [INFO] [comm.py:637:init_distributed] cdb=None
[default4]:[2024-04-21 10:47:35,851] [INFO] [comm.py:637:init_distributed] cdb=None
[default1]:[2024-04-21 10:47:35,851] [INFO] [comm.py:637:init_distributed] cdb=None
[default2]:[2024-04-21 10:47:35,849] [INFO] [comm.py:637:init_distributed] cdb=None
[default7]:[2024-04-21 10:47:35,851] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-04-21 10:47:35,843] [INFO] [comm.py:637:init_distributed] cdb=None
[default3]:[2024-04-21 10:47:35,855] [INFO] [comm.py:637:init_distributed] cdb=None
[default5]:[2024-04-21 10:48:10,558] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default3]:[2024-04-21 10:48:10,558] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default1]:[2024-04-21 10:48:10,558] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default4]:[2024-04-21 10:48:10,558] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default2]:[2024-04-21 10:48:10,558] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default7]:[2024-04-21 10:48:10,558] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default0]:[2024-04-21 10:48:10,558] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default6]:[2024-04-21 10:48:10,558] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   flops_profiler_config ........ {
[default0]:    "enabled": false, 
[default0]:    "recompute_fwd_factor": 0.0, 
[default0]:    "profile_step": 1, 
[default0]:    "module_depth": -1, 
[default0]:    "top_modules": 1, 
[default0]:    "detailed": true, 
[default0]:    "output_file": null
[default0]:}
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   fp16_enabled ................. False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   global_rank .................. 0
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[default0]:[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 2
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   gradient_clipping ............ 0.0
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   graph_harvesting ............. False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 1
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   loss_scale ................... 1.0
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   memory_breakdown ............. False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   nebula_config ................ {
[default0]:    "enabled": false, 
[default0]:    "persistent_storage_path": null, 
[default0]:    "persistent_time_interval": 100, 
[default0]:    "num_of_version_in_retention": 2, 
[default0]:    "enable_nebula_load": true, 
[default0]:    "load_path": null
[default0]:}
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   optimizer_name ............... None
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   optimizer_params ............. None
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   pld_enabled .................. False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   pld_params ................... False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   prescale_gradients ........... False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   scheduler_name ............... None
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   scheduler_params ............. None
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   sparse_attention ............. None
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   steps_per_print .............. 1
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   train_batch_size ............. 192
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  3
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   weight_quantization_config ... None
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   world_size ................... 32
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   zero_enabled ................. False
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[default0]:[2024-04-21 10:48:11,178] [INFO] [config.py:974:print_user_config]   json = {
[default0]:    "train_batch_size": 192, 
[default0]:    "train_micro_batch_size_per_gpu": 3, 
[default0]:    "steps_per_print": 1, 
[default0]:    "zero_optimization": {
[default0]:        "stage": 0
[default0]:    }, 
[default0]:    "bf16": {
[default0]:        "enabled": true
[default0]:    }
[default0]:}
[default0]:[2024-04-21 10:48:11,178] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=2 micro_batch_size=3
[default0]:[2024-04-21 10:48:11,178] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[default0]:[2024-04-21 10:48:11,251] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=3369205760 (3369.206M) TOTAL_PARAMS=6738415616 (6738.416M) UNIQUE_PARAMS=6738415616 (6738.416M)
[default3]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default0]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default0]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default2]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default2]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default1]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default1]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default6]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default7]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default7]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default3]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default3]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default5]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default5]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default4]:/linkhome/rech/genlor01/urc37ho/.conda/envs/lucie-torch211/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847130/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
[default4]:  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[default6]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:WARNING: could not find the metadata file /gpfswork/rech/qgz/urc37ho/checkpoints12/ 
[default0]:    will not load any checkpoints and will start from random
[default1]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default4]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[after model, optimizer, and learning rate scheduler are built] datetime: 2024-04-21 10:48:11 
[default0]:> building train, validation, and test datasets ...
[default0]: > datasets target sizes (minimum size):
[default0]:    train:      48000000
[default0]:    validation: 480192
[default0]:    test:       192
[default0]:> building train, validation, and test datasets for GPT ...
[default0]:Single data path provided for train, valid & test
[default0]: > building dataset index ...
[default0]:    reading sizes...
[default0]:    reading pointers...
[default0]:    reading document index...
[default0]:    creating numpy buffer of mmap...
[default0]:    creating memory view of numpy buffer...
[default0]: > finished creating indexed dataset in 0.018799 seconds
[default0]:    number of documents: 32615
[default0]: > dataset split:
[default0]:    train:
[default0]:     document indices in [0, 31604) total of 31604 documents
[default0]:    validation:
[default0]:     document indices in [31604, 32582) total of 978 documents
[default0]:    test:
[default0]:     document indices in [32582, 32615) total of 33 documents
[default0]: > loading doc-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/673ca9566fd0a0592efcd055282dcca9_doc_idx.npy
[default0]: > loading sample-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/673ca9566fd0a0592efcd055282dcca9_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/673ca9566fd0a0592efcd055282dcca9_shuffle_idx.npy
[default0]:    loaded indexed file in 0.141 seconds
[default0]:    total number of samples: 48000143
[default0]:    total number of epochs: 5351
[default0]: > loading doc-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/db2568a3328e07310385991066958c1e_doc_idx.npy
[default0]: > loading sample-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/db2568a3328e07310385991066958c1e_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/db2568a3328e07310385991066958c1e_shuffle_idx.npy
[default0]:    loaded indexed file in 0.108 seconds
[default0]:    total number of samples: 480446
[default0]:    total number of epochs: 1829
[default0]: > loading doc-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/30956b97538cab6aea2f744cd89c55de_doc_idx.npy
[default0]: > loading sample-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/30956b97538cab6aea2f744cd89c55de_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /linkhome/rech/genlor01/urc37ho/.cache/30956b97538cab6aea2f744cd89c55de_shuffle_idx.npy
[default0]:    loaded indexed file in 0.002 seconds
[default0]:    total number of samples: 194
[default0]:    total number of epochs: 27
[default0]:> finished creating GPT datasets ...
[default0]:[after dataloaders are built] datetime: 2024-04-21 10:48:17 
[default0]:done with setup ...
[default0]:training ...
[default0]:[before the start of training step] datetime: 2024-04-21 10:48:17 
[default0]:[2024-04-21 10:48:23,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[1.5e-07, 1.5e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 1 loss: 11.1955 iter time (s): 7.165 samples/sec: 26.797
[default0]:[Rank 0] (after 1 iterations) memory (MB) | allocated: 26988.48828125 | max allocated: 52993.8623046875 | reserved: 60830.0 | max reserved: 60830.0
[default0]:[2024-04-21 10:48:29,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[3e-07, 3e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 2 loss: 11.2012 iter time (s): 5.618 samples/sec: 34.173
[default0]:[2024-04-21 10:48:35,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[4.4999999999999993e-07, 4.4999999999999993e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 3 loss: 11.1974 iter time (s): 5.583 samples/sec: 34.392
[default0]:[2024-04-21 10:48:41,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[6e-07, 6e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 4 loss: 11.1934 iter time (s): 5.559 samples/sec: 34.537
[default0]:[2024-04-21 10:48:46,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[7.499999999999999e-07, 7.499999999999999e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 5 loss: 11.1862 iter time (s): 5.554 samples/sec: 34.568
[default0]:[2024-04-21 10:48:52,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[8.999999999999999e-07, 8.999999999999999e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 6 loss: 11.1229 iter time (s): 5.614 samples/sec: 34.198
[default0]:[2024-04-21 10:48:57,857] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[1.05e-06, 1.05e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 7 loss: 11.0022 iter time (s): 5.578 samples/sec: 34.420
[default0]:[2024-04-21 10:49:03,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[1.2e-06, 1.2e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 8 loss: 10.9500 iter time (s): 5.562 samples/sec: 34.518
[default0]:[2024-04-21 10:49:09,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[1.35e-06, 1.35e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 9 loss: 10.6245 iter time (s): 5.644 samples/sec: 34.019
[default0]:[2024-04-21 10:49:14,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.4999999999999998e-06, 1.4999999999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 10 loss: 10.3296 iter time (s): 5.567 samples/sec: 34.492
[default0]:[2024-04-21 10:49:20,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=11, skipped=0, lr=[1.6499999999999999e-06, 1.6499999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 11 loss: 10.2097 iter time (s): 5.633 samples/sec: 34.082
[default0]:[2024-04-21 10:49:25,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=12, skipped=0, lr=[1.7999999999999997e-06, 1.7999999999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 "recompute_fwd_factor": 0.0, 
    "p ofile_sW      
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-21 10:48:11,177] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          q                         [default1]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default5]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default4]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default6]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:(min, max) time across ranks (ms):
[default7]:    load-checkpoint ................................: (1.40, 1.56)
[default7]:(min, max) time across ranks (ms):
[default7]:    model-and-optimizer-setup ......................: (4842.15, 4851.08)
[default7]:    train/valid/test-data-iterators-setup ..........: (3730.91, 5522.23)
[default7]: iteration        1/  250000 | consumed samples:          192 | consumed tokens:       393216 | elapsed time per iteration (ms): 7218.6 | learning rate: 1.500E-07 | global batch size:   192 | lm loss: 1.119546E+01 | grad norm: 16.735 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.598 | TFLOPs: 36.14 |
[default7]: iteration        2/  250000 | consumed samples:          384 | consumed tokens:       786432 | elapsed time per iteration (ms): 5626.1 | learning rate: 3.000E-07 | global batch size:   192 | lm loss: 1.120121E+01 | grad norm: 20.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.127 | TFLOPs: 46.38 |
[default7]: iteration        3/  250000 | consumed samples:          576 | consumed tokens:      1179648 | elapsed time per iteration (ms): 5587.5 | learning rate: 4.500E-07 | global batch size:   192 | lm loss: 1.119739E+01 | grad norm: 19.121 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.363 | TFLOPs: 46.70 |
[default7]: iteration        4/  250000 | consumed samples:          768 | consumed tokens:      1572864 | elapsed time per iteration (ms): 5583.3 | learning rate: 6.000E-07 | global batch size:   192 | lm loss: 1.119344E+01 | grad norm: 20.862 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.388 | TFLOPs: 46.73 |
[default7]: iteration        5/  250000 | consumed samples:          960 | consumed tokens:      1966080 | elapsed time per iteration (ms): 5562.7 | learning rate: 7.500E-07 | global batch size:   192 | lm loss: 1.118623E+01 | grad norm: 21.151 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.515 | TFLOPs: 46.90 |
[default7]: iteration        6/  250000 | consumed samples:         1152 | consumed tokens:      2359296 | elapsed time per iteration (ms): 5623.5 | learning rate: 9.000E-07 | global batch size:   192 | lm loss: 1.112291E+01 | grad norm: 21.483 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.143 | TFLOPs: 46.40 |
[default7]: iteration        7/  250000 | consumed samples:         1344 | consumed tokens:      2752512 | elapsed time per iteration (ms): 5592.2 | learning rate: 1.050E-06 | global batch size:   192 | lm loss: 1.100224E+01 | grad norm: 18.999 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.334 | TFLOPs: 46.66 |
[default7]: iteration        8/  250000 | consumed samples:         1536 | consumed tokens:      3145728 | elapsed time per iteration (ms): 5575.0 | learning rate: 1.200E-06 | global batch size:   192 | lm loss: 1.094995E+01 | grad norm: 16.994 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.440 | TFLOPs: 46.80 |
[default7]: iteration        9/  250000 | consumed samples:         1728 | consumed tokens:      3538944 | elapsed time per iteration (ms): 5651.2 | learning rate: 1.350E-06 | global batch size:   192 | lm loss: 1.062449E+01 | grad norm: 18.039 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.975 | TFLOPs: 46.17 |
[default7]: iteration       10/  250000 | consumed samples:         1920 | consumed tokens:      3932160 | elapsed time per iteration (ms): 5580.4 | learning rate: 1.500E-06 | global batch size:   192 | lm loss: 1.032963E+01 | grad norm: 17.757 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.406 | TFLOPs: 46.75 |
[default7]: iteration       11/  250000 | consumed samples:         2112 | consumed tokens:      4325376 | elapsed time per iteration (ms): 5640.0 | learning rate: 1.650E-06 | global batch size:   192 | lm loss: 1.020966E+01 | grad norm: 26.329 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.042 | TFLOPs: 46.26 |
[default7]: iteration       12/  250000 | consumed samples:         2304 | consumed tokens:      4718592 | elapsed time per iteration (ms): 5593.5 | learning rate: 1.800E-06 | global batch size:   192 | lm loss: 9.904139E+00 | grad norm: 30.680 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.325 | TFLOPs: 46.64 |
[default7]: iteration       13/  250000 | consumed samples:         2496 | consumed tokens:      5111808 | elapsed time per iteration (ms): 5682.1 | learning rate: 1.950E-06 | global batch size:   192 | lm loss: 9.676219E+00 | grad norm: 22.254 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.790 | TFLOPs: 45.92 |
                                                                                                                                                                                                                                                                                             [default7]: iteration       14/  250000 | consumed samples:         2688 | consumed tokens:      5505024 | elapsed time per iteration (ms): 5611.2 | learning rate: 2.100E-06 | global batch size:   192 | lm loss: 9.328545E+00 | grad norm: 20.062 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.217 | TFLOPs: 46.50 |
[default7]: iteration       15/  250000 | consumed samples:         2880 | consumed tokens:      5898240 | elapsed time per iteration (ms): 5563.6 | learning rate: 2.250E-06 | global batch size:   192 | lm loss: 9.073113E+00 | grad norm: 26.253 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.510 | TFLOPs: 46.90 |
[default7]: iteration       16/  250000 | consumed samples:         3072 | consumed tokens:      6291456 | elapsed time per iteration (ms): 5611.4 | learning rate: 2.400E-06 | global batch size:   192 | lm loss: 8.988796E+00 | grad norm: 24.782 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.216 | TFLOPs: 46.50 |
[default7]: iteration       17/  250000 | consumed samples:         3264 | consumed tokens:      6684672 | elapsed time per iteration (ms): 5578.2 | learning rate: 2.550E-06 | global batch size:   192 | lm loss: 8.670739E+00 | grad norm: 18.759 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.420 | TFLOPs: 46.77 |
[default7]: iteration       18/  250000 | consumed samples:         3456 | consumed tokens:      7077888 | elapsed time per iteration (ms): 5574.4 | learning rate: 2.700E-06 | global batch size:   192 | lm loss: 8.705063E+00 | grad norm: 19.849 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.443 | TFLOPs: 46.80 |
[default7]: iteration       19/  250000 | consumed samples:         3648 | consumed tokens:      7471104 | elapsed time per iteration (ms): 5606.7 | learning rate: 2.850E-06 | global batch size:   192 | lm loss: 8.546761E+00 | grad norm: 18.568 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.245 | TFLOPs: 46.54 |
[default7]: iteration       20/  250000 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 5553.8 | learning rate: 3.000E-06 | global batch size:   192 | lm loss: 8.356374E+00 | grad norm: 19.767 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.571 | TFLOPs: 46.98 |
[default7]: iteration       21/  250000 | consumed samples:         4032 | consumed tokens:      8257536 | elapsed time per iteration (ms): 5563.1 | learning rate: 3.150E-06 | global batch size:   192 | lm loss: 8.245579E+00 | grad norm: 17.279 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.513 | TFLOPs: 46.90 |
[default7]: iteration       22/  250000 | consumed samples:         4224 | consumed tokens:      8650752 | elapsed time per iteration (ms): 5619.5 | learning rate: 3.300E-06 | global batch size:   192 | lm loss: 8.251591E+00 | grad norm: 16.101 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.167 | TFLOPs: 46.43 |
[default7]: iteration       23/  250000 | consumed samples:         4416 | consumed tokens:      9043968 | elapsed time per iteration (ms): 5583.7 | learning rate: 3.450E-06 | global batch size:   192 | lm loss: 8.203691E+00 | grad norm: 22.949 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.386 | TFLOPs: 46.73 |
[default7]: iteration       24/  250000 | consumed samples:         4608 | consumed tokens:      9437184 | elapsed time per iteration (ms): 5538.7 | learning rate: 3.600E-06 | global batch size:   192 | lm loss: 8.008379E+00 | grad norm: 14.967 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.665 | TFLOPs: 47.11 |
[default7]: iteration       25/  250000 | consumed samples:         4800 | consumed tokens:      9830400 | elapsed time per iteration (ms): 5602.3 | learning rate: 3.750E-06 | global batch size:   192 | lm loss: 7.885547E+00 | grad norm: 14.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.272 | TFLOPs: 46.57 |
[default7]: iteration       26/  250000 | consumed samples:         4992 | consumed tokens:     10223616 | elapsed time per iteration (ms): 5568.5 | learning rate: 3.900E-06 | global batch size:   192 | lm loss: 7.896993E+00 | grad norm: 15.350 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.480 | TFLOPs: 46.85 |
[default7]: iteration       27/  250000 | consumed samples:         5184 | consumed tokens:     10616832 | elapsed time per iteration (ms): 5556.6 | learning rate: 4.050E-06 | global batch size:   192 | lm loss: 7.776515E+00 | grad norm: 10.027 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.554 | TFLOPs: 46.96 |
[default7]: iteration       28/  250000 | consumed samples:         5376 | consumed tokens:     11010048 | elapsed time per iteration (ms): 5559.9 | learning rate: 4.200E-06 | global batch size:   192 | lm loss: 7.905875E+00 | grad norm: 12.901 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.533 | TFLOPs: 46.93 |
[default7]: iteration       29/  250000 | consumed samples:         5568 | consumed tokens:     11403264 | elapsed time per iteration (ms): 5548.8 | learning rate: 4.350E-06 | global batch size:   192 | lm loss: 7.697672E+00 | grad norm: 9.433 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.602 | TFLOPs: 47.02 |
[default7]: iteration       30/  250000 | consumed samples:         5760 | consumed tokens:     11796480 | elapsed time per iteration (ms): 5579.7 | learning rate: 4.500E-06 | global batch size:   192 | lm loss: 7.709596E+00 | grad norm: 9.487 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.411 | TFLOPs: 46.76 |
[default7]: iteration       31/  250000 | consumed samples:         5952 | consumed tokens:     12189696 | elapsed time per iteration (ms): 5581.9 | learning rate: 4.650E-06 | global batch size:   192 | lm loss: 7.646453E+00 | grad norm: 8.964 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.397 | TFLOPs: 46.74 |
[default7]: iteration       32/  250000 | consumed samples:         6144 | consumed tokens:     12582912 | elapsed time per iteration (ms): 5585.8 | learning rate: 4.800E-06 | global batch size:   192 | lm loss: 7.634462E+00 | grad norm: 6.828 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.373 | TFLOPs: 46.71 |
[default7]: iteration       33/  250000 | consumed samples:         6336 | consumed tokens:     12976128 | elapsed time per iteration (ms): 5592.3 | learning rate: 4.950E-06 | global batch size:   192 | lm loss: 7.244685E+00 | grad norm: 8.791 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.333 | TFLOPs: 46.65 |
[2024-04-21 10:48:11,177] [INFO] [config.py[default0]:steps: 12 loss: 9.9041 iter time (s): 5.587 samples/sec: 34.367
[default0]:[2024-04-21 10:49:31,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=13, skipped=0, lr=[1.95e-06, 1.95e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 13 loss: 9.6762 iter time (s): 5.670 samples/sec: 33.863
[default0]:[2024-04-21 10:49:37,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=14, skipped=0, lr=[2.1e-06, 2.1e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 14 loss: 9.3285 iter time (s): 5.606 samples/sec: 34.248
[default0]:[2024-04-21 10:49:42,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=15, skipped=0, lr=[2.2499999999999996e-06, 2.2499999999999996e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 15 loss: 9.0731 iter time (s): 5.557 samples/sec: 34.554
[default0]:[2024-04-21 10:49:48,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=16, skipped=0, lr=[2.4e-06, 2.4e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 16 loss: 8.9888 iter time (s): 5.601 samples/sec: 34.277
[default0]:[2024-04-21 10:49:53,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=17, skipped=0, lr=[2.5499999999999997e-06, 2.5499999999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 17 loss: 8.6707 iter time (s): 5.569 samples/sec: 34.475
[default0]:[2024-04-21 10:49:59,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=18, skipped=0, lr=[2.7e-06, 2.7e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 18 loss: 8.7051 iter time (s): 5.568 samples/sec: 34.486
[default0]:[2024-04-21 10:50:05,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=19, skipped=0, lr=[2.8499999999999994e-06, 2.8499999999999994e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 19 loss: 8.5468 iter time (s): 5.586 samples/sec: 34.374
[default0]:[2024-04-21 10:50:10,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[2.9999999999999997e-06, 2.9999999999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 20 loss: 8.3564 iter time (s): 5.547 samples/sec: 34.614
[default0]:[2024-04-21 10:50:16,244] [INFO] [logging.py:96:log_dist] [Rank 0] step=21, skipped=0, lr=[3.1499999999999995e-06, 3.1499999999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 21 loss: 8.2456 iter time (s): 5.559 samples/sec: 34.540
[default0]:[2024-04-21 10:50:21,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=22, skipped=0, lr=[3.2999999999999997e-06, 3.2999999999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 22 loss: 8.2516 iter time (s): 5.615 samples/sec: 34.193
[default0]:[2024-04-21 10:50:27,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=23, skipped=0, lr=[3.45e-06, 3.45e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 23 loss: 8.2037 iter time (s): 5.579 samples/sec: 34.413
[default0]:[2024-04-21 10:50:33,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=24, skipped=0, lr=[3.5999999999999994e-06, 3.5999999999999994e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 24 loss: 8.0084 iter time (s): 5.534 samples/sec: 34.695
[default0]:[2024-04-21 10:50:38,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=25, skipped=0, lr=[3.7499999999999997e-06, 3.7499999999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 25 loss: 7.8855 iter time (s): 5.597 samples/sec: 34.305
[default0]:[2024-04-21 10:50:44,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=26, skipped=0, lr=[3.9e-06, 3.9e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 26 loss: 7.8970 iter time (s): 5.560 samples/sec: 34.530
[default0]:[2024-04-21 10:50:49,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=27, skipped=0, lr=[4.05e-06, 4.05e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 27 loss: 7.7765 iter time (s): 5.551 samples/sec: 34.586
[default0]:[2024-04-21 10:50:55,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=28, skipped=0, lr=[4.2e-06, 4.2e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 28 loss: 7.9059 iter time (s): 5.549 samples/sec: 34.600
[default0]:[2024-04-21 10:51:00,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=29, skipped=0, lr=[4.35e-06, 4.35e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 29 loss: 7.6977 iter time (s): 5.543 samples/sec: 34.640
[default0]:[2024-04-21 10:51:06,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[4.499999999999999e-06, 4.499999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 30 loss: 7.7096 iter time (s): 5.574 samples/sec: 34.444
[default0]:[2024-04-21 10:51:11,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=31, skipped=0, lr=[4.6499999999999995e-06, 4.6499999999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 31 loss: 7.6465 iter time (s): 5.577 samples/sec: 34.429
[default0]:[2024-04-21 10:51:17,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=32, skipped=0, lr=[4.8e-06, 4.8e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 32 loss: 7.6345 iter time (s): 5.579 samples/sec: 34.413
[default0]:[2024-04-21 10:51:23,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=33, skipped=0, lr=[4.949999999999999e-06, 4.949999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 33 loss: 7.2447 iter time (s): 5.579 samples/sec: 34.412
[default0]:[2024-04-21 10:51:28,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=34, skipped=0, lr=[5.0999999999999995e-06, 5.0999999999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 34 loss: 7.4733 iter time (s): 5.536 samples/sec: 34.685
[default0]:[2024-04-21 10:51:34,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=35, skipped=0, lr=[5.25e-06, 5.25e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 35 loss: 7.3870 iter time (s): 5.558 samples/sec: 34.546
[default0]:[2024-04-21 10:51:39,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=36, skipped=0, lr=[5.4e-06, 5.4e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 36 loss: 7.4186 iter time (s): 5.559 samples/sec: 34.538
[default0]:[2024-04-21 10:51:45,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=37, skipped=0, lr=[5.549999999999999e-06, 5.549999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 37 loss: 7.1891 iter time (s): 5.553 samples/sec: 34.577
[default0]:[2024-04-21 10:51:50,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=38, skipped=0, lr=[5.699999999999999e-06, 5.699999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 38 loss: 7.2606 iter time (s): 5.563 samples/sec: 34.512
[default0]:[2024-04-21 10:51:56,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=39, skipped=0, lr=[5.85e-06, 5.85e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 39 loss: 7.1499 iter time (s): 5.571 samples/sec: 34.467
[default0]:[2024-04-21 10:52:02,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[5.999999999999999e-06, 5.999999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 40 loss: 7.3412 iter time (s): 5.532 samples/sec: 34.709
[default0]:[2024-04-21 10:52:07,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=41, skipped=0, lr=[6.1499999999999996e-06, 6.1499999999999996e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 41 loss: 7.1480 iter time (s): 5.534 samples/sec: 34.692
[default0]:[2024-04-21 10:52:13,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=42, skipped=0, lr=[6.299999999999999e-06, 6.299999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 42 loss: 7.2202 iter time (s): 5.542 samples/sec: 34.643
[default0]:[2024-04-21 10:52:18,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=43, skipped=0, lr=[6.45e-06, 6.45e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 43 loss: 7.1534 iter time (s): 5.564 samples/sec: 34.510
[default0]:[2024-04-21 10:52:24,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=44, skipped=0, lr=[6.5999999999999995e-06, 6.5999999999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 44 loss: 7.2303 iter time (s): 5.529 samples/sec: 34.726
[default0]:[2024-04-21 10:52:29,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=45, skipped=0, lr=[6.749999999999999e-06, 6.749999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 45 loss: 7.1319 iter time (s): 5.602 samples/sec: 34.273
[default7]: iteration       34/  250000 | consumed samples:         6528 | consumed tokens:     13369344 | elapsed time per iteration (ms): 5542.4 | learning rate: 5.100E-06 | global batch size:   192 | lm loss: 7.473324E+00 | grad norm: 6.527 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.642 | TFLOPs: 47.08 |
[default7]: iteration       35/  250000 | consumed samples:         6720 | consumed tokens:     13762560 | elapsed time per iteration (ms): 5563.8 | learning rate: 5.250E-06 | global batch size:   192 | lm loss: 7.386988E+00 | grad norm: 8.500 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.509 | TFLOPs: 46.89 |
[default7]: iteration       36/  250000 | consumed samples:         6912 | consumed tokens:     14155776 | elapsed time per iteration (ms): 5565.8 | learning rate: 5.400E-06 | global batch size:   192 | lm loss: 7.418590E+00 | grad norm: 6.518 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.497 | TFLOPs: 46.88 |
[default7]: iteration       37/  250000 | consumed samples:         7104 | consumed tokens:     14548992 | elapsed time per iteration (ms): 5564.3 | learning rate: 5.550E-06 | global batch size:   192 | lm loss: 7.189134E+00 | grad norm: 6.153 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.505 | TFLOPs: 46.89 |
[default7]: iteration       38/  250000 | consumed samples:         7296 | consumed tokens:     14942208 | elapsed time per iteration (ms): 5567.7 | learning rate: 5.700E-06 | global batch size:   192 | lm loss: 7.260588E+00 | grad norm: 6.305 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.485 | TFLOPs: 46.86 |
[default7]: iteration       39/  250000 | consumed samples:         7488 | consumed tokens:     15335424 | elapsed time per iteration (ms): 5584.5 | learning rate: 5.850E-06 | global batch size:   192 | lm loss: 7.149909E+00 | grad norm: 4.923 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.381 | TFLOPs: 46.72 |
[default7]: iteration       40/  250000 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 5536.0 | learning rate: 6.000E-06 | global batch size:   192 | lm loss: 7.341202E+00 | grad norm: 6.616 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.682 | TFLOPs: 47.13 |
[default7]: iteration       41/  250000 | consumed samples:         7872 | consumed tokens:     16121856 | elapsed time per iteration (ms): 5539.0 | learning rate: 6.150E-06 | global batch size:   192 | lm loss: 7.148006E+00 | grad norm: 5.885 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.663 | TFLOPs: 47.10 |
[default7]: iteration       42/  250000 | consumed samples:         8064 | consumed tokens:     16515072 | elapsed time per iteration (ms): 5552.4 | learning rate: 6.300E-06 | global batch size:   192 | lm loss: 7.220218E+00 | grad norm: 5.700 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.580 | TFLOPs: 46.99 |
[default7]: iteration       43/  250000 | consumed samples:         8256 | consumed tokens:     16908288 | elapsed time per iteration (ms): 5568.0 | learning rate: 6.450E-06 | global batch size:   192 | lm loss: 7.153432E+00 | grad norm: 7.696 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.483 | TFLOPs: 46.86 |
[default7]: iteration       44/  250000 | consumed samples:         8448 | consumed tokens:     17301504 | elapsed time per iteration (ms): 5533.6 | learning rate: 6.600E-06 | global batch size:   192 | lm loss: 7.230340E+00 | grad norm: 5.996 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.697 | TFLOPs: 47.15 |
[default7]: iteration       45/  250000 | consumed samples:         8640 | consumed tokens:     17694720 | elapsed time per iteration (ms): 5606.6 | learning rate: 6.750E-06 | global batch size:   192 | lm loss: 7.131898E+00 | grad norm: 7.105 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.245 | TFLOPs: 46.54 |
[default7]: iteration       46/  250000 | consumed samples:         8832 | consumed tokens:     18087936 | elapsed time per iteration (ms): 5593.3 | learning rate: 6.900E-06 | global batch size:   192 | lm loss: 7.059325E+00 | grad norm: 6.354 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.327 | TFLOPs: 46.65 |
[default7]: iteration       47/  250000 | consumed samples:         9024 | consumed tokens:     18481152 | elapsed time per iteration (ms): 5637.4 | learning rate: 7.050E-06 | global batch size:   192 | lm loss: 7.021202E+00 | grad norm: 8.349 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.058 | TFLOPs: 46.28 |
[default7]: iteration       48/  250000 | consumed samples:         9216 | consumed tokens:     18874368 | elapsed time per iteration (ms): 5549.2 | learning rate: 7.200E-06 | global batch size:   192 | lm loss: 6.885613E+00 | grad norm: 8.454 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.600 | TFLOPs: 47.02 |
[default7]: iteration       49/  250000 | consumed samples:         9408 | consumed tokens:     19267584 | elapsed time per iteration (ms): 5614.4 | learning rate: 7.350E-06 | global batch size:   192 | lm loss: 7.042629E+00 | grad norm: 4.704 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.198 | TFLOPs: 46.47 |
[default7]: iteration       50/  250000 | consumed samples:         9600 | consumed tokens:     19660800 | elapsed time per iteration (ms): 5579.9 | learning rate: 7.500E-06 | global batch size:   192 | lm loss: 6.953404E+00 | grad norm: 15.990 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.409 | TFLOPs: 46.76 |
[default7]: iteration       51/  250000 | consumed samples:         9792 | consumed tokens:     20054016 | elapsed time per iteration (ms): 5587.4 | learning rate: 7.650E-06 | global batch size:   192 | lm loss: 6.966556E+00 | grad norm: 11.503 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.363 | TFLOPs: 46.70 |
[default7]: iteration       52/  250000 | consumed samples:         9984 | consumed tokens:     20447232 | elapsed time per iteration (ms): 5693.1 | learning rate: 7.800E-06 | global batch size:   192 | lm loss: 6.891538E+00 | grad norm: 10.811 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.725 | TFLOPs: 45.83 |
[default7]: iteration       53/  250000 | consumed samples:        10176 | consumed tokens:     20840448 | elapsed time per iteration (ms): 5629.1 | learning rate: 7.950E-06 | global batch size:   192 | lm loss: 6.857669E+00 | grad norm: 5.542 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.109 | TFLOPs: 46.35 |
[default7]: iteration       54/  250000 | consumed samples:        10368 | consumed tokens:     21233664 | elapsed time per iteration (ms): 5544.8 | learning rate: 8.100E-06 | global batch size:   192 | lm loss: 6.860970E+00 | grad norm: 24.324 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.627 | TFLOPs: 47.05 |
[default7]: iteration       55/  250000 | consumed samples:        10560 | consumed tokens:     21626880 | elapsed time per iteration (ms): 5598.8 | learning rate: 8.250E-06 | global batch size:   192 | lm loss: 6.807024E+00 | grad norm: 10.864 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.293 | TFLOPs: 46.60 |
[default7]: iteration       56/  250000 | consumed samples:        10752 | consumed tokens:     22020096 | elapsed time per iteration (ms): 5563.5 | learning rate: 8.400E-06 | global batch size:   192 | lm loss: 6.972313E+00 | grad norm: 10.540 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.511 | TFLOPs: 46.90 |
[default7]: iteration       57/  250000 | consumed samples:        10944 | consumed tokens:     22413312 | elapsed time per iteration (ms): 5606.0 | learning rate: 8.550E-06 | global batch size:   192 | lm loss: 6.928015E+00 | grad norm: 8.904 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.249 | TFLOPs: 46.54 |
[default7]: iteration       58/  250000 | consumed samples:        11136 | consumed tokens:     22806528 | elapsed time per iteration (ms): 5621.4 | learning rate: 8.700E-06 | global batch size:   192 | lm loss: 6.818995E+00 | grad norm: 5.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.155 | TFLOPs: 46.41 |
[default7]: iteration       59/  250000 | consumed samples:        11328 | consumed tokens:     23199744 | elapsed time per iteration (ms): 5586.6 | learning rate: 8.850E-06 | global batch size:   192 | lm loss: 6.936934E+00 | grad norm: 18.657 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.368 | TFLOPs: 46.70 |
[default7]: iteration       60/  250000 | consumed samples:        11520 | consumed tokens:     23592960 | elapsed time per iteration (ms): 5569.1 | learning rate: 9.000E-06 | global batch size:   192 | lm loss: 6.877006E+00 | grad norm: 5.791 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.476 | TFLOPs: 46.85 |
[default7]: iteration       61/  250000 | consumed samples:        11712 | consumed tokens:     23986176 | elapsed time per iteration (ms): 5560.1 | learning rate: 9.150E-06 | global batch size:   192 | lm loss: 6.687098E+00 | grad norm: 7.955 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.532 | TFLOPs: 46.93 |
[default7]: iteration       62/  250000 | consumed samples:        11904 | consumed tokens:     24379392 | elapsed time per iteration (ms): 5601.4 | learning rate: 9.300E-06 | global batch size:   192 | lm loss: 6.811944E+00 | grad norm: 6.599 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.277 | TFLOPs: 46.58 |
[default7]: iteration       63/  250000 | consumed samples:        12096 | consumed tokens:     24772608 | elapsed time per iteration (ms): 5580.1 | learning rate: 9.450E-06 | global batch size:   192 | lm loss: 6.831275E+00 | grad norm: 4.477 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.408 | TFLOPs: 46.76 |
[default7]: iteration       64/  250000 | consumed samples:        12288 | consumed tokens:     25165824 | elapsed time per iteration (ms): 5631.4 | learning rate: 9.600E-06 | global batch size:   192 | lm loss: 6.718836E+00 | grad norm: 6.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.095 | TFLOPs: 46.33 |
[default7]: iteration       65/  250000 | consumed samples:        12480 | consumed tokens:     25559040 | elapsed time per iteration (ms): 5584.1 | learning rate: 9.750E-06 | global batch size:   192 | lm loss: 6.639339E+00 | grad norm: 7.937 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.383 | TFLOPs: 46.72 |
[default7]: iteration       66/  250000 | consumed samples:        12672 | consumed tokens:     25952256 | elapsed time per iteration (ms): 5588.4 | learning rate: 9.900E-06 | global batch size:   192 | lm loss: 6.765018E+00 | grad norm: 4.929 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.357 | TFLOPs: 46.69 |
[default7]: iteration       67/  250000 | consumed samples:        12864 | consumed tokens:     26345472 | elapsed time per iteration (ms): 5606.6 | learning rate: 1.005E-05 | global batch size:   192 | lm loss: 6.474369E+00 | grad norm: 6.493 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.245 | TFLOPs: 46.54 |
[default7]: iteration       68/  250000 | consumed samples:        13056 | consumed tokens:     26738688 | elapsed time per iteration (ms): 5627.9 | learning rate: 1.020E-05 | global batch size:   192 | lm loss: 6.620712E+00 | grad norm: 4.021 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.116 | TFLOPs: 46.36 |
[default7]: iteration       69/  250000 | consumed samples:        13248 | consumed tokens:     27131904 | elapsed time per iteration (ms): 5544.3 | learning rate: 1.035E-05 | global batch size:   192 | lm loss: 6.476369E+00 | grad norm: 4.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.630 | TFLOPs: 47.06 |
[default7]: iteration       70/  250000 | consumed samples:        13440 | consumed tokens:     27525120 | elapsed time per iteration (ms): 5594.8 | learning rate: 1.050E-05 | global batch size:   192 | lm loss: 6.499898E+00 | grad norm: 6.021 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.318 | TFLOPs: 46.63 |
[default7]: iteration       71/  250000 | consumed samples:        13632 | consumed tokens:     27918336 | elapsed time per iteration (ms): 5655.2 | learning rate: 1.065E-05 | global batch size:   192 | lm loss: 6.494319E+00 | grad norm: 3.093 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.951 | TFLOPs: 46.14 |
[default7]: iteration       72/  250000 | consumed samples:        13824 | consumed tokens:     28311552 | elapsed time per iteration (ms): 5581.5 | learning rate: 1.080E-05 | global batch size:   192 | lm loss: 6.480914E+00 | grad norm: 4.538 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.399 | TFLOPs: 46.75 |
[default7]: iteration       73/  250000 | consumed samples:        14016 | consumed tokens:     28704768 | elapsed time per iteration (ms): 5622.5 | learning rate: 1.095E-05 | global batch size:   192 | lm loss: 6.570725E+00 | grad norm: 4.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.148 | TFLOPs: 46.40 |
[default0]:[2024-04-21 10:52:35,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=46, skipped=0, lr=[6.9e-06, 6.9e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 46 loss: 7.0593 iter time (s): 5.589 samples/sec: 34.355
[default0]:[2024-04-21 10:52:41,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=47, skipped=0, lr=[7.049999999999999e-06, 7.049999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 47 loss: 7.0212 iter time (s): 5.630 samples/sec: 34.102
[default0]:[2024-04-21 10:52:46,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=48, skipped=0, lr=[7.199999999999999e-06, 7.199999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 48 loss: 6.8856 iter time (s): 5.543 samples/sec: 34.640
[default0]:[2024-04-21 10:52:52,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=49, skipped=0, lr=[7.349999999999999e-06, 7.349999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 49 loss: 7.0426 iter time (s): 5.610 samples/sec: 34.224
[default0]:[2024-04-21 10:52:57,877] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[7.499999999999999e-06, 7.499999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 50 loss: 6.9534 iter time (s): 5.574 samples/sec: 34.447
[default0]:[2024-04-21 10:53:03,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=51, skipped=0, lr=[7.65e-06, 7.65e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 51 loss: 6.9666 iter time (s): 5.583 samples/sec: 34.391
[default0]:[2024-04-21 10:53:09,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=52, skipped=0, lr=[7.8e-06, 7.8e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 52 loss: 6.8915 iter time (s): 5.689 samples/sec: 33.751
[default0]:[2024-04-21 10:53:14,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=53, skipped=0, lr=[7.949999999999998e-06, 7.949999999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 53 loss: 6.8577 iter time (s): 5.623 samples/sec: 34.148
[default0]:[2024-04-21 10:53:20,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=54, skipped=0, lr=[8.1e-06, 8.1e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 54 loss: 6.8610 iter time (s): 5.540 samples/sec: 34.656
[default0]:[2024-04-21 10:53:25,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=55, skipped=0, lr=[8.249999999999999e-06, 8.249999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 55 loss: 6.8070 iter time (s): 5.594 samples/sec: 34.321
[default0]:[2024-04-21 10:53:31,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=56, skipped=0, lr=[8.4e-06, 8.4e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 56 loss: 6.9723 iter time (s): 5.546 samples/sec: 34.619
[default0]:[2024-04-21 10:53:37,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=57, skipped=0, lr=[8.55e-06, 8.55e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 57 loss: 6.9280 iter time (s): 5.600 samples/sec: 34.283
[default0]:[2024-04-21 10:53:42,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=58, skipped=0, lr=[8.7e-06, 8.7e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 58 loss: 6.8190 iter time (s): 5.601 samples/sec: 34.280
[default0]:[2024-04-21 10:53:48,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=59, skipped=0, lr=[8.849999999999998e-06, 8.849999999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 59 loss: 6.9369 iter time (s): 5.582 samples/sec: 34.396
[default0]:[2024-04-21 10:53:53,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[8.999999999999999e-06, 8.999999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 60 loss: 6.8770 iter time (s): 5.563 samples/sec: 34.513
[default0]:[2024-04-21 10:53:59,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=61, skipped=0, lr=[9.149999999999999e-06, 9.149999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 61 loss: 6.6871 iter time (s): 5.555 samples/sec: 34.565
[default0]:[2024-04-21 10:54:05,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=62, skipped=0, lr=[9.299999999999999e-06, 9.299999999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 62 loss: 6.8119 iter time (s): 5.597 samples/sec: 34.304
[default0]:[2024-04-21 10:54:10,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=63, skipped=0, lr=[9.45e-06, 9.45e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 63 loss: 6.8313 iter time (s): 5.575 samples/sec: 34.440
[default0]:[2024-04-21 10:54:16,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=64, skipped=0, lr=[9.6e-06, 9.6e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 64 loss: 6.7188 iter time (s): 5.627 samples/sec: 34.123
[default0]:[2024-04-21 10:54:21,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=65, skipped=0, lr=[9.75e-06, 9.75e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 65 loss: 6.6393 iter time (s): 5.580 samples/sec: 34.410
[default0]:[2024-04-21 10:54:27,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=66, skipped=0, lr=[9.899999999999998e-06, 9.899999999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 66 loss: 6.7650 iter time (s): 5.583 samples/sec: 34.391
[default0]:[2024-04-21 10:54:33,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=67, skipped=0, lr=[1.0049999999999999e-05, 1.0049999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 67 loss: 6.4744 iter time (s): 5.602 samples/sec: 34.272
[default0]:[2024-04-21 10:54:38,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=68, skipped=0, lr=[1.0199999999999999e-05, 1.0199999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 68 loss: 6.6207 iter time (s): 5.616 samples/sec: 34.189
[default0]:[2024-04-21 10:54:44,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=69, skipped=0, lr=[1.035e-05, 1.035e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 69 loss: 6.4764 iter time (s): 5.538 samples/sec: 34.670
[default0]:[2024-04-21 10:54:49,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.05e-05, 1.05e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 70 loss: 6.4999 iter time (s): 5.582 samples/sec: 34.397
[default0]:[2024-04-21 10:54:55,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=71, skipped=0, lr=[1.065e-05, 1.065e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 71 loss: 6.4943 iter time (s): 5.645 samples/sec: 34.011
[default0]:[2024-04-21 10:55:01,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=72, skipped=0, lr=[1.08e-05, 1.08e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 72 loss: 6.4809 iter time (s): 5.577 samples/sec: 34.425
[default0]:[2024-04-21 10:55:06,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=73, skipped=0, lr=[1.0949999999999998e-05, 1.0949999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 73 loss: 6.5707 iter time (s): 5.618 samples/sec: 34.174
[default0]:[2024-04-21 10:55:12,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=74, skipped=0, lr=[1.1099999999999999e-05, 1.1099999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 74 loss: 6.5339 iter time (s): 5.561 samples/sec: 34.526
[default0]:[2024-04-21 10:55:17,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=75, skipped=0, lr=[1.1249999999999999e-05, 1.1249999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 75 loss: 6.4028 iter time (s): 5.611 samples/sec: 34.215
[default0]:[2024-04-21 10:55:23,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=76, skipped=0, lr=[1.1399999999999998e-05, 1.1399999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 76 loss: 6.4988 iter time (s): 5.655 samples/sec: 33.952
[default0]:[2024-04-21 10:55:29,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=77, skipped=0, lr=[1.155e-05, 1.155e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 77 loss: 6.4873 iter time (s): 5.645 samples/sec: 34.011
[default0]:[2024-04-21 10:55:34,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=78, skipped=0, lr=[1.17e-05, 1.17e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 78 loss: 6.4473 iter time (s): 5.656 samples/sec: 33.948
[default0]:[2024-04-21 10:55:40,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=79, skipped=0, lr=[1.185e-05, 1.185e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default7]: iteration       74/  250000 | consumed samples:        14208 | consumed tokens:     29097984 | elapsed time per iteration (ms): 5565.7 | learning rate: 1.110E-05 | global batch size:   192 | lm loss: 6.533938E+00 | grad norm: 3.057 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.497 | TFLOPs: 46.88 |
[default7]: iteration       75/  250000 | consumed samples:        14400 | consumed tokens:     29491200 | elapsed time per iteration (ms): 5623.3 | learning rate: 1.125E-05 | global batch size:   192 | lm loss: 6.402808E+00 | grad norm: 3.849 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.144 | TFLOPs: 46.40 |
[default7]: iteration       76/  250000 | consumed samples:        14592 | consumed tokens:     29884416 | elapsed time per iteration (ms): 5661.3 | learning rate: 1.140E-05 | global batch size:   192 | lm loss: 6.498805E+00 | grad norm: 3.892 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.915 | TFLOPs: 46.09 |
[default7]: iteration       77/  250000 | consumed samples:        14784 | consumed tokens:     30277632 | elapsed time per iteration (ms): 5651.4 | learning rate: 1.155E-05 | global batch size:   192 | lm loss: 6.487298E+00 | grad norm: 4.563 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.974 | TFLOPs: 46.17 |
[default7]: iteration       78/  250000 | consumed samples:        14976 | consumed tokens:     30670848 | elapsed time per iteration (ms): 5663.0 | learning rate: 1.170E-05 | global batch size:   192 | lm loss: 6.447329E+00 | grad norm: 4.979 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.904 | TFLOPs: 46.07 |
[default7]: iteration       79/  250000 | consumed samples:        15168 | consumed tokens:     31064064 | elapsed time per iteration (ms): 5602.2 | learning rate: 1.185E-05 | global batch size:   192 | lm loss: 6.400451E+00 | grad norm: 6.030 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.272 | TFLOPs: 46.57 |
[default7]: iteration       80/  250000 | consumed samples:        15360 | consumed tokens:     31457280 | elapsed time per iteration (ms): 5587.6 | learning rate: 1.200E-05 | global batch size:   192 | lm loss: 6.404664E+00 | grad norm: 5.812 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.362 | TFLOPs: 46.69 |
[default7]: iteration       81/  250000 | consumed samples:        15552 | consumed tokens:     31850496 | elapsed time per iteration (ms): 5600.3 | learning rate: 1.215E-05 | global batch size:   192 | lm loss: 6.262426E+00 | grad norm: 5.302 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.284 | TFLOPs: 46.59 |
[default7]: iteration       82/  250000 | consumed samples:        15744 | consumed tokens:     32243712 | elapsed time per iteration (ms): 5592.1 | learning rate: 1.230E-05 | global batch size:   192 | lm loss: 6.415198E+00 | grad norm: 4.750 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.334 | TFLOPs: 46.66 |
[default7]: iteration       83/  250000 | consumed samples:        15936 | consumed tokens:     32636928 | elapsed time per iteration (ms): 5564.8 | learning rate: 1.245E-05 | global batch size:   192 | lm loss: 6.365761E+00 | grad norm: 4.296 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.502 | TFLOPs: 46.89 |
[default7]: iteration       84/  250000 | consumed samples:        16128 | consumed tokens:     33030144 | elapsed time per iteration (ms): 5627.0 | learning rate: 1.260E-05 | global batch size:   192 | lm loss: 6.420250E+00 | grad norm: 4.440 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.121 | TFLOPs: 46.37 |
[default7]: iteration       85/  250000 | consumed samples:        16320 | consumed tokens:     33423360 | elapsed time per iteration (ms): 5634.7 | learning rate: 1.275E-05 | global batch size:   192 | lm loss: 6.292951E+00 | grad norm: 5.868 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.074 | TFLOPs: 46.30 |
[default7]: iteration       86/  250000 | consumed samples:        16512 | consumed tokens:     33816576 | elapsed time per iteration (ms): 5625.4 | learning rate: 1.290E-05 | global batch size:   192 | lm loss: 6.335280E+00 | grad norm: 7.063 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.131 | TFLOPs: 46.38 |
[default7]: iteration       87/  250000 | consumed samples:        16704 | consumed tokens:     34209792 | elapsed time per iteration (ms): 5593.6 | learning rate: 1.305E-05 | global batch size:   192 | lm loss: 6.362469E+00 | grad norm: 7.732 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.325 | TFLOPs: 46.64 |
[default7]: iteration       88/  250000 | consumed samples:        16896 | consumed tokens:     34603008 | elapsed time per iteration (ms): 5660.2 | learning rate: 1.320E-05 | global batch size:   192 | lm loss: 6.241780E+00 | grad norm: 5.692 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.921 | TFLOPs: 46.10 |
[default7]: iteration       89/  250000 | consumed samples:        17088 | consumed tokens:     34996224 | elapsed time per iteration (ms): 5591.7 | learning rate: 1.335E-05 | global batch size:   192 | lm loss: 6.209259E+00 | grad norm: 8.435 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.337 | TFLOPs: 46.66 |
[default7]: iteration       90/  250000 | consumed samples:        17280 | consumed tokens:     35389440 | elapsed time per iteration (ms): 5615.2 | learning rate: 1.350E-05 | global batch size:   192 | lm loss: 6.383793E+00 | grad norm: 12.452 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.193 | TFLOPs: 46.46 |
[default7]: iteration       91/  250000 | consumed samples:        17472 | consumed tokens:     35782656 | elapsed time per iteration (ms): 5587.7 | learning rate: 1.365E-05 | global batch size:   192 | lm loss: 6.230978E+00 | grad norm: 9.412 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.361 | TFLOPs: 46.69 |
[default7]: iteration       92/  250000 | consumed samples:        17664 | consumed tokens:     36175872 | elapsed time per iteration (ms): 5578.3 | learning rate: 1.380E-05 | global batch size:   192 | lm loss: 6.375531E+00 | grad norm: 26.472 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.419 | TFLOPs: 46.77 |
[default7]: iteration       93/  250000 | consumed samples:        17856 | consumed tokens:     36569088 | elapsed time per iteration (ms): 5668.4 | learning rate: 1.395E-05 | global batch size:   192 | lm loss: 6.538034E+00 | grad norm: 12.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.872 | TFLOPs: 46.03 |
[default0]:steps: 79 loss: 6.4005 iter time (s): 5.592 samples/sec: 34.337
[default0]:[2024-04-21 10:55:46,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.1999999999999999e-05, 1.1999999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 80 loss: 6.4047 iter time (s): 5.583 samples/sec: 34.393
[default0]:[2024-04-21 10:55:51,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=81, skipped=0, lr=[1.2149999999999999e-05, 1.2149999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 81 loss: 6.2624 iter time (s): 5.595 samples/sec: 34.314
[default0]:[2024-04-21 10:55:57,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=82, skipped=0, lr=[1.2299999999999999e-05, 1.2299999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 82 loss: 6.4152 iter time (s): 5.586 samples/sec: 34.373
[default0]:[2024-04-21 10:56:02,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=83, skipped=0, lr=[1.2449999999999998e-05, 1.2449999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 83 loss: 6.3658 iter time (s): 5.558 samples/sec: 34.544
[default0]:[2024-04-21 10:56:08,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=84, skipped=0, lr=[1.2599999999999998e-05, 1.2599999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 84 loss: 6.4203 iter time (s): 5.618 samples/sec: 34.178
[default0]:[2024-04-21 10:56:14,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=85, skipped=0, lr=[1.275e-05, 1.275e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 85 loss: 6.2930 iter time (s): 5.620 samples/sec: 34.161
[default0]:[2024-04-21 10:56:19,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=86, skipped=0, lr=[1.29e-05, 1.29e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 86 loss: 6.3353 iter time (s): 5.614 samples/sec: 34.199
[default0]:[2024-04-21 10:56:25,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=87, skipped=0, lr=[1.3049999999999999e-05, 1.3049999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 87 loss: 6.3625 iter time (s): 5.589 samples/sec: 34.351
[default0]:[2024-04-21 10:56:30,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=88, skipped=0, lr=[1.3199999999999999e-05, 1.3199999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 88 loss: 6.2418 iter time (s): 5.654 samples/sec: 33.959
[default0]:[2024-04-21 10:56:36,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=89, skipped=0, lr=[1.335e-05, 1.335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 89 loss: 6.2093 iter time (s): 5.587 samples/sec: 34.365
[default0]:[2024-04-21 10:56:42,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.3499999999999998e-05, 1.3499999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 90 loss: 6.3838 iter time (s): 5.609 samples/sec: 34.231
[default0]:[2024-04-21 10:56:47,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=91, skipped=0, lr=[1.3649999999999998e-05, 1.3649999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 91 loss: 6.2310 iter time (s): 5.572 samples/sec: 34.457
[default0]:[2024-04-21 10:56:53,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=92, skipped=0, lr=[1.38e-05, 1.38e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 92 loss: 6.3755 iter time (s): 5.573 samples/sec: 34.449
[default0]:[2024-04-21 10:56:58,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=93, skipped=0, lr=[1.395e-05, 1.395e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 93 loss: 6.5380 iter time (s): 5.664 samples/sec: 33.898
[default0]:[2024-04-21 10:57:04,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=94, skipped=0, lr=[1.4099999999999999e-05, 1.4099999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 94 loss: 6.6010 iter time (s): 5.660 samples/sec: 33.922
[default0]:[2024-04-21 10:57:10,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=95, skipped=0, lr=[1.4249999999999999e-05, 1.4249999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 95 loss: 6.6602 iter time (s): 5.557 samples/sec: 34.551
[default0]:[2024-04-21 10:57:15,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=96, skipped=0, lr=[1.4399999999999998e-05, 1.4399999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 96 loss: 6.6377 iter time (s): 5.600 samples/sec: 34.284
[default0]:[2024-04-21 10:57:21,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=97, skipped=0, lr=[1.4549999999999998e-05, 1.4549999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 97 loss: 6.6822 iter time (s): 5.563 samples/sec: 34.514
[default0]:[2024-04-21 10:57:26,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=98, skipped=0, lr=[1.4699999999999998e-05, 1.4699999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 98 loss: 6.5303 iter time (s): 5.561 samples/sec: 34.528
[default0]:[2024-04-21 10:57:32,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=99, skipped=0, lr=[1.485e-05, 1.485e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 99 loss: 6.4749 iter time (s): 5.579 samples/sec: 34.413
[default0]:[2024-04-21 10:57:38,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1.4999999999999999e-05, 1.4999999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 100 loss: 6.2423 iter time (s): 5.568 samples/sec: 34.484
[default0]:saving checkpoint at iteration     100 to /gpfswork/rech/qgz/urc37ho/checkpoints12/
[default0]:[2024-04-21 10:57:39,005] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
[default0]:[2024-04-21 10:57:39,110] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_01-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:39,216] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_01-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:39,271] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_02-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:39,494] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_02-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:39,548] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_03-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:39,752] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_03-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:39,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_04-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:40,031] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_04-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:40,085] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_05-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:40,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_05-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:40,355] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_06-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:40,559] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_06-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:40,610] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_07-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:40,837] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_07-model_00-model_states.pt.
[default0]:[2024-04-21 10:48:11,251] [INFO] [engine.py:158:__init__] RANK=32 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=3369209856 (3369.210M) TOTAL_PARAMS=6738415616 (6738.416M) UNIQUE_PARAMS=6738415616 (6738.416M)
[default6]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default4]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default5]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[Rank 32] (after 1 iterations) memory (MB) | allocated: 28673.47509765625 | max allocated: 45846.26416015625 | reserved: 53190.0 | max reserved: 53190.0
[default0]:[2024-04-21 10:57:39,132] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_18-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:39,349] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_18-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:39,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_19-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:39,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_19-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:39,685] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_20-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:39,910] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_20-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:39,969] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_21-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:40,197] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_21-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:40,254] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_22-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:40,486] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_22-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:40,543] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_23-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:40,777] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_23-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:40,834] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_24-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:41,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_24-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:41,123] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_25-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:41,334] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_25-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:41,391] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_26-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:41,616] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_26-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:41,674] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_27-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:41,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_27-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:41,962] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_28-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:42,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_28-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:42,255] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_29-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:42,483] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_29-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:42,540] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_30-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:42,786] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_30-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:42,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_31-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:43,054] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_31-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:43,110] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_32-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:40,893] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_08-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:41,123] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_08-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:41,174] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_09-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:41,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_09-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:41,538] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_10-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:41,759] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_10-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:41,811] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_11-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:42,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_11-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:42,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_12-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:42,315] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_12-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:42,366] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_13-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:42,729] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_13-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:42,785] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_14-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:43,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_14-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:43,091] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_15-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:43,429] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_15-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:43,485] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_16-model_00-model_states.pt...
[default0]:[2024-04-21 10:57:43,735] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_16-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:43,787] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_17-model_00-model_states.pt...
[default1]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[default0]:[2024-04-21 10:57:44,001] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/layer_17-model_00-model_states.pt.
[default0]:[2024-04-21 10:57:44,003] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/mp_rank_00_model_states.pt
[default0]:[2024-04-21 10:57:44,003] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/mp_rank_00_model_states.pt...
[default0]:[2024-04-21 10:57:44,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/mp_rank_00_model_states.pt.
[default0]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[default6]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[default3]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[default4]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[default5]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[default2]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[default7]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[default3]:[2024-04-21 10:57:46,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[default3]:[2024-04-21 10:57:46,540] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[default3]:[2024-04-21 10:57:46,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default2]:[2024-04-21 10:57:46,570] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[default2]:[2024-04-21 10:57:46,571] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[default2]:[2024-04-21 10:57:46,571] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default7]:[2024-04-21 10:57:46,589] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[default7]:[2024-04-21 10:57:46,589] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[default7]:[2024-04-21 10:57:46,589] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default6]:[2024-04-21 10:57:46,666] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[default6]:[2024-04-21 10:57:46,666] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[default6]:[2024-04-21 10:57:46,666] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default7]: iteration       94/  250000 | consumed samples:        18048 | consumed tokens:     36962304 | elapsed time per iteration (ms): 5664.3 | learning rate: 1.410E-05 | global batch size:   192 | lm loss: 6.601020E+00 | grad norm: 7.723 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.896 | TFLOPs: 46.06 |
[default7]: iteration       95/  250000 | consumed samples:        18240 | consumed tokens:     37355520 | elapsed time per iteration (ms): 5563.1 | learning rate: 1.425E-05 | global batch size:   192 | lm loss: 6.660180E+00 | grad norm: 18.341 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.513 | TFLOPs: 46.90 |
[default7]: iteration       96/  250000 | consumed samples:        18432 | consumed tokens:     37748736 | elapsed time per iteration (ms): 5606.7 | learning rate: 1.440E-05 | global batch size:   192 | lm loss: 6.637652E+00 | grad norm: 12.259 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.245 | TFLOPs: 46.54 |
[default7]: iteration       97/  250000 | consumed samples:        18624 | consumed tokens:     38141952 | elapsed time per iteration (ms): 5567.4 | learning rate: 1.455E-05 | global batch size:   192 | lm loss: 6.682206E+00 | grad norm: 7.839 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.486 | TFLOPs: 46.86 |
[default7]: iteration       98/  250000 | consumed samples:        18816 | consumed tokens:     38535168 | elapsed time per iteration (ms): 5565.1 | learning rate: 1.470E-05 | global batch size:   192 | lm loss: 6.530277E+00 | grad norm: 7.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.501 | TFLOPs: 46.88 |
[default7]: iteration       99/  250000 | consumed samples:        19008 | consumed tokens:     38928384 | elapsed time per iteration (ms): 5592.2 | learning rate: 1.485E-05 | global batch size:   192 | lm loss: 6.474902E+00 | grad norm: 6.354 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.334 | TFLOPs: 46.66 |
[default7]: iteration      100/  250000 | consumed samples:        19200 | consumed tokens:     39321600 | elapsed time per iteration (ms): 5579.8 | learning rate: 1.500E-05 | global batch size:   192 | lm loss: 6.242265E+00 | grad norm: 5.365 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.410 | TFLOPs: 46.76 |
[default7]:-----------------------------------------------------------------------------------------------
[default7]: validation loss at iteration 100 | lm loss value: 6.765426E+00 | lm loss PPL: 8.673353E+02 | 
[default7]:-----------------------------------------------------------------------------------------------
[default7]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_31_mp_rank_01_optim_states.pt...
[default6]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_30_mp_rank_01_optim_states.pt...
[default4]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_28_mp_rank_01_optim_states.pt...
[default3]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_27_mp_rank_01_optim_states.pt...
[default5]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_29_mp_rank_01_optim_states.pt...
[default0]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_24_mp_rank_01_optim_states.pt...
[default1]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_25_mp_rank_01_optim_states.pt...
[default2]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_26_mp_rank_01_optim_states.pt...
[default3]:[2024-04-21 10:57:45,305] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_27_mp_rank_01_optim_states.pt.
[default3]:[2024-04-21 10:57:45,306] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_27_mp_rank_01_optim_states.pt
[default3]:[2024-04-21 10:57:45,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default2]:[2024-04-21 10:57:45,325] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_26_mp_rank_01_optim_states.pt.
[default2]:[2024-04-21 10:57:45,325] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_26_mp_rank_01_optim_states.pt
[default2]:[2024-04-21 10:57:45,325] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:[2024-04-21 10:57:45,609] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_24_mp_rank_01_optim_states.pt.
[default0]:[2024-04-21 10:57:45,609] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_24_mp_rank_01_optim_states.pt
[default0]:[2024-04-21 10:57:45,609] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default1]:[2024-04-21 10:57:45,654] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_25_mp_rank_01_optim_states.pt.
[default1]:[2024-04-21 10:57:45,654] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_25_mp_rank_01_optim_states.pt
[default1]:[2024-04-21 10:57:45,654] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default4]:[2024-04-21 10:57:46,381] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_28_mp_rank_01_optim_states.pt.
[default4]:[2024-04-21 10:57:46,381] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_28_mp_rank_01_optim_states.pt
[default4]:[2024-04-21 10:57:46,381] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default5]:[2024-04-21 10:57:46,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_29_mp_rank_01_optim_states.pt.
[default5]:[2024-04-21 10:57:46,629] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_29_mp_rank_01_optim_states.pt
[default5]:[2024-04-21 10:57:46,629] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default5]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default6]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default4]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_31_mp_rank_00_optim_states.pt...
[default0]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_24_mp_rank_00_optim_states.pt...
[default2]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_26_mp_rank_00_optim_states.pt...
[default1]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_25_mp_rank_00_optim_states.pt...
[default3]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_27_mp_rank_00_optim_states.pt...
[default6]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_30_mp_rank_00_optim_states.pt...
[default5]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_29_mp_rank_00_optim_states.pt...
[default4]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_28_mp_rank_00_optim_states.pt...
[default3]:[2024-04-21 10:57:45,606] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_27_mp_rank_00_optim_states.pt.
[default3]:[2024-04-21 10:57:45,606] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_27_mp_rank_00_optim_states.pt
[default3]:[2024-04-21 10:57:45,606] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default6]:[2024-04-21 10:57:45,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_30_mp_rank_00_optim_states.pt.
[default6]:[2024-04-21 10:57:45,628] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_30_mp_rank_00_optim_states.pt
[default6]:[2024-04-21 10:57:45,628] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default2]:[2024-04-21 10:57:45,970] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_26_mp_rank_00_optim_states.pt.
[default2]:[2024-04-21 10:57:45,970] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_26_mp_rank_00_optim_states.pt
[default2]:[2024-04-21 10:57:45,970] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default7]:[2024-04-21 10:57:45,987] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_31_mp_rank_00_optim_states.pt.
[default7]:[2024-04-21 10:57:45,988] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_31_mp_rank_00_optim_states.pt
[default7]:[2024-04-21 10:57:45,988] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:[2024-04-21 10:57:46,074] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_24_mp_rank_00_optim_states.pt.
[default0]:[2024-04-21 10:57:46,074] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_24_mp_rank_00_optim_states.pt
[default0]:[2024-04-21 10:57:46,074] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default1]:[2024-04-21 10:57:46,056] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_25_mp_rank_00_optim_states.pt.
[default1]:[2024-04-21 10:57:46,056] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_25_mp_rank_00_optim_states.pt
[default1]:[2024-04-21 10:57:46,056] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default5]:[2024-04-21 10:57:47,599] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_29_mp_rank_00_optim_states.pt.
[default5]:[2024-04-21 10:57:47,599] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_29_mp_rank_00_optim_states.pt
[default5]:[2024-04-21 10:57:47,599] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default4]:[2024-04-21 10:57:47,608] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_28_mp_rank_00_optim_states.pt.
[default4]:[2024-04-21 10:57:47,608] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_28_mp_rank_00_optim_states.pt
[default3]:fused_lion ............. [93m[[default0]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default6]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default5]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default4]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_11_mp_rank_01_optim_states.pt...
[default7]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_15_mp_rank_01_optim_states.pt...
[default6]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_14_mp_rank_01_optim_states.pt...
[default4]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_12_mp_rank_01_optim_states.pt...
[default0]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_8_mp_rank_01_optim_states.pt...
[default5]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_13_mp_rank_01_optim_states.pt...
[default2]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_10_mp_rank_01_optim_states.pt...
[default1]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_9_mp_rank_01_optim_states.pt...
[default3]:[2024-04-21 10:57:45,567] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_11_mp_rank_01_optim_states.pt.
[default3]:[2024-04-21 10:57:45,567] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_11_mp_rank_01_optim_states.pt
[default3]:[2024-04-21 10:57:45,567] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default6]:[2024-04-21 10:57:45,567] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_14_mp_rank_01_optim_states.pt.
[default6]:[2024-04-21 10:57:45,567] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_14_mp_rank_01_optim_states.pt
[default6]:[2024-04-21 10:57:45,567] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default7]:[2024-04-21 10:57:45,907] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_15_mp_rank_01_optim_states.pt.
[default7]:[2024-04-21 10:57:45,907] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_15_mp_rank_01_optim_states.pt
[default7]:[2024-04-21 10:57:45,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default2]:[2024-04-21 10:57:45,900] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_10_mp_rank_01_optim_states.pt.
[default2]:[2024-04-21 10:57:45,901] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_10_mp_rank_01_optim_states.pt
[default2]:[2024-04-21 10:57:45,901] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default1]:[2024-04-21 10:57:46,042] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_9_mp_rank_01_optim_states.pt.
[default1]:[2024-04-21 10:57:46,042] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_9_mp_rank_01_optim_states.pt
[default1]:[2024-04-21 10:57:46,042] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:[2024-04-21 10:57:46,087] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_8_mp_rank_01_optim_states.pt.
[default0]:[2024-04-21 10:57:46,087] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_8_mp_rank_01_optim_states.pt
[default0]:[2024-04-21 10:57:46,087] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default4]:[2024-04-21 10:57:47,674] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_12_mp_rank_01_optim_states.pt.
[default4]:[2024-04-21 10:57:47,674] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_12_mp_rank_01_optim_states.pt
[default4]:[2024-04-21 10:57:47,674] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default5]:[2024-04-21 10:57:47,683] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_13_mp_rank_01_optim_states.pt.
[default5]:[2024-04-21 10:57:47,683] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_13_mp_rank_01_optim_states.pt
[default3]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default6]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default4]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default5]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default6]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_14_mp_rank_00_optim_states.pt...
[default5]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_13_mp_rank_00_optim_states.pt...
[default0]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_8_mp_rank_00_optim_states.pt...
[default1]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_9_mp_rank_00_optim_states.pt...
[default4]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_12_mp_rank_00_optim_states.pt...
[default7]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_15_mp_rank_00_optim_states.pt...
[default3]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_11_mp_rank_00_optim_states.pt...
[default2]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_10_mp_rank_00_optim_states.pt...
[default1]:[2024-04-21 10:57:45,453] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_9_mp_rank_00_optim_states.pt.
[default1]:[2024-04-21 10:57:45,454] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_9_mp_rank_00_optim_states.pt
[default1]:[2024-04-21 10:57:45,454] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default3]:[2024-04-21 10:57:45,777] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_11_mp_rank_00_optim_states.pt.
[default3]:[2024-04-21 10:57:45,777] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_11_mp_rank_00_optim_states.pt
[default3]:[2024-04-21 10:57:45,777] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default2]:[2024-04-21 10:57:45,809] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_10_mp_rank_00_optim_states.pt.
[default2]:[2024-04-21 10:57:45,809] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_10_mp_rank_00_optim_states.pt
[default2]:[2024-04-21 10:57:45,809] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:[2024-04-21 10:57:45,813] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_8_mp_rank_00_optim_states.pt.
[default0]:[2024-04-21 10:57:45,813] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_8_mp_rank_00_optim_states.pt
[default0]:[2024-04-21 10:57:45,813] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default6]:[2024-04-21 10:57:47,741] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_14_mp_rank_00_optim_states.pt.
[default6]:[2024-04-21 10:57:47,742] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_14_mp_rank_00_optim_states.pt
[default6]:[2024-04-21 10:57:47,742] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default4]:[2024-04-21 10:57:47,820] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_12_mp_rank_00_optim_states.pt.
[default4]:[2024-04-21 10:57:47,820] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_12_mp_rank_00_optim_states.pt
[default4]:[2024-04-21 10:57:47,820] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default7]:[2024-04-21 10:57:47,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_15_mp_rank_00_optim_states.pt.
[default7]:[2024-04-21 10:57:47,784] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_15_mp_rank_00_optim_states.pt
[default7]:[2024-04-21 10:57:47,784] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default5]:[2024-04-21 10:57:47,766] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_13_mp_rank_00_optim_states.pt.
[default5]:[2024-04-21 10:57:47,766] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_13_mp_rank_00_optim_states.pt
                                                                                                                                                                                                                                                                                                                                                                                                  1                                                                                                                                                                                                                                                                                                                                                   [default1]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default4]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default6]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default5]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-04-21 10:48:11,311] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_18_mp_rank_01_optim_states.pt...
[default0]:[2024-04-21 10:57:43,871] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_16_mp_rank_01_optim_states.pt...
[default4]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_20_mp_rank_01_optim_states.pt...
[default7]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_23_mp_rank_01_optim_states.pt...
[default5]:[2024-04-21 10:57:43,871] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_21_mp_rank_01_optim_states.pt...
[default6]:[2024-04-21 10:57:43,871] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_22_mp_rank_01_optim_states.pt...
[default3]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_19_mp_rank_01_optim_states.pt...
[default1]:[2024-04-21 10:57:43,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_17_mp_rank_01_optim_states.pt...
[default2]:[2024-04-21 10:57:46,384] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_18_mp_rank_01_optim_states.pt.
[default2]:[2024-04-21 10:57:46,384] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_18_mp_rank_01_optim_states.pt
[default2]:[2024-04-21 10:57:46,385] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:[2024-04-21 10:57:46,508] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_16_mp_rank_01_optim_states.pt.
[default0]:[2024-04-21 10:57:46,508] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_16_mp_rank_01_optim_states.pt
[default0]:[2024-04-21 10:57:46,508] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default6]:[2024-04-21 10:57:46,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_22_mp_rank_01_optim_states.pt.
[default6]:[2024-04-21 10:57:46,536] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_22_mp_rank_01_optim_states.pt
[default6]:[2024-04-21 10:57:46,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default1]:[2024-04-21 10:57:46,741] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_17_mp_rank_01_optim_states.pt.
[default1]:[2024-04-21 10:57:46,741] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_17_mp_rank_01_optim_states.pt
[default1]:[2024-04-21 10:57:46,741] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default7]:[2024-04-21 10:57:46,712] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_23_mp_rank_01_optim_states.pt.
[default7]:[2024-04-21 10:57:46,712] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_23_mp_rank_01_optim_states.pt
[default7]:[2024-04-21 10:57:46,712] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default3]:[2024-04-21 10:57:46,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_19_mp_rank_01_optim_states.pt.
[default3]:[2024-04-21 10:57:46,774] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_19_mp_rank_01_optim_states.pt
[default3]:[2024-04-21 10:57:46,774] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default4]:[2024-04-21 10:57:47,925] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_20_mp_rank_01_optim_states.pt.
[default4]:[2024-04-21 10:57:47,925] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_20_mp_rank_01_optim_states.pt
[default4]:[2024-04-21 10:57:47,925] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default5]:[2024-04-21 10:57:47,903] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_21_mp_rank_01_optim_states.pt.
[default5]:[2024-04-21 10:57:47,903] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_21_mp_rank_01_optim_states.pt
[default1]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default5]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default6]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default4]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2024-04-21 10:48:11,312] [WARNING] [engine.py:2714:load_checkpoint] Unable to find latest file at /gpfswork/rech/qgz/urc37ho/checkpoints12/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default7]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_23_mp_rank_00_optim_states.pt...
[default1]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_17_mp_rank_00_optim_states.pt...
[default4]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_20_mp_rank_00_optim_states.pt...
[default6]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_22_mp_rank_00_optim_states.pt...
[default5]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_21_mp_rank_00_optim_states.pt...
[default2]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_18_mp_rank_00_optim_states.pt...
[default3]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_19_mp_rank_00_optim_states.pt...
[default0]:[2024-04-21 10:57:44,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_16_mp_rank_00_optim_states.pt...
[default4]:[2024-04-21 10:57:46,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_20_mp_rank_00_optim_states.pt.
[default4]:[2024-04-21 10:57:46,782] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_20_mp_rank_00_optim_states.pt
[default4]:[2024-04-21 10:57:46,782] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default3]:[2024-04-21 10:57:47,223] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_19_mp_rank_00_optim_states.pt.
[default3]:[2024-04-21 10:57:47,223] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_19_mp_rank_00_optim_states.pt
[default3]:[2024-04-21 10:57:47,223] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default1]:[2024-04-21 10:57:47,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_17_mp_rank_00_optim_states.pt.
[default1]:[2024-04-21 10:57:47,213] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_17_mp_rank_00_optim_states.pt
[default1]:[2024-04-21 10:57:47,213] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default2]:[2024-04-21 10:57:47,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_18_mp_rank_00_optim_states.pt.
[default2]:[2024-04-21 10:57:47,254] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_18_mp_rank_00_optim_states.pt
[default2]:[2024-04-21 10:57:47,254] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default5]:[2024-04-21 10:57:47,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_21_mp_rank_00_optim_states.pt.
[default5]:[2024-04-21 10:57:47,388] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_21_mp_rank_00_optim_states.pt
[default5]:[2024-04-21 10:57:47,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:[2024-04-21 10:57:47,467] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_16_mp_rank_00_optim_states.pt.
[default0]:[2024-04-21 10:57:47,467] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_16_mp_rank_00_optim_states.pt
[default0]:[2024-04-21 10:57:47,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default7]:[2024-04-21 10:57:48,162] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_23_mp_rank_00_optim_states.pt.
[default7]:[2024-04-21 10:57:48,162] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_23_mp_rank_00_optim_states.pt
[default7]:[2024-04-21 10:57:48,163] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default6]:[2024-04-21 10:57:48,178] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_22_mp_rank_00_optim_states.pt.
[default6]:[2024-04-21 10:57:48,178] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_22_mp_rank_00_optim_states.pt
[default7]:[2024-04-21 10:57:47,433] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_31_mp_rank_01_optim_states.pt.
[default7]:[2024-04-21 10:57:47,433] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_31_mp_rank_01_optim_states.pt
[default7]:[2024-04-21 10:57:47,433] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default6]:[2024-04-21 10:57:47,455] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_30_mp_rank_01_optim_states.pt.
[default6]:[2024-04-21 10:57:47,455] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_30_mp_rank_01_optim_states.pt
[default6]:[2024-04-21 10:57:47,455] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default7]:(min, max) time across ranks (ms):
[default7]:    save-checkpoint ................................: (9195.76, 9195.82)
[default7]: iteration      101/  250000 | consumed samples:        19392 | consumed tokens:     39714816 | elapsed time per iteration (ms): 15641.8 | learning rate: 1.515E-05 | global batch size:   192 | lm loss: 6.766898E+00 | grad norm: 43.981 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 12.275 | TFLOPs: 16.68 |
[default7]: iteration      102/  250000 | consumed samples:        19584 | consumed tokens:     40108032 | elapsed time per iteration (ms): 5571.7 | learning rate: 1.530E-05 | global batch size:   192 | lm loss: 6.687145E+00 | grad norm: 8.830 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.460 | TFLOPs: 46.83 |
[default7]: iteration      103/  250000 | consumed samples:        19776 | consumed tokens:     40501248 | elapsed time per iteration (ms): 5556.3 | learning rate: 1.545E-05 | global batch size:   192 | lm loss: 6.561891E+00 | grad norm: 5.958 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.555 | TFLOPs: 46.96 |
[default7]: iteration      104/  250000 | consumed samples:        19968 | consumed tokens:     40894464 | elapsed time per iteration (ms): 5528.1 | learning rate: 1.560E-05 | global batch size:   192 | lm loss: 6.677666E+00 | grad norm: 5.262 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.731 | TFLOPs: 47.20 |
[default7]: iteration      105/  250000 | consumed samples:        20160 | consumed tokens:     41287680 | elapsed time per iteration (ms): 5520.1 | learning rate: 1.575E-05 | global batch size:   192 | lm loss: 6.555500E+00 | grad norm: 4.552 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.782 | TFLOPs: 47.27 |
[default7]: iteration      106/  250000 | consumed samples:        20352 | consumed tokens:     41680896 | elapsed time per iteration (ms): 5582.6 | learning rate: 1.590E-05 | global batch size:   192 | lm loss: 6.663439E+00 | grad norm: 5.059 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.393 | TFLOPs: 46.74 |
[default7]: iteration      107/  250000 | consumed samples:        20544 | consumed tokens:     42074112 | elapsed time per iteration (ms): 5542.1 | learning rate: 1.605E-05 | global batch size:   192 | lm loss: 6.646069E+00 | grad norm: 4.478 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.644 | TFLOPs: 47.08 |
[default7]: iteration      108/  250000 | consumed samples:        20736 | consumed tokens:     42467328 | elapsed time per iteration (ms): 5559.1 | learning rate: 1.620E-05 | global batch size:   192 | lm loss: 6.557377E+00 | grad norm: 4.535 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.538 | TFLOPs: 46.93 |
[default7]: iteration      109/  250000 | consumed samples:        20928 | consumed tokens:     42860544 | elapsed time per iteration (ms): 5557.6 | learning rate: 1.635E-05 | global batch size:   192 | lm loss: 6.386064E+00 | grad norm: 3.840 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.547 | TFLOPs: 46.95 |
[default7]: iteration      110/  250000 | consumed samples:        21120 | consumed tokens:     43253760 | elapsed time per iteration (ms): 5558.7 | learning rate: 1.650E-05 | global batch size:   192 | lm loss: 6.401956E+00 | grad norm: 4.482 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.540 | TFLOPs: 46.94 |
[default7]: iteration      111/  250000 | consumed samples:        21312 | consumed tokens:     43646976 | elapsed time per iteration (ms): 5580.9 | learning rate: 1.665E-05 | global batch size:   192 | lm loss: 6.321705E+00 | grad norm: 3.863 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.403 | TFLOPs: 46.75 |
[default7]: iteration      112/  250000 | consumed samples:        21504 | consumed tokens:     44040192 | elapsed time per iteration (ms): 5614.7 | learning rate: 1.680E-05 | global batch size:   192 | lm loss: 6.274512E+00 | grad norm: 3.808 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.196 | TFLOPs: 46.47 |
[default7]: iteration      113/  250000 | consumed samples:        21696 | consumed tokens:     44433408 | elapsed time per iteration (ms): 5584.7 | learning rate: 1.695E-05 | global batch size:   192 | lm loss: 6.266344E+00 | grad norm: 13.811 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.380 | TFLOPs: 46.72 |
[default7]: iteration      114/  250000 | consumed samples:        21888 | consumed tokens:     44826624 | elapsed time per iteration (ms): 5566.9 | learning rate: 1.710E-05 | global batch size:   192 | lm loss: 6.057811E+00 | grad norm: 3.450 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.490 | TFLOPs: 46.87 |
[default7]: iteration      115/  250000 | consumed samples:        22080 | consumed tokens:     45219840 | elapsed time per iteration (ms): 5579.8 | learning rate: 1.725E-05 | global batch size:   192 | lm loss: 6.188701E+00 | grad norm: 3.761 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.410 | TFLOPs: 46.76 |
[default7]: iteration      116/  250000 | consumed samples:        22272 | consumed tokens:     45613056 | elapsed time per iteration (ms): 5588.5 | learning rate: 1.740E-05 | global batch size:   192 | lm loss: 6.146191E+00 | grad norm: 3.258 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.356 | TFLOPs: 46.69 |
[default7]: iteration      117/  250000 | consumed samples:        22464 | consumed tokens:     46006272 | elapsed time per iteration (ms): 5540.2 | learning rate: 1.755E-05 | global batch size:   192 | lm loss: 6.111570E+00 | grad norm: 4.112 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.656 | TFLOPs: 47.09 |
[default1]:[2024-04-21 10:57:46,690] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[default1]:[2024-04-21 10:57:46,690] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[default1]:[2024-04-21 10:57:46,690] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:[2024-04-21 10:57:46,712] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[default0]:[2024-04-21 10:57:46,795] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[default0]:[2024-04-21 10:57:46,795] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default4]:[2024-04-21 10:57:48,042] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[default4]:[2024-04-21 10:57:48,042] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[default4]:[2024-04-21 10:57:48,042] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default5]:[2024-04-21 10:57:48,023] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[default5]:[2024-04-21 10:57:48,023] [INFO] [engine.py:3431:_save_zero_checkpoint] bf16_zero checkpoint saved /gpfswork/rech/qgz/urc37ho/checkpoints12/global_step100/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[default5]:[2024-04-21 10:57:48,023] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:  successfully saved checkpoint at iteration     100 to /gpfswork/rech/qgz/urc37ho/checkpoints12/
[default0]:Checkpoint Save GB: 94.338, GB/Sec: 10.26, Latency(second): 9.196
[default0]:[2024-04-21 10:57:53,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=101, skipped=0, lr=[1.5149999999999999e-05, 1.5149999999999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 101 loss: 6.7669 iter time (s): 5.584 samples/sec: 34.386
[default0]:[2024-04-21 10:57:59,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=102, skipped=0, lr=[1.53e-05, 1.53e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 102 loss: 6.6871 iter time (s): 5.565 samples/sec: 34.501
[default0]:[2024-04-21 10:58:04,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=103, skipped=0, lr=[1.545e-05, 1.545e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 103 loss: 6.5619 iter time (s): 5.550 samples/sec: 34.594
[default0]:[2024-04-21 10:58:10,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=104, skipped=0, lr=[1.56e-05, 1.56e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 104 loss: 6.6777 iter time (s): 5.522 samples/sec: 34.768
[default0]:[2024-04-21 10:58:15,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=105, skipped=0, lr=[1.5749999999999997e-05, 1.5749999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 105 loss: 6.5555 iter time (s): 5.516 samples/sec: 34.810
[default0]:[2024-04-21 10:58:21,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=106, skipped=0, lr=[1.5899999999999997e-05, 1.5899999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 106 loss: 6.6634 iter time (s): 5.576 samples/sec: 34.434
[default0]:[2024-04-21 10:58:27,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=107, skipped=0, lr=[1.605e-05, 1.605e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 107 loss: 6.6461 iter time (s): 5.535 samples/sec: 34.688
[default0]:[2024-04-21 10:58:32,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=108, skipped=0, lr=[1.62e-05, 1.62e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 108 loss: 6.5574 iter time (s): 5.544 samples/sec: 34.633
[default0]:[2024-04-21 10:58:38,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=109, skipped=0, lr=[1.6349999999999998e-05, 1.6349999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 109 loss: 6.3861 iter time (s): 5.553 samples/sec: 34.574
[default0]:[2024-04-21 10:58:43,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1.6499999999999998e-05, 1.6499999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 110 loss: 6.4020 iter time (s): 5.552 samples/sec: 34.585
[default0]:[2024-04-21 10:58:49,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=111, skipped=0, lr=[1.6649999999999998e-05, 1.6649999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 111 loss: 6.3217 iter time (s): 5.575 samples/sec: 34.440
[default0]:[2024-04-21 10:58:54,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=112, skipped=0, lr=[1.68e-05, 1.68e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 112 loss: 6.2745 iter time (s): 5.608 samples/sec: 34.237
[default0]:[2024-04-21 10:59:00,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=113, skipped=0, lr=[1.695e-05, 1.695e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 113 loss: 6.2663 iter time (s): 5.580 samples/sec: 34.407
[default0]:[2024-04-21 10:59:06,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=114, skipped=0, lr=[1.71e-05, 1.71e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 114 loss: 6.0578 iter time (s): 5.562 samples/sec: 34.517
[default0]:[2024-04-21 10:59:11,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=115, skipped=0, lr=[1.725e-05, 1.725e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 115 loss: 6.1887 iter time (s): 5.575 samples/sec: 34.437
[default0]:[2024-04-21 10:59:17,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=116, skipped=0, lr=[1.74e-05, 1.74e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 116 loss: 6.1462 iter time (s): 5.584 samples/sec: 34.384
[default0]:[2024-04-21 10:59:22,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=117, skipped=0, lr=[1.755e-05, 1.755e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 117 loss: 6.1116 iter time (s): 5.534 samples/sec: 34.694
[default0]:[2024-04-21 10:59:28,405] [INFO] [logging.py:96:log_dist] [Rank 0] step=118, skipped=0, lr=[1.7699999999999997e-05, 1.7699999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 118 loss: 6.0168 iter time (s): 5.616 samples/sec: 34.190
[default0]:[2024-04-21 10:59:33,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=119, skipped=0, lr=[1.7849999999999997e-05, 1.7849999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 119 loss: 6.0295 iter time (s): 5.518 samples/sec: 34.792
[default0]:[2024-04-21 10:59:39,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[1.7999999999999997e-05, 1.7999999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 120 loss: 6.1294 iter time (s): 5.532 samples/sec: 34.704
[default0]:[2024-04-21 10:59:44,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=121, skipped=0, lr=[1.815e-05, 1.815e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 121 loss: 6.2120 iter time (s): 5.506 samples/sec: 34.868
[default0]:[2024-04-21 10:59:50,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=122, skipped=0, lr=[1.8299999999999998e-05, 1.8299999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 122 loss: 6.0111 iter time (s): 5.528 samples/sec: 34.732
[default0]:[2024-04-21 10:59:56,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=123, skipped=0, lr=[1.8449999999999998e-05, 1.8449999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 123 loss: 5.8988 iter time (s): 5.577 samples/sec: 34.425
[default0]:[2024-04-21 11:00:01,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=124, skipped=0, lr=[1.8599999999999998e-05, 1.8599999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default7]: iteration      118/  250000 | consumed samples:        22656 | consumed tokens:     46399488 | elapsed time per iteration (ms): 5621.8 | learning rate: 1.770E-05 | global batch size:   192 | lm loss: 6.016826E+00 | grad norm: 3.790 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.153 | TFLOPs: 46.41 |
[default7]: iteration      119/  250000 | consumed samples:        22848 | consumed tokens:     46792704 | elapsed time per iteration (ms): 5525.3 | learning rate: 1.785E-05 | global batch size:   192 | lm loss: 6.029466E+00 | grad norm: 3.317 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.749 | TFLOPs: 47.22 |
[default7]: iteration      120/  250000 | consumed samples:        23040 | consumed tokens:     47185920 | elapsed time per iteration (ms): 5536.8 | learning rate: 1.800E-05 | global batch size:   192 | lm loss: 6.129441E+00 | grad norm: 4.794 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.677 | TFLOPs: 47.12 |
[default7]: iteration      121/  250000 | consumed samples:        23232 | consumed tokens:     47579136 | elapsed time per iteration (ms): 5512.8 | learning rate: 1.815E-05 | global batch size:   192 | lm loss: 6.211955E+00 | grad norm: 2.368 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.828 | TFLOPs: 47.33 |
[default7]: iteration      122/  250000 | consumed samples:        23424 | consumed tokens:     47972352 | elapsed time per iteration (ms): 5540.1 | learning rate: 1.830E-05 | global batch size:   192 | lm loss: 6.011142E+00 | grad norm: 3.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.656 | TFLOPs: 47.09 |
[default7]: iteration      123/  250000 | consumed samples:        23616 | consumed tokens:     48365568 | elapsed time per iteration (ms): 5590.4 | learning rate: 1.845E-05 | global batch size:   192 | lm loss: 5.898766E+00 | grad norm: 3.534 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.345 | TFLOPs: 46.67 |
[default7]: iteration      124/  250000 | consumed samples:        23808 | consumed tokens:     48758784 | elapsed time per iteration (ms): 5575.1 | learning rate: 1.860E-05 | global batch size:   192 | lm loss: 5.979981E+00 | grad norm: 3.039 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.439 | TFLOPs: 46.80 |
[default7]: iteration      125/  250000 | consumed samples:        24000 | consumed tokens:     49152000 | elapsed time per iteration (ms): 5615.8 | learning rate: 1.875E-05 | global batch size:   192 | lm loss: 6.032945E+00 | grad norm: 2.527 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.189 | TFLOPs: 46.46 |
[default7]: iteration      126/  250000 | consumed samples:        24192 | consumed tokens:     49545216 | elapsed time per iteration (ms): 5571.3 | learning rate: 1.890E-05 | global batch size:   192 | lm loss: 5.969007E+00 | grad norm: 2.295 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.462 | TFLOPs: 46.83 |
[default7]: iteration      127/  250000 | consumed samples:        24384 | consumed tokens:     49938432 | elapsed time per iteration (ms): 5579.0 | learning rate: 1.905E-05 | global batch size:   192 | lm loss: 5.892727E+00 | grad norm: 2.801 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.415 | TFLOPs: 46.77 |
[default7]: iteration      128/  250000 | consumed samples:        24576 | consumed tokens:     50331648 | elapsed time per iteration (ms): 5616.2 | learning rate: 1.920E-05 | global batch size:   192 | lm loss: 5.755872E+00 | grad norm: 3.353 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.187 | TFLOPs: 46.46 |
[default7]: iteration      129/  250000 | consumed samples:        24768 | consumed tokens:     50724864 | elapsed time per iteration (ms): 5515.2 | learning rate: 1.935E-05 | global batch size:   192 | lm loss: 5.920781E+00 | grad norm: 3.621 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.813 | TFLOPs: 47.31 |
[default7]: iteration      130/  250000 | consumed samples:        24960 | consumed tokens:     51118080 | elapsed time per iteration (ms): 5657.4 | learning rate: 1.950E-05 | global batch size:   192 | lm loss: 5.854619E+00 | grad norm: 2.524 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.938 | TFLOPs: 46.12 |
[default7]: iteration      131/  250000 | consumed samples:        25152 | consumed tokens:     51511296 | elapsed time per iteration (ms): 5584.5 | learning rate: 1.965E-05 | global batch size:   192 | lm loss: 5.807979E+00 | grad norm: 3.526 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.381 | TFLOPs: 46.72 |
[default7]: iteration      132/  250000 | consumed samples:        25344 | consumed tokens:     51904512 | elapsed time per iteration (ms): 5570.1 | learning rate: 1.980E-05 | global batch size:   192 | lm loss: 5.902511E+00 | grad norm: 3.308 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.470 | TFLOPs: 46.84 |
[default7]: iteration      133/  250000 | consumed samples:        25536 | consumed tokens:     52297728 | elapsed time per iteration (ms): 5553.5 | learning rate: 1.995E-05 | global batch size:   192 | lm loss: 5.918236E+00 | grad norm: 2.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.573 | TFLOPs: 46.98 |
[default7]: iteration      134/  250000 | consumed samples:        25728 | consumed tokens:     52690944 | elapsed time per iteration (ms): 5542.7 | learning rate: 2.010E-05 | global batch size:   192 | lm loss: 5.823510E+00 | grad norm: 3.482 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.640 | TFLOPs: 47.07 |
[default7]: iteration      135/  250000 | consumed samples:        25920 | consumed tokens:     53084160 | elapsed time per iteration (ms): 5545.8 | learning rate: 2.025E-05 | global batch size:   192 | lm loss: 5.814623E+00 | grad norm: 3.587 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.621 | TFLOPs: 47.05 |
[default7]: iteration      136/  250000 | consumed samples:        26112 | consumed tokens:     53477376 | elapsed time per iteration (ms): 5566.6 | learning rate: 2.040E-05 | global batch size:   192 | lm loss: 5.824173E+00 | grad norm: 3.839 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.491 | TFLOPs: 46.87 |
[default7]: iteration      137/  250000 | consumed samples:        26304 | consumed tokens:     53870592 | elapsed time per iteration (ms): 5602.1 | learning rate: 2.055E-05 | global batch size:   192 | lm loss: 5.769128E+00 | grad norm: 4.060 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.273 | TFLOPs: 46.57 |
[default0]:steps: 124 loss: 5.9800 iter time (s): 5.568 samples/sec: 34.480
[default0]:[2024-04-21 11:00:07,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=125, skipped=0, lr=[1.875e-05, 1.875e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 125 loss: 6.0329 iter time (s): 5.605 samples/sec: 34.257
[default0]:[2024-04-21 11:00:12,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=126, skipped=0, lr=[1.89e-05, 1.89e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 126 loss: 5.9690 iter time (s): 5.557 samples/sec: 34.554
[default0]:[2024-04-21 11:00:18,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=127, skipped=0, lr=[1.905e-05, 1.905e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 127 loss: 5.8927 iter time (s): 5.567 samples/sec: 34.490
[default0]:[2024-04-21 11:00:24,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=128, skipped=0, lr=[1.92e-05, 1.92e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 128 loss: 5.7559 iter time (s): 5.612 samples/sec: 34.213
[default0]:[2024-04-21 11:00:29,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=129, skipped=0, lr=[1.935e-05, 1.935e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 129 loss: 5.9208 iter time (s): 5.511 samples/sec: 34.840
[default0]:[2024-04-21 11:00:35,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[1.95e-05, 1.95e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 130 loss: 5.8546 iter time (s): 5.653 samples/sec: 33.964
[default0]:[2024-04-21 11:00:40,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=131, skipped=0, lr=[1.965e-05, 1.965e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 131 loss: 5.8080 iter time (s): 5.570 samples/sec: 34.468
[default0]:[2024-04-21 11:00:46,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=132, skipped=0, lr=[1.9799999999999997e-05, 1.9799999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 132 loss: 5.9025 iter time (s): 5.566 samples/sec: 34.497
[default0]:[2024-04-21 11:00:51,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=133, skipped=0, lr=[1.9949999999999997e-05, 1.9949999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 133 loss: 5.9182 iter time (s): 5.549 samples/sec: 34.600
[default0]:[2024-04-21 11:00:57,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=134, skipped=0, lr=[2.0099999999999997e-05, 2.0099999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 134 loss: 5.8235 iter time (s): 5.537 samples/sec: 34.679
[default0]:[2024-04-21 11:01:03,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=135, skipped=0, lr=[2.0249999999999998e-05, 2.0249999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 135 loss: 5.8146 iter time (s): 5.540 samples/sec: 34.655
[default0]:[2024-04-21 11:01:08,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=136, skipped=0, lr=[2.0399999999999998e-05, 2.0399999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 136 loss: 5.8242 iter time (s): 5.554 samples/sec: 34.567
[default0]:[2024-04-21 11:01:14,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=137, skipped=0, lr=[2.0549999999999998e-05, 2.0549999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 137 loss: 5.7691 iter time (s): 5.597 samples/sec: 34.304
[default0]:[2024-04-21 11:01:19,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=138, skipped=0, lr=[2.07e-05, 2.07e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 138 loss: 5.7976 iter time (s): 5.544 samples/sec: 34.633
[default0]:[2024-04-21 11:01:25,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=139, skipped=0, lr=[2.085e-05, 2.085e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 139 loss: 5.8234 iter time (s): 5.572 samples/sec: 34.456
[default0]:[2024-04-21 11:01:30,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[2.1e-05, 2.1e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 140 loss: 5.8463 iter time (s): 5.544 samples/sec: 34.634
[default0]:[2024-04-21 11:01:36,451] [INFO] [logging.py:96:log_dist] [Rank 0] step=141, skipped=0, lr=[2.115e-05, 2.115e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 141 loss: 5.8359 iter time (s): 5.525 samples/sec: 34.751
[default0]:[2024-04-21 11:01:42,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=142, skipped=0, lr=[2.13e-05, 2.13e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 142 loss: 5.7675 iter time (s): 5.572 samples/sec: 34.456
[default0]:[2024-04-21 11:01:47,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=143, skipped=0, lr=[2.1449999999999996e-05, 2.1449999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 143 loss: 5.8002 iter time (s): 5.618 samples/sec: 34.173
[default0]:[2024-04-21 11:01:53,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=144, skipped=0, lr=[2.16e-05, 2.16e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 144 loss: 5.6307 iter time (s): 5.594 samples/sec: 34.322
[default0]:[2024-04-21 11:01:58,858] [INFO] [logging.py:96:log_dist] [Rank 0] step=145, skipped=0, lr=[2.1749999999999997e-05, 2.1749999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 145 loss: 5.7900 iter time (s): 5.588 samples/sec: 34.361
[default0]:[2024-04-21 11:02:04,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=146, skipped=0, lr=[2.1899999999999997e-05, 2.1899999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 146 loss: 5.6849 iter time (s): 5.654 samples/sec: 33.957
[default0]:[2024-04-21 11:02:10,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=147, skipped=0, lr=[2.205e-05, 2.205e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 147 loss: 5.6946 iter time (s): 5.613 samples/sec: 34.206
[default0]:[2024-04-21 11:02:15,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=148, skipped=0, lr=[2.2199999999999998e-05, 2.2199999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 148 loss: 5.6307 iter time (s): 5.574 samples/sec: 34.445
[default0]:[2024-04-21 11:02:21,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=149, skipped=0, lr=[2.235e-05, 2.235e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 149 loss: 5.7465 iter time (s): 5.645 samples/sec: 34.015
[default0]:[2024-04-21 11:02:26,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[2.2499999999999998e-05, 2.2499999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 150 loss: 5.6386 iter time (s): 5.552 samples/sec: 34.582
[default0]:[2024-04-21 11:02:32,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=151, skipped=0, lr=[2.2649999999999998e-05, 2.2649999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 151 loss: 5.6005 iter time (s): 5.684 samples/sec: 33.780
[default0]:[2024-04-21 11:02:38,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=152, skipped=0, lr=[2.2799999999999995e-05, 2.2799999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 152 loss: 5.5931 iter time (s): 5.655 samples/sec: 33.955
[default0]:[2024-04-21 11:02:43,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=153, skipped=0, lr=[2.295e-05, 2.295e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 153 loss: 5.7280 iter time (s): 5.593 samples/sec: 34.331
[default0]:[2024-04-21 11:02:49,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=154, skipped=0, lr=[2.31e-05, 2.31e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 154 loss: 5.6574 iter time (s): 5.631 samples/sec: 34.097
[default0]:[2024-04-21 11:02:55,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=155, skipped=0, lr=[2.3249999999999996e-05, 2.3249999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 155 loss: 5.6695 iter time (s): 5.549 samples/sec: 34.602
[default0]:[2024-04-21 11:03:00,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=156, skipped=0, lr=[2.34e-05, 2.34e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 156 loss: 5.6576 iter time (s): 5.569 samples/sec: 34.475
[default0]:[2024-04-21 11:03:06,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=157, skipped=0, lr=[2.3549999999999996e-05, 2.3549999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 157 loss: 5.6224 iter time (s): 5.568 samples/sec: 34.482
[default7]: iteration      138/  250000 | consumed samples:        26496 | consumed tokens:     54263808 | elapsed time per iteration (ms): 5554.8 | learning rate: 2.070E-05 | global batch size:   192 | lm loss: 5.797564E+00 | grad norm: 5.029 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.564 | TFLOPs: 46.97 |
[default7]: iteration      139/  250000 | consumed samples:        26688 | consumed tokens:     54657024 | elapsed time per iteration (ms): 5576.8 | learning rate: 2.085E-05 | global batch size:   192 | lm loss: 5.823449E+00 | grad norm: 4.634 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.428 | TFLOPs: 46.78 |
[default7]: iteration      140/  250000 | consumed samples:        26880 | consumed tokens:     55050240 | elapsed time per iteration (ms): 5549.7 | learning rate: 2.100E-05 | global batch size:   192 | lm loss: 5.846262E+00 | grad norm: 4.581 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.597 | TFLOPs: 47.01 |
[default7]: iteration      141/  250000 | consumed samples:        27072 | consumed tokens:     55443456 | elapsed time per iteration (ms): 5536.0 | learning rate: 2.115E-05 | global batch size:   192 | lm loss: 5.835933E+00 | grad norm: 4.573 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.682 | TFLOPs: 47.13 |
[default7]: iteration      142/  250000 | consumed samples:        27264 | consumed tokens:     55836672 | elapsed time per iteration (ms): 5576.8 | learning rate: 2.130E-05 | global batch size:   192 | lm loss: 5.767463E+00 | grad norm: 4.448 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.428 | TFLOPs: 46.78 |
[default7]: iteration      143/  250000 | consumed samples:        27456 | consumed tokens:     56229888 | elapsed time per iteration (ms): 5630.9 | learning rate: 2.145E-05 | global batch size:   192 | lm loss: 5.800208E+00 | grad norm: 4.318 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.098 | TFLOPs: 46.34 |
[default7]: iteration      144/  250000 | consumed samples:        27648 | consumed tokens:     56623104 | elapsed time per iteration (ms): 5599.8 | learning rate: 2.160E-05 | global batch size:   192 | lm loss: 5.630652E+00 | grad norm: 3.779 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.287 | TFLOPs: 46.59 |
[default7]: iteration      145/  250000 | consumed samples:        27840 | consumed tokens:     57016320 | elapsed time per iteration (ms): 5592.1 | learning rate: 2.175E-05 | global batch size:   192 | lm loss: 5.790034E+00 | grad norm: 4.119 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.334 | TFLOPs: 46.66 |
[default7]: iteration      146/  250000 | consumed samples:        28032 | consumed tokens:     57409536 | elapsed time per iteration (ms): 5658.6 | learning rate: 2.190E-05 | global batch size:   192 | lm loss: 5.684945E+00 | grad norm: 3.310 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.931 | TFLOPs: 46.11 |
[default7]: iteration      147/  250000 | consumed samples:        28224 | consumed tokens:     57802752 | elapsed time per iteration (ms): 5618.0 | learning rate: 2.205E-05 | global batch size:   192 | lm loss: 5.694610E+00 | grad norm: 2.715 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.176 | TFLOPs: 46.44 |
[default7]: iteration      148/  250000 | consumed samples:        28416 | consumed tokens:     58195968 | elapsed time per iteration (ms): 5578.4 | learning rate: 2.220E-05 | global batch size:   192 | lm loss: 5.630731E+00 | grad norm: 3.244 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.419 | TFLOPs: 46.77 |
[default7]: iteration      149/  250000 | consumed samples:        28608 | consumed tokens:     58589184 | elapsed time per iteration (ms): 5655.7 | learning rate: 2.235E-05 | global batch size:   192 | lm loss: 5.746455E+00 | grad norm: 2.432 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.948 | TFLOPs: 46.13 |
[default7]: iteration      150/  250000 | consumed samples:        28800 | consumed tokens:     58982400 | elapsed time per iteration (ms): 5564.8 | learning rate: 2.250E-05 | global batch size:   192 | lm loss: 5.638551E+00 | grad norm: 2.626 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.502 | TFLOPs: 46.89 |
[default7]: iteration      151/  250000 | consumed samples:        28992 | consumed tokens:     59375616 | elapsed time per iteration (ms): 5694.8 | learning rate: 2.265E-05 | global batch size:   192 | lm loss: 5.600510E+00 | grad norm: 2.838 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.715 | TFLOPs: 45.82 |
[default7]: iteration      152/  250000 | consumed samples:        29184 | consumed tokens:     59768832 | elapsed time per iteration (ms): 5659.0 | learning rate: 2.280E-05 | global batch size:   192 | lm loss: 5.593122E+00 | grad norm: 3.970 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.928 | TFLOPs: 46.11 |
[default7]: iteration      153/  250000 | consumed samples:        29376 | consumed tokens:     60162048 | elapsed time per iteration (ms): 5599.6 | learning rate: 2.295E-05 | global batch size:   192 | lm loss: 5.728038E+00 | grad norm: 4.319 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.288 | TFLOPs: 46.59 |
[default7]: iteration      154/  250000 | consumed samples:        29568 | consumed tokens:     60555264 | elapsed time per iteration (ms): 5635.7 | learning rate: 2.310E-05 | global batch size:   192 | lm loss: 5.657429E+00 | grad norm: 6.124 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.068 | TFLOPs: 46.30 |
[default7]: iteration      155/  250000 | consumed samples:        29760 | consumed tokens:     60948480 | elapsed time per iteration (ms): 5553.3 | learning rate: 2.325E-05 | global batch size:   192 | lm loss: 5.669502E+00 | grad norm: 3.742 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.574 | TFLOPs: 46.98 |
[default7]: iteration      156/  250000 | consumed samples:        29952 | consumed tokens:     61341696 | elapsed time per iteration (ms): 5575.3 | learning rate: 2.340E-05 | global batch size:   192 | lm loss: 5.657571E+00 | grad norm: 3.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.438 | TFLOPs: 46.80 |
[default7]: iteration      157/  250000 | consumed samples:        30144 | consumed tokens:     61734912 | elapsed time per iteration (ms): 5575.0 | learning rate: 2.355E-05 | global batch size:   192 | lm loss: 5.622413E+00 | grad norm: 2.581 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.440 | TFLOPs: 46.80 |
[default7]: iteration      158/  250000 | consumed samples:        30336 | consumed tokens:     62128128 | elapsed time per iteration (ms): 5596.7 | learning rate: 2.370E-05 | global batch size:   192 | lm loss: 5.566034E+00 | grad norm: 2.818 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.306 | TFLOPs: 46.62 |
[default7]: iteration      159/  250000 | consumed samples:        30528 | consumed tokens:     62521344 | elapsed time per iteration (ms): 5596.1 | learning rate: 2.385E-05 | global batch size:   192 | lm loss: 5.504580E+00 | grad norm: 2.936 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.309 | TFLOPs: 46.62 |
[default7]: iteration      160/  250000 | consumed samples:        30720 | consumed tokens:     62914560 | elapsed time per iteration (ms): 5615.1 | learning rate: 2.400E-05 | global batch size:   192 | lm loss: 5.513438E+00 | grad norm: 3.095 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.194 | TFLOPs: 46.47 |
[default7]: iteration      161/  250000 | consumed samples:        30912 | consumed tokens:     63307776 | elapsed time per iteration (ms): 5660.2 | learning rate: 2.415E-05 | global batch size:   192 | lm loss: 5.361684E+00 | grad norm: 3.535 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.921 | TFLOPs: 46.10 |
[default7]: iteration      162/  250000 | consumed samples:        31104 | consumed tokens:     63700992 | elapsed time per iteration (ms): 5631.1 | learning rate: 2.430E-05 | global batch size:   192 | lm loss: 5.460402E+00 | grad norm: 3.547 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.096 | TFLOPs: 46.33 |
[default7]: iteration      163/  250000 | consumed samples:        31296 | consumed tokens:     64094208 | elapsed time per iteration (ms): 5554.1 | learning rate: 2.445E-05 | global batch size:   192 | lm loss: 5.405450E+00 | grad norm: 3.059 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.569 | TFLOPs: 46.98 |
[default7]: iteration      164/  250000 | consumed samples:        31488 | consumed tokens:     64487424 | elapsed time per iteration (ms): 5552.4 | learning rate: 2.460E-05 | global batch size:   192 | lm loss: 5.425708E+00 | grad norm: 3.116 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.579 | TFLOPs: 46.99 |
[default7]: iteration      165/  250000 | consumed samples:        31680 | consumed tokens:     64880640 | elapsed time per iteration (ms): 5532.1 | learning rate: 2.475E-05 | global batch size:   192 | lm loss: 5.484314E+00 | grad norm: 4.345 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.707 | TFLOPs: 47.16 |
[default7]: iteration      166/  250000 | consumed samples:        31872 | consumed tokens:     65273856 | elapsed time per iteration (ms): 5575.3 | learning rate: 2.490E-05 | global batch size:   192 | lm loss: 5.264068E+00 | grad norm: 4.039 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.438 | TFLOPs: 46.80 |
[default7]: iteration      167/  250000 | consumed samples:        32064 | consumed tokens:     65667072 | elapsed time per iteration (ms): 5598.2 | learning rate: 2.505E-05 | global batch size:   192 | lm loss: 5.316464E+00 | grad norm: 4.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.297 | TFLOPs: 46.61 |
[default7]: iteration      168/  250000 | consumed samples:        32256 | consumed tokens:     66060288 | elapsed time per iteration (ms): 5603.0 | learning rate: 2.520E-05 | global batch size:   192 | lm loss: 5.385224E+00 | grad norm: 4.886 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.268 | TFLOPs: 46.57 |
[default7]: iteration      169/  250000 | consumed samples:        32448 | consumed tokens:     66453504 | elapsed time per iteration (ms): 5689.6 | learning rate: 2.535E-05 | global batch size:   192 | lm loss: 5.429765E+00 | grad norm: 6.542 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.746 | TFLOPs: 45.86 |
[default7]: iteration      170/  250000 | consumed samples:        32640 | consumed tokens:     66846720 | elapsed time per iteration (ms): 5589.0 | learning rate: 2.550E-05 | global batch size:   192 | lm loss: 5.579053E+00 | grad norm: 4.830 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.353 | TFLOPs: 46.68 |
[default7]: iteration      171/  250000 | consumed samples:        32832 | consumed tokens:     67239936 | elapsed time per iteration (ms): 5604.8 | learning rate: 2.565E-05 | global batch size:   192 | lm loss: 5.673410E+00 | grad norm: 4.846 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.256 | TFLOPs: 46.55 |
[default7]: iteration      172/  250000 | consumed samples:        33024 | consumed tokens:     67633152 | elapsed time per iteration (ms): 5561.8 | learning rate: 2.580E-05 | global batch size:   192 | lm loss: 5.437352E+00 | grad norm: 3.376 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.521 | TFLOPs: 46.91 |
[default7]: iteration      173/  250000 | consumed samples:        33216 | consumed tokens:     68026368 | elapsed time per iteration (ms): 5563.9 | learning rate: 2.595E-05 | global batch size:   192 | lm loss: 5.410245E+00 | grad norm: 5.008 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.508 | TFLOPs: 46.89 |
[default7]: iteration      174/  250000 | consumed samples:        33408 | consumed tokens:     68419584 | elapsed time per iteration (ms): 5543.2 | learning rate: 2.610E-05 | global batch size:   192 | lm loss: 5.386577E+00 | grad norm: 4.486 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.637 | TFLOPs: 47.07 |
[default7]: iteration      175/  250000 | consumed samples:        33600 | consumed tokens:     68812800 | elapsed time per iteration (ms): 5543.4 | learning rate: 2.625E-05 | global batch size:   192 | lm loss: 5.405876E+00 | grad norm: 3.144 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.636 | TFLOPs: 47.07 |
[default7]: iteration      176/  250000 | consumed samples:        33792 | consumed tokens:     69206016 | elapsed time per iteration (ms): 5638.5 | learning rate: 2.640E-05 | global batch size:   192 | lm loss: 5.391779E+00 | grad norm: 7.005 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.052 | TFLOPs: 46.27 |
[default7]: iteration      177/  250000 | consumed samples:        33984 | consumed tokens:     69599232 | elapsed time per iteration (ms): 5601.4 | learning rate: 2.655E-05 | global batch size:   192 | lm loss: 5.361073E+00 | grad norm: 4.597 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.277 | TFLOPs: 46.58 |
[default0]:[2024-04-21 11:03:11,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=158, skipped=0, lr=[2.37e-05, 2.37e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 158 loss: 5.5660 iter time (s): 5.584 samples/sec: 34.382
[default0]:[2024-04-21 11:03:17,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=159, skipped=0, lr=[2.3849999999999997e-05, 2.3849999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 159 loss: 5.5046 iter time (s): 5.592 samples/sec: 34.337
[default0]:[2024-04-21 11:03:23,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[2.3999999999999997e-05, 2.3999999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 160 loss: 5.5134 iter time (s): 5.599 samples/sec: 34.290
[default0]:[2024-04-21 11:03:28,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=161, skipped=0, lr=[2.415e-05, 2.415e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 161 loss: 5.3617 iter time (s): 5.647 samples/sec: 34.001
[default0]:[2024-04-21 11:03:34,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=162, skipped=0, lr=[2.4299999999999998e-05, 2.4299999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 162 loss: 5.4604 iter time (s): 5.627 samples/sec: 34.123
[default0]:[2024-04-21 11:03:39,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=163, skipped=0, lr=[2.4449999999999998e-05, 2.4449999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 163 loss: 5.4054 iter time (s): 5.542 samples/sec: 34.643
[default0]:[2024-04-21 11:03:45,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=164, skipped=0, lr=[2.4599999999999998e-05, 2.4599999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 164 loss: 5.4257 iter time (s): 5.548 samples/sec: 34.606
[default0]:[2024-04-21 11:03:50,984] [INFO] [logging.py:96:log_dist] [Rank 0] step=165, skipped=0, lr=[2.475e-05, 2.475e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 165 loss: 5.4843 iter time (s): 5.526 samples/sec: 34.747
[default0]:[2024-04-21 11:03:56,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=166, skipped=0, lr=[2.4899999999999995e-05, 2.4899999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 166 loss: 5.2641 iter time (s): 5.562 samples/sec: 34.519
[default0]:[2024-04-21 11:04:02,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=167, skipped=0, lr=[2.505e-05, 2.505e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 167 loss: 5.3165 iter time (s): 5.585 samples/sec: 34.377
[default0]:[2024-04-21 11:04:07,757] [INFO] [logging.py:96:log_dist] [Rank 0] step=168, skipped=0, lr=[2.5199999999999996e-05, 2.5199999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 168 loss: 5.3852 iter time (s): 5.597 samples/sec: 34.303
[default0]:[2024-04-21 11:04:13,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=169, skipped=0, lr=[2.5349999999999996e-05, 2.5349999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 169 loss: 5.4298 iter time (s): 5.685 samples/sec: 33.772
[default0]:[2024-04-21 11:04:19,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[2.55e-05, 2.55e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 170 loss: 5.5791 iter time (s): 5.585 samples/sec: 34.380
[default0]:[2024-04-21 11:04:24,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=171, skipped=0, lr=[2.5649999999999997e-05, 2.5649999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 171 loss: 5.6734 iter time (s): 5.600 samples/sec: 34.284
[default0]:[2024-04-21 11:04:30,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=172, skipped=0, lr=[2.58e-05, 2.58e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 172 loss: 5.4374 iter time (s): 5.557 samples/sec: 34.553
[default0]:[2024-04-21 11:04:35,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=173, skipped=0, lr=[2.5949999999999997e-05, 2.5949999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 173 loss: 5.4102 iter time (s): 5.557 samples/sec: 34.550
[default0]:[2024-04-21 11:04:41,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=174, skipped=0, lr=[2.6099999999999997e-05, 2.6099999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 174 loss: 5.3866 iter time (s): 5.531 samples/sec: 34.713
[default0]:[2024-04-21 11:04:46,857] [INFO] [logging.py:96:log_dist] [Rank 0] step=175, skipped=0, lr=[2.6249999999999994e-05, 2.6249999999999994e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 175 loss: 5.4059 iter time (s): 5.537 samples/sec: 34.677
[default0]:[2024-04-21 11:04:52,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=176, skipped=0, lr=[2.6399999999999998e-05, 2.6399999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 176 loss: 5.3918 iter time (s): 5.634 samples/sec: 34.079
[default0]:[2024-04-21 11:04:58,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=177, skipped=0, lr=[2.6549999999999998e-05, 2.6549999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 177 loss: 5.3611 iter time (s): 5.585 samples/sec: 34.375
[default0]:[2024-04-21 11:05:03,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=178, skipped=0, lr=[2.67e-05, 2.67e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 178 loss: 5.3177 iter time (s): 5.600 samples/sec: 34.284
[default0]:[2024-04-21 11:05:09,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=179, skipped=0, lr=[2.685e-05, 2.685e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 179 loss: 5.3226 iter time (s): 5.568 samples/sec: 34.483
[default0]:[2024-04-21 11:05:14,913] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[2.6999999999999996e-05, 2.6999999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 180 loss: 5.1757 iter time (s): 5.632 samples/sec: 34.091
[default0]:[2024-04-21 11:05:20,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=181, skipped=0, lr=[2.715e-05, 2.715e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 181 loss: 5.2580 iter time (s): 5.568 samples/sec: 34.482
[default0]:[2024-04-21 11:05:26,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=182, skipped=0, lr=[2.7299999999999996e-05, 2.7299999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 182 loss: 5.3352 iter time (s): 5.577 samples/sec: 34.429
[default0]:[2024-04-21 11:05:31,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=183, skipped=0, lr=[2.7449999999999996e-05, 2.7449999999999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 183 loss: 5.3189 iter time (s): 5.568 samples/sec: 34.480
[default0]:[2024-04-21 11:05:37,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=184, skipped=0, lr=[2.76e-05, 2.76e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 184 loss: 5.2339 iter time (s): 5.618 samples/sec: 34.173
[default0]:[2024-04-21 11:05:42,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=185, skipped=0, lr=[2.7749999999999997e-05, 2.7749999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 185 loss: 5.2654 iter time (s): 5.604 samples/sec: 34.259
[default0]:[2024-04-21 11:05:48,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=186, skipped=0, lr=[2.79e-05, 2.79e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 186 loss: 5.1926 iter time (s): 5.611 samples/sec: 34.218
[default0]:[2024-04-21 11:05:54,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=187, skipped=0, lr=[2.8049999999999997e-05, 2.8049999999999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 187 loss: 5.1089 iter time (s): 5.622 samples/sec: 34.150
[default0]:[2024-04-21 11:05:59,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=188, skipped=0, lr=[2.8199999999999998e-05, 2.8199999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 188 loss: 5.0932 iter time (s): 5.597 samples/sec: 34.303
[default0]:[2024-04-21 11:06:05,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=189, skipped=0, lr=[2.8349999999999995e-05, 2.8349999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:steps: 189 loss: 4.9567 iter time (s): 5.629 samples/sec: 34.111
[default0]:[2024-04-21 11:06:10,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[2.8499999999999998e-05, 2.8499999999999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
slurmstepd: error: *** JOB 1578116 ON jean-zay-iam03 CANCELLED AT 2024-04-21T11:06:55 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 1578116.0 ON jean-zay-iam03 CANCELLED AT 2024-04-21T11:06:55 DUE TO TIME LIMIT ***
[2024-04-21 11:06:55,448] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-04-21 11:06:55,448] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2264396 closing signal SIGTERM
[2024-04-21 11:06:55,449] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2264397 closing signal SIGTERM
[2024-04-21 11:06:55,449] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2264398 closing signal SIGTERM
[2024-04-21 11:06:55,449] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-04-21 11:06:55,449] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2033455 closing signal SIGTERM
[2024-04-21 11:06:55,449] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2033456 closing signal SIGTERM
[2024-04-21 11:06:55,449] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2264399 closing signal SIGTERM
[2024-04-21 11:06:55,450] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2264400 closing signal SIGTERM
[2024-04-21 11:06:55,450] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2264401 closing signal SIGTERM
[2024-04-21 11:06:55,450] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2264402 closing signal SIGTERM
[2024-04-21 11:06:55,450] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-04-21 11:06:55,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2264403 closing signal SIGTERM
[2024-04-21 11:06:55,450] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-04-21 11:06:55,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2291234 closing signal SIGTERM
[2024-04-21 11:06:55,451] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-04-21 11:06:55,451] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-04-21 11:06:55,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3601853 closing signal SIGTERM
[2024-04-21 11:06:55,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796559 closing signal SIGTERM
[2024-04-21 11:06:55,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796560 closing signal SIGTERM
[2024-04-21 11:06:55,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2291235 closing signal SIGTERM
[2024-04-21 11:06:55,451] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-04-21 11:06:55,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3601854 closing signal SIGTERM
[2024-04-21 11:06:55,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2291236 closing signal SIGTERM
[2024-04-21 11:06:55,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 955263 closing signal SIGTERM
[2024-04-21 11:06:55,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796561 closing signal SIGTERM
[2024-04-21 11:06:55,451] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 947017 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 947018 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3601855 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2291237 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3601856 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796562 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3601857 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2291238 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2800454 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3601858 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2291239 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2800455 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3601859 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796563 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3601860 closing signal SIGTERM
[2024-04-21 11:06:55,452] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 947019 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2800456 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2291240 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 947020 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 947021 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2291241 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796564 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 947022 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2800457 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 955264 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796565 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 947023 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 947024 closing signal SIGTERM
[2024-04-21 11:06:55,453] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796566 closing signal SIGTERM
[2024-04-21 11:06:55,454] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2800458 closing signal SIGTERM
[2024-04-21 11:06:55,454] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 955265 closing signal SIGTERM
[2024-04-21 11:06:55,454] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2800459 closing signal SIGTERM
[2024-04-21 11:06:55,454] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2800460 closing signal SIGTERM
[2024-04-21 11:06:55,454] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 955266 closing signal SIGTERM
[2024-04-21 11:06:55,454] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2800461 closing signal SIGTERM
[2024-04-21 11:06:55,454] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 955267 closing signal SIGTERM
[2024-04-21 11:06:55,455] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 955268 closing signal SIGTERM
[2024-04-21 11:06:55,455] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 955269 closing signal SIGTERM
[2024-04-21 11:06:55,455] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 955270 closing signal SIGTERM
[2024-04-21 11:06:55,462] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2033457 closing signal SIGTERM
[2024-04-21 11:06:55,462] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2033458 closing signal SIGTERM
[2024-04-21 11:06:55,462] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2033459 closing signal SIGTERM
[2024-04-21 11:06:55,462] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2033460 closing signal SIGTERM
[2024-04-21 11:06:55,463] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2033461 closing signal SIGTERM
[2024-04-21 11:06:55,463] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2033462 closing signal SIGTERM
